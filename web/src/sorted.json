[
  {
    "id": 7654321,
    "title": "Transferring state abstractions between MDPs",
    "abst": "",
    "url": "",
    "lang": "en",
    "authors": [],
    "fos": [],
    "journals": [],
    "conferences": [],
    "conference_series": [],
    "references": [],
    "filter_matches": [
      "eta"
    ],
    "rank": 0,
    "citation_count": 51,
    "estimated_citation_count": 0,
    "publication_date": "2006-01-01",
    "found_in": 1,
    "transfer_experiment_type": [
      "r",
      "s"
    ],
    "transfer_experiment_subtype": [
      "multi"
    ],
    "transfer_data_type": [
      "fea"
    ],
    "transfer_performance_metrics": [
      "tt"
    ],
    "implementation": [
      "Formulas",
      "Figures",
      "Theorem"
    ],
    "policy_type": [
      "GATA"
    ],
    "task_mappings": [
      "N/A"
    ],
    "autonomous_transfer": 0,
    "is_deep_rl": 0,
    "tags": [
      "MDP"
    ],
    "allowed_learner": [
      "any"
    ],
    "country": [
      "USA"
    ],
    "uni": [
      "Rutgers University"
    ],
    "department": [
      "Computer Science"
    ],
    "source_task_selection": [
      "all"
    ],
    "was_in_survey": [
      "taylor"
    ],
    "in_title": [
      "t"
    ],
    "in_abs": [
      "t",
      "l",
      "k"
    ],
    "in_contet": [
      "t",
      "r",
      "l",
      "k"
    ],
    "rejected_or_unpublished": null,
    "duplication_status": null,
    "paper_for_thesis": 0,
    "paper_available": 1,
    "behind_paywall": 0,
    "paywall_circumventable": 0
  },
  {
    "id": 1234567,
    "title": "Model Transfer for Markov Decision Tasks via Parameter Matching",
    "abst": "",
    "url": "",
    "lang": "en",
    "authors": [],
    "fos": [],
    "journals": [],
    "conferences": [],
    "conference_series": [],
    "references": [],
    "filter_matches": [
      "eta"
    ],
    "rank": 0,
    "citation_count": 26,
    "estimated_citation_count": 0,
    "publication_date": "2006-01-01",
    "found_in": 1,
    "transfer_experiment_type": [
      "t"
    ],
    "transfer_experiment_subtype": [
      "multi"
    ],
    "transfer_data_type": [
      "pri"
    ],
    "transfer_performance_metrics": [
      "j",
      "tr"
    ],
    "implementation": [
      "Custom",
      "CS",
      "Pseudo",
      "Formulas",
      "Figures",
      "Tables"
    ],
    "policy_type": [
      "Bayesian"
    ],
    "task_mappings": [
      "N/A"
    ],
    "autonomous_transfer": 0,
    "is_deep_rl": 0,
    "tags": [
      "MDP"
    ],
    "allowed_learner": [
      "B"
    ],
    "country": [
      "UK"
    ],
    "uni": [
      "University of Birmingham",
      "University Hospital Birmingham"
    ],
    "department": [
      "Computer Science"
    ],
    "source_task_selection": [
      "all"
    ],
    "was_in_survey": [
      "taylor"
    ],
    "in_title": [
      "t"
    ],
    "in_abs": [
      "t",
      "k"
    ],
    "in_contet": [
      "t",
      "r",
      "l",
      "k"
    ],
    "rejected_or_unpublished": null,
    "duplication_status": null,
    "paper_for_thesis": 0,
    "paper_available": 1,
    "behind_paywall": 0,
    "paywall_circumventable": 0
  },
  {
    "id": 2962858248,
    "title": "Actor-Mimic: Deep Multitask and Transfer Reinforcement Learning",
    "abst": "Abstract: The ability to act in multiple environments and transfer previous knowledge to new situations can be considered a critical aspect of any intelligent agent. Towards this goal, we define a novel method of multitask and transfer learning that enables an autonomous agent to learn how to behave in multiple tasks simultaneously, and then generalize its knowledge to new domains. This method, termed &quot;Actor-Mimic&quot;, exploits the use of deep reinforcement learning and model compression techniques to train a single policy network that learns how to act in a set of distinct tasks by using the guidance of several expert teachers. We then show that the representations learnt by the deep policy network are capable of generalizing to new tasks with no prior expert guidance, speeding up learning in novel environments. Although our method can in general be applied to a wide range of problems, we use Atari games as a testing environment to demonstrate these methods.",
    "url": "https://ui.adsabs.harvard.edu/abs/2015arXiv151106342P/abstract",
    "lang": "en",
    "authors": [
      2031945151,
      2097546270,
      2161182173
    ],
    "fos": [
      165696696,
      2985801418,
      150899416,
      177148314,
      74072328,
      41008148,
      97541855,
      154945302,
      119857082,
      13687954
    ],
    "journals": [],
    "conferences": [
      2322552234
    ],
    "conference_series": [
      2584161585
    ],
    "references": [],
    "filter_matches": [
      "tr",
      "tl",
      "trl",
      "rl"
    ],
    "rank": 17855,
    "citation_count": 83,
    "estimated_citation_count": 109,
    "publication_date": "2016-01-01",
    "found_in": 1,
    "transfer_experiment_type": [
      "p",
      "r",
      "s",
      "#",
      "a"
    ],
    "transfer_experiment_subtype": [
      "lit"
    ],
    "transfer_data_type": [
      "weights"
    ],
    "transfer_performance_metrics": [
      "j",
      "tt"
    ],
    "implementation": [
      "Custom",
      "CS",
      "Theorem",
      "Lemma",
      "Formulas",
      "Figures",
      "Tables"
    ],
    "policy_type": [
      "DQN",
      "AMN"
    ],
    "task_mappings": [
      "N/A"
    ],
    "autonomous_transfer": 1,
    "is_deep_rl": 1,
    "tags": [
      "Simulation",
      "Games",
      "VideoGames",
      "Atari",
      "Generalization"
    ],
    "allowed_learner": [
      "TD"
    ],
    "country": [
      "Canada"
    ],
    "uni": [
      "University of Toronto"
    ],
    "department": [
      "Computer Science"
    ],
    "source_task_selection": [
      "all"
    ],
    "was_in_survey": [
      "mag",
      "zhu",
      "multi"
    ],
    "in_title": [
      "t",
      "r",
      "l",
      "ta"
    ],
    "in_abs": [
      "t",
      "r",
      "l",
      "k",
      "de",
      "do",
      "g",
      "ta"
    ],
    "in_contet": [
      "t",
      "r",
      "l",
      "k",
      "de",
      "do",
      "ta"
    ],
    "rejected_or_unpublished": null,
    "duplication_status": null,
    "paper_for_thesis": 0,
    "paper_available": 1,
    "behind_paywall": 0,
    "paywall_circumventable": 0,
    "author_names": [
      "Ruslan Salakhutdinov",
      "Jimmy Ba",
      "Emilio Parisotto"
    ]
  },
  {
    "id": 2440926996,
    "title": "Successor Features for Transfer in Reinforcement Learning",
    "abst": "Transfer in reinforcement learning refers to the notion that generalization should occur not only within a task but also across tasks. We propose a transfer framework for the scenario where the reward function changes between tasks but the environment&#039;s dynamics remain the same. Our approach rests on two key ideas: &quot;successor features&quot;, a value function representation that decouples the dynamics of the environment from the rewards, and &quot;generalized policy improvement&quot;, a generalization of dynamic programming&#039;s policy improvement operation that considers a set of policies rather than a single one. Put together, the two ideas lead to an approach that integrates seamlessly within the reinforcement learning framework and allows the free exchange of information across tasks. The proposed method also provides performance guarantees for the transferred policy even before any learning has taken place. We derive two theorems that set our approach in firm theoretical ground and present experiments that show that it successfully promotes transfer in practice, significantly outperforming alternative methods in a sequence of navigation tasks and in the control of a simulated robotic arm.",
    "url": "https://export.arxiv.org/pdf/1606.05312",
    "lang": "en",
    "authors": [
      55380488,
      254054780,
      2158224575,
      2593774290,
      2620408480,
      2686305946,
      2778963749
    ],
    "fos": [
      119857082,
      154945302,
      41008148,
      75306776,
      150415221,
      2776375849,
      3018644393,
      14646407,
      97541855,
      37404715
    ],
    "journals": [
      2596500785
    ],
    "conferences": [],
    "conference_series": [],
    "references": [
      567721252,
      1522301498,
      1554944419,
      1822705290,
      2056354534,
      2061562262,
      2082691056,
      2097381042,
      2117629901,
      2123979492,
      2124175081,
      2132351269,
      2138326839,
      2159849946,
      2312609093,
      2341171179,
      2950577311
    ],
    "filter_matches": [
      "tr",
      "tl",
      "trl",
      "rl"
    ],
    "rank": 18372,
    "citation_count": 20,
    "estimated_citation_count": 26,
    "publication_date": "2016-06-16",
    "found_in": 1,
    "transfer_experiment_type": [
      "s",
      "s_i",
      "s_f",
      "t"
    ],
    "transfer_experiment_subtype": [
      "multi"
    ],
    "transfer_data_type": [
      "fea",
      "pi"
    ],
    "transfer_performance_metrics": [
      "j",
      "tt"
    ],
    "implementation": [
      "Custom",
      "CS",
      "Pseudo",
      "Theorem",
      "Formulas",
      "Figures",
      "Tables",
      "Lemma"
    ],
    "policy_type": [
      "DQN",
      "SFDQN",
      "SFQL",
      "PRQL"
    ],
    "task_mappings": [
      "N/A"
    ],
    "autonomous_transfer": 0,
    "is_deep_rl": 1,
    "tags": [
      "2D",
      "Navigation",
      "Collect",
      "Simulation",
      "Robotics",
      "Mujoco",
      "Arm"
    ],
    "allowed_learner": [
      "TD",
      "H"
    ],
    "country": [
      "USA"
    ],
    "uni": [
      "DeepMind"
    ],
    "department": [
      "Industry"
    ],
    "source_task_selection": [
      "all"
    ],
    "was_in_survey": [
      "mag",
      "zhu"
    ],
    "in_title": [
      "t",
      "r",
      "l"
    ],
    "in_abs": [
      "t",
      "r",
      "l"
    ],
    "in_contet": [
      "t",
      "r",
      "l",
      "k",
      "s"
    ],
    "rejected_or_unpublished": null,
    "duplication_status": null,
    "paper_for_thesis": 0,
    "paper_available": 1,
    "behind_paywall": 0,
    "paywall_circumventable": 0,
    "author_names": [
      "Tom Schaul",
      "R&#x00E9;mi Munos",
      "Jonathan J. Hunt",
      "David Silver",
      "Will Dabney",
      "Andre Barreto",
      "Hado van Hasselt"
    ]
  },
  {
    "id": 2605368761,
    "title": "Learning Invariant Feature Spaces to Transfer Skills with Reinforcement Learning",
    "abst": "People can learn a wide range of tasks from their own experience, but can also learn from observing other creatures. This can accelerate acquisition of new skills even when the observed agent differs substantially from the learning agent in terms of morphology. In this paper, we examine how reinforcement learning algorithms can transfer knowledge between morphologically different agents (e.g., different robots). We introduce a problem formulation where twp agents are tasked with learning multiple skills by sharing information. Our method uses the skills that were learned by both agents to train invariant feature spaces that can then be used to transfer other skills from one agent to another. The process of learning these invariant feature spaces can be viewed as a kind of ``analogy making,&#039;&#039; or implicit learning of partial correspondences between two distinct domains. We evaluate our transfer learning algorithm in two simulated robotic manipulation skills, and illustrate that we can transfer knowledge between simulated robotic arms with different numbers of links, as well as simulated arms with different actuation mechanisms, where one robot is torque-driven while the other is tendon-driven.",
    "url": "https://openreview.net/pdf?id=Hyq4yhile",
    "lang": null,
    "authors": [
      243981275,
      2140030024,
      2312135413,
      2549226862,
      2604642998
    ],
    "fos": [
      41008148,
      150899416,
      77075516,
      58973888,
      97541855,
      119857082,
      154945302,
      59404180,
      199190896,
      188888258,
      28006648
    ],
    "journals": [],
    "conferences": [
      2583582360
    ],
    "conference_series": [
      2584161585
    ],
    "references": [],
    "filter_matches": [
      "tr",
      "tl",
      "trl",
      "rl"
    ],
    "rank": 18432,
    "citation_count": 26,
    "estimated_citation_count": 26,
    "publication_date": "2017-04-24",
    "found_in": 1,
    "transfer_experiment_type": [
      "#",
      "t",
      "v",
      "a"
    ],
    "transfer_experiment_subtype": [
      "diff-no"
    ],
    "transfer_data_type": [
      "fea"
    ],
    "transfer_performance_metrics": [
      "j",
      "tt",
      "ap"
    ],
    "implementation": [
      "Custom",
      "CS",
      "Mujoco",
      "Figures",
      "Formulas",
      "Tables",
      "Videos"
    ],
    "policy_type": [
      "PG"
    ],
    "task_mappings": [
      "exp"
    ],
    "autonomous_transfer": 0,
    "is_deep_rl": 1,
    "tags": [
      "Robotics",
      "Simulation",
      "Mujoco",
      "Button",
      "Pull",
      "Push",
      "MultiAgent"
    ],
    "allowed_learner": [
      "TD"
    ],
    "country": [
      "USA"
    ],
    "uni": [
      "University of California",
      "OpenAI"
    ],
    "department": [
      "Electrical Engineering and Computer Science",
      "Industry",
      "Uni",
      "Colab"
    ],
    "source_task_selection": [
      "all"
    ],
    "was_in_survey": [
      "mag",
      "silva",
      "zhu"
    ],
    "in_title": [
      "t",
      "r",
      "l"
    ],
    "in_abs": [
      "t",
      "r",
      "l",
      "k",
      "s"
    ],
    "in_contet": [
      "t",
      "r",
      "l",
      "k",
      "s"
    ],
    "rejected_or_unpublished": null,
    "duplication_status": null,
    "paper_for_thesis": 0,
    "paper_available": 1,
    "behind_paywall": 0,
    "paywall_circumventable": 0,
    "author_names": [
      "Pieter Abbeel",
      "Sergey Levine",
      "Abhishek Gupta",
      "Coline Devin",
      "YuXuan Liu"
    ]
  },
  {
    "id": 2962985403,
    "title": "Universal Successor Representations for Transfer Reinforcement Learning",
    "abst": null,
    "url": "https://openreview.net/pdf?id=HJ_CpYyDz",
    "lang": "en",
    "authors": [
      161269817,
      2098772737,
      2798256004
    ],
    "fos": [
      41008148,
      75306776,
      119857082,
      154945302,
      97541855
    ],
    "journals": [],
    "conferences": [
      2783520570
    ],
    "conference_series": [
      2584161585
    ],
    "references": [],
    "filter_matches": [
      "tr",
      "tl",
      "trl",
      "rl"
    ],
    "rank": 18637,
    "citation_count": 3,
    "estimated_citation_count": 3,
    "publication_date": "2018-02-12",
    "found_in": 1,
    "transfer_experiment_type": [
      "s_i",
      "s_f",
      "levels"
    ],
    "transfer_experiment_subtype": [
      "multi"
    ],
    "transfer_data_type": [
      "r",
      "fea"
    ],
    "transfer_performance_metrics": [
      "ap",
      "tr",
      "tt",
      "j"
    ],
    "implementation": [
      "Custom",
      "CS",
      "Pseudo",
      "Formulas",
      "Figures"
    ],
    "policy_type": [
      "USRA",
      "AC"
    ],
    "task_mappings": [
      "exp"
    ],
    "autonomous_transfer": 0,
    "is_deep_rl": 1,
    "tags": [
      "2D",
      "Navigation",
      "Grid",
      "Simulation",
      "Successor"
    ],
    "allowed_learner": [
      "TD"
    ],
    "country": [
      "USA",
      "Canada"
    ],
    "uni": [
      "University of Alberta",
      "University of Montreal"
    ],
    "department": [
      "Computer Science"
    ],
    "source_task_selection": [
      "all"
    ],
    "was_in_survey": [
      "mag",
      "zhu"
    ],
    "in_title": [
      "t",
      "r",
      "l"
    ],
    "in_abs": [
      "t",
      "r",
      "l",
      "k"
    ],
    "in_contet": [
      "t",
      "r",
      "l",
      "k"
    ],
    "rejected_or_unpublished": null,
    "duplication_status": null,
    "paper_for_thesis": 0,
    "paper_available": 1,
    "behind_paywall": 0,
    "paywall_circumventable": 0,
    "author_names": [
      "Yoshua Bengio",
      "Junfeng Wen",
      "Chen Ma"
    ]
  },
  {
    "id": 2963611966,
    "title": "DARLA: improving zero-shot transfer in reinforcement learning",
    "abst": "Domain adaptation is an important open problem in deep reinforcement learning (RL). In many scenarios of interest data is hard to obtain, so agents may learn a source policy in a setting where data is readily available, with the hope that it generalises well to the target domain. We propose a new multi-stage RL agent, DARLA (DisentAngled Representation Learning Agent), which learns to see before learning to act. DARLA&#039;s vision is based on learning a disentangled representation of the observed environment. Once DARLA can see, it is able to acquire source policies that are robust to many domain shifts - even with no access to the target domain. DARLA significantly outperforms conventional baselines in zero-shot domain adaptation scenarios, an effect that holds across a variety of RL environments (Jaco arm, DeepMind Lab) and base RL algorithms (DQN, A3C and EC).",
    "url": "https://www.arxiv-vanity.com/papers/1707.08475/",
    "lang": "en",
    "authors": [
      30689654,
      1995076491,
      2277055860,
      2518520521,
      2642523356,
      2689643194,
      2706928120,
      2720964890,
      2734332924
    ],
    "fos": [
      59404180,
      41008148,
      154945302,
      2777185003,
      97541855,
      119857082,
      2776434776
    ],
    "journals": [],
    "conferences": [
      2330412348
    ],
    "conference_series": [
      1180662882
    ],
    "references": [
      2966661,
      203338875,
      1494871301,
      1515851193,
      1522301498,
      1691728462,
      1771410628,
      1782879880,
      1786904711,
      1906772730,
      1909320841,
      1915328033,
      1959608418,
      1978119203,
      1994618660,
      2024390895,
      2047057213,
      2076927265,
      2077534194,
      2099647287,
      2100377190,
      2119051448,
      2119567691,
      2121103318,
      2145094598,
      2145339207,
      2157617585,
      2158282517,
      2158782408,
      2163922914,
      2165698076,
      2173248099,
      2174786457,
      2202109488,
      2271840356,
      2281112906,
      2294748366,
      2342877626,
      2434741482,
      2521274174,
      2534269850,
      2551887912,
      2561179021,
      2605102758,
      2605368761,
      2610686804,
      2617317556,
      2618530766,
      2751158183,
      2751842161,
      2753738274,
      2949480225,
      2949514005,
      2951475414,
      2953152866,
      2963007778,
      2963174698,
      2963184621,
      2963634205,
      2963829960,
      2963946410,
      2964294881
    ],
    "filter_matches": [
      "tr",
      "tl",
      "trl",
      "rl"
    ],
    "rank": 18670,
    "citation_count": 28,
    "estimated_citation_count": 36,
    "publication_date": "2017-08-06",
    "found_in": 1,
    "transfer_experiment_type": [
      "t",
      "s_i",
      "s_f",
      "levels"
    ],
    "transfer_experiment_subtype": [
      "diff-no",
      "sim2real"
    ],
    "transfer_data_type": [
      "fea",
      "curriculum"
    ],
    "transfer_performance_metrics": [
      "ap",
      "tr",
      "tt",
      "j"
    ],
    "implementation": [
      "Custom",
      "CS",
      "Formulas",
      "Figures",
      "Tables",
      "Videos",
      "TensorFlow",
      "Mujoco",
      "DeepMind Lab"
    ],
    "policy_type": [
      "DQN",
      "A3C",
      "EC",
      "UNREAL"
    ],
    "task_mappings": [
      "exp"
    ],
    "autonomous_transfer": 1,
    "is_deep_rl": 1,
    "tags": [
      "Simulation",
      "Representation",
      "JacoArm",
      "DeepMind Lab",
      "Robotics"
    ],
    "allowed_learner": [
      "TD"
    ],
    "country": [
      "UK"
    ],
    "uni": [
      "DeepMind"
    ],
    "department": [
      "Industry"
    ],
    "source_task_selection": [
      "all"
    ],
    "was_in_survey": [
      "mag",
      "zhu"
    ],
    "in_title": [
      "t",
      "r",
      "l"
    ],
    "in_abs": [
      "r",
      "l"
    ],
    "in_contet": [
      "t",
      "r",
      "l",
      "k"
    ],
    "rejected_or_unpublished": null,
    "duplication_status": null,
    "paper_for_thesis": 0,
    "paper_available": 1,
    "behind_paywall": 0,
    "paywall_circumventable": 0,
    "author_names": [
      "Matthew Botvinick",
      "Alexander Pritzel",
      "Charles Blundell",
      "Andrei A. Rusu",
      "Loic Matthey",
      "Arka Pal",
      "Irina Higgins",
      "Alexander Lerchner",
      "Christopher P. Burgess"
    ]
  },
  {
    "id": 2012036715,
    "title": "Transfer of Learning by Composing Solutions of Elemental Sequential Tasks",
    "abst": "Although building sophisticated learning agents that operate in complex environments will require learning to perform multiple tasks, most applications of reinforcement learning have focused on single tasks. In this paper I consider a class of sequential decision tasks (SDTs), called composite sequential decision tasks, formed by temporally concatenating a number of elemental sequential decision tasks. Elemental SDTs cannot be decomposed into simpler SDTs. I consider a learning agent that has to learn to solve a set of elemental and composite SDTs. I assume that the structure of the composite tasks is unknown to the learning agent. The straightforward application of reinforcement learning to multiple tasks requires learning the tasks separately, which can waste computational resources, both memory and time. I present a new learning algorithm and a modular architecture that learns the decomposition of composite SDTs, and achieves transfer of learning by sharing the solutions of elemental SDTs across multiple composite SDTs. The solution of a composite SDT is constructed by computationally inexpensive modifications of the solutions of its constituent elemental SDTs. I provide a proof of one aspect of the learning algorithm.",
    "url": "https://rd.springer.com/chapter/10.1007/978-1-4615-3618-5_6",
    "lang": "en",
    "authors": [
      2102570927
    ],
    "fos": [
      2992882098,
      154945302,
      28006648,
      2988947689,
      150899416,
      11866591,
      2988486947,
      119857082,
      97541855,
      41008148
    ],
    "journals": [
      62148650
    ],
    "conferences": [],
    "conference_series": [],
    "references": [
      1253821906,
      1491843047,
      1500024457,
      1545148916,
      1557517019,
      1586172133,
      1640247718,
      1931792391,
      1979071892,
      1979500821,
      1996847178,
      2001729196,
      2035446426,
      2060587510,
      2091565802,
      2100677568,
      2110415190,
      2135630072,
      2135995262,
      2150884987,
      3011120880,
      3017143921
    ],
    "filter_matches": [
      "tl"
    ],
    "rank": 18877,
    "citation_count": 245,
    "estimated_citation_count": 425,
    "publication_date": "1992-05-01",
    "found_in": 1,
    "transfer_experiment_type": [
      "r"
    ],
    "transfer_experiment_subtype": [
      "same_all"
    ],
    "transfer_data_type": [
      "Q"
    ],
    "transfer_performance_metrics": [
      "ap",
      "tr"
    ],
    "implementation": [
      "Custom",
      "CS",
      "Formulas",
      "Figures",
      "Tables"
    ],
    "policy_type": [
      "Q",
      "CQL"
    ],
    "task_mappings": [
      "N/A"
    ],
    "autonomous_transfer": 0,
    "is_deep_rl": 0,
    "tags": [
      "2D",
      "Grid",
      "Navigation",
      "Simulation"
    ],
    "allowed_learner": [
      "TD"
    ],
    "country": [
      "USA"
    ],
    "uni": [
      "University of Massachusetts"
    ],
    "department": [
      "Computer Science"
    ],
    "source_task_selection": [
      "all"
    ],
    "was_in_survey": [
      "taylor"
    ],
    "in_title": [
      "t",
      "l"
    ],
    "in_abs": [
      "t",
      "r",
      "l"
    ],
    "in_contet": [
      "t",
      "r",
      "l"
    ],
    "rejected_or_unpublished": null,
    "duplication_status": null,
    "paper_for_thesis": 0,
    "paper_available": 1,
    "behind_paywall": 0,
    "paywall_circumventable": 0,
    "author_names": [
      "Satinder Singh"
    ]
  },
  {
    "id": 1492014007,
    "title": "Building portable options: skill transfer in reinforcement learning",
    "abst": "The options framework provides methods for reinforcement learning agents to build new high-level skills. However, since options are usually learned in the same state space as the problem the agent is solving, they cannot be used in other tasks that are similar but have different state spaces. We introduce the notion of learning options in agentspace, the space generated by a feature set that is present and retains the same semantics across successive problem instances, rather than in problemspace. Agent-space options can be reused in later tasks that share the same agent-space but have different problem-spaces. We present experimental results demonstrating the use of agent-space options in building transferrable skills, and show that they perform best when used in conjunction with problem-space options.",
    "url": "http://ijcai.org/Proceedings/07/Papers/144.pdf",
    "lang": "en",
    "authors": [
      1965389844,
      1997216218
    ],
    "fos": [
      97541855,
      2986740527,
      119857082,
      154945302,
      2986635905,
      72434380,
      41008148,
      124246873
    ],
    "journals": [],
    "conferences": [
      2793510348
    ],
    "conference_series": [
      1203999783
    ],
    "references": [
      92265916,
      143164768,
      199176224,
      1486707268,
      1494114146,
      1510402218,
      1514587017,
      1515851193,
      1534331386,
      1540462738,
      1586944634,
      1592847719,
      1598052524,
      1612195517,
      1968768508,
      2079247031,
      2085662847,
      2090170171,
      2107628283,
      2109910161,
      2114451917,
      2121517924,
      2121863487,
      2139612737,
      2143435603,
      2158548602,
      2160808139,
      2170198047
    ],
    "filter_matches": [
      "tr",
      "tl",
      "trl",
      "rl"
    ],
    "rank": 18897,
    "citation_count": 148,
    "estimated_citation_count": 225,
    "publication_date": "2007-01-06",
    "found_in": 1,
    "transfer_experiment_type": [
      "p"
    ],
    "transfer_experiment_subtype": [
      "diff-no"
    ],
    "transfer_data_type": [
      "options"
    ],
    "transfer_performance_metrics": [
      "j",
      "tr"
    ],
    "implementation": [
      "Custom",
      "CS",
      "Figures",
      "Formulas"
    ],
    "policy_type": [
      "SARSA"
    ],
    "task_mappings": [
      "N/A"
    ],
    "autonomous_transfer": 0,
    "is_deep_rl": 0,
    "tags": [
      "2D",
      "Navigation",
      "Collect",
      "Simulation"
    ],
    "allowed_learner": [
      "TD",
      "H"
    ],
    "country": [
      "USA"
    ],
    "uni": [
      "University of Massachusetts"
    ],
    "department": [
      "Computer Science"
    ],
    "source_task_selection": [
      "h"
    ],
    "was_in_survey": [
      "taylor",
      "mag",
      "lazaric",
      "bone"
    ],
    "in_title": [
      "t",
      "r",
      "l"
    ],
    "in_abs": [
      "t",
      "l"
    ],
    "in_contet": [
      "t",
      "r",
      "l"
    ],
    "rejected_or_unpublished": null,
    "duplication_status": null,
    "paper_for_thesis": 0,
    "paper_available": 1,
    "behind_paywall": 0,
    "paywall_circumventable": 0,
    "author_names": [
      "George Konidaris",
      "Andrew G. Barto"
    ]
  },
  {
    "id": 2164114810,
    "title": "Cross-domain transfer for reinforcement learning",
    "abst": "A typical goal for transfer learning algorithms is to utilize knowledge gained in a source task to learn a target task faster. Recently introduced transfer methods in reinforcement learning settings have shown considerable promise, but they typically transfer between pairs of very similar tasks. This work introduces Rule Transfer, a transfer algorithm that first learns rules to summarize a source task policy and then leverages those rules to learn faster in a target task. This paper demonstrates that Rule Transfer can effectively speed up learning in Keepaway, a benchmark RL problem in the robot soccer domain, based on experience from source tasks in the gridworld domain. We empirically show, through the use of three distinct transfer metrics, that Rule Transfer is effective across these domains.",
    "url": "https://www.researchgate.net/profile/Matthew_Taylor12/publication/221345060_Cross-domain_transfer_for_reinforcement_learning/links/0fcfd5114478d8d544000000.pdf",
    "lang": "en",
    "authors": [
      2147180669,
      2148762994
    ],
    "fos": [
      119857082,
      199190896,
      154945302,
      41008148,
      90509273,
      150899416,
      28006648,
      97541855,
      188888258,
      68339613,
      77075516
    ],
    "journals": [],
    "conferences": [
      2784560012
    ],
    "conference_series": [
      1180662882
    ],
    "references": [
      36691172,
      1506146479,
      1515851193,
      1570448133,
      1670263352,
      2014512216,
      2098723043,
      2113913482,
      2140584963,
      2169659168,
      2966207845
    ],
    "filter_matches": [
      "tr",
      "tl",
      "trl",
      "rl"
    ],
    "rank": 18948,
    "citation_count": 116,
    "estimated_citation_count": 174,
    "publication_date": "2007-06-20",
    "found_in": 1,
    "transfer_experiment_type": [
      "a",
      "r",
      "v"
    ],
    "transfer_experiment_subtype": [
      "diff-it"
    ],
    "transfer_data_type": [
      "rule",
      "advisor"
    ],
    "transfer_performance_metrics": [
      "j",
      "tt",
      "tr"
    ],
    "implementation": [
      "Custom",
      "CS",
      "Figures",
      "Formulas",
      "Pseudo",
      "Tables"
    ],
    "policy_type": [
      "Q"
    ],
    "task_mappings": [
      "sup"
    ],
    "autonomous_transfer": 0,
    "is_deep_rl": 0,
    "tags": [
      "Simulation",
      "RoboCup",
      "2D",
      "Navigation",
      "Grid",
      "KnightJoust",
      "Ringworld",
      "MultiAgent",
      "KeepAway",
      "3v2"
    ],
    "allowed_learner": [
      "TD",
      "any"
    ],
    "country": [
      "USA"
    ],
    "uni": [
      "The University of Texas"
    ],
    "department": [
      "Computer Science"
    ],
    "source_task_selection": [
      "h"
    ],
    "was_in_survey": [
      "taylor",
      "mag",
      "bone"
    ],
    "in_title": [
      "t",
      "r",
      "l"
    ],
    "in_abs": [
      "t",
      "r",
      "l",
      "k"
    ],
    "in_contet": [
      "t",
      "r",
      "l",
      "k",
      "s"
    ],
    "rejected_or_unpublished": null,
    "duplication_status": null,
    "paper_for_thesis": 0,
    "paper_available": 1,
    "behind_paywall": 0,
    "paywall_circumventable": 0,
    "author_names": [
      "Peter Stone",
      "Matthew D. Taylor"
    ]
  },
  {
    "id": 2079247031,
    "title": "Autonomous shaping: knowledge transfer in reinforcement learning",
    "abst": "We introduce the use of learned shaping rewards in reinforcement learning tasks, where an agent uses prior experience on a sequence of tasks to learn a portable predictor that estimates intermediate rewards, resulting in accelerated learning in later tasks that are related but distinct. Such agents can be trained on a sequence of relatively easy tasks in order to develop a more informative measure of reward that can be transferred to improve performance on more difficult tasks without requiring a hand coded shaping function. We use a rod positioning task to show that this significantly improves performance even after a very brief training period.",
    "url": "http://portal.acm.org/citation.cfm?doid=1143844.1143906",
    "lang": "en",
    "authors": [
      1965389844,
      1997216218
    ],
    "fos": [
      119857082,
      28006648,
      97541855,
      154945302,
      3017400417,
      41008148,
      2993645114,
      2776960227,
      47932503
    ],
    "journals": [],
    "conferences": [
      2784769503
    ],
    "conference_series": [
      1180662882
    ],
    "references": [
      24477102,
      92265916,
      1499408472,
      1504212531,
      1507591516,
      1512407760,
      1562462135,
      1586944634,
      1777239053,
      1982948368,
      1996847178,
      2009533501,
      2021061679,
      2048226872,
      2071311198,
      2114451917,
      2121863487,
      2128905965,
      2130750514,
      2137351138,
      2143958939,
      2556741007,
      2911283634,
      2914048451
    ],
    "filter_matches": [
      "tr",
      "tl",
      "trl",
      "rl"
    ],
    "rank": 18982,
    "citation_count": 140,
    "estimated_citation_count": 211,
    "publication_date": "2006-06-25",
    "found_in": 1,
    "transfer_experiment_type": [
      "p"
    ],
    "transfer_experiment_subtype": [
      "diff-no"
    ],
    "transfer_data_type": [
      "r"
    ],
    "transfer_performance_metrics": [
      "j",
      "tr"
    ],
    "implementation": [
      "Custom",
      "CS",
      "Figures",
      "Formulas"
    ],
    "policy_type": [
      "SARSA"
    ],
    "task_mappings": [
      "N/A"
    ],
    "autonomous_transfer": 0,
    "is_deep_rl": 0,
    "tags": [
      "2D",
      "Navigation",
      "Simulation",
      "Homing"
    ],
    "allowed_learner": [
      "TD"
    ],
    "country": [
      "USA"
    ],
    "uni": [
      "University of Massachusetts"
    ],
    "department": [
      "Computer Science"
    ],
    "source_task_selection": [
      "h"
    ],
    "was_in_survey": [
      "taylor",
      "mag",
      "lazaric",
      "taylor-intertask",
      "bone",
      "silva",
      "zhu"
    ],
    "in_title": [
      "t",
      "r",
      "l",
      "k"
    ],
    "in_abs": [
      "t",
      "r",
      "l"
    ],
    "in_contet": [
      "t",
      "r",
      "l",
      "k"
    ],
    "rejected_or_unpublished": null,
    "duplication_status": null,
    "paper_for_thesis": 0,
    "paper_available": 1,
    "behind_paywall": 0,
    "paywall_circumventable": 0,
    "author_names": [
      "George Konidaris",
      "Andrew G. Barto"
    ]
  },
  {
    "id": 2169743339,
    "title": "Multi-task reinforcement learning: a hierarchical Bayesian approach",
    "abst": "We consider the problem of multi-task reinforcement learning, where the agent needs to solve a sequence of Markov Decision Processes (MDPs) chosen randomly from a fixed but unknown distribution. We model the distribution over MDPs using a hierarchical Bayesian infinite mixture model. For each novel MDP, we use the previously learned distribution as an informed prior for modelbased Bayesian reinforcement learning. The hierarchical Bayesian framework provides a strong prior that allows us to rapidly infer the characteristics of new environments based on previous environments, while the use of a nonparametric model allows us to quickly adapt to environments we have not encountered before. In addition, the use of infinite mixtures allows for the model to automatically learn the number of underlying MDP components. We evaluate our approach and show that it leads to significant speedups in convergence to an optimal policy after observing only a small number of tasks.",
    "url": "https://dl.acm.org/citation.cfm?doid=1273496.1273624",
    "lang": "en",
    "authors": [
      1993564419,
      2126535395,
      2139785505,
      2440006205
    ],
    "fos": [
      61224824,
      191413810,
      97541855,
      178980831,
      71923881,
      71983512,
      2776886580,
      41008148,
      119857082,
      154945302,
      106189395,
      107673813
    ],
    "journals": [],
    "conferences": [
      2784560012
    ],
    "conference_series": [
      1180662882
    ],
    "references": [
      1496855202,
      1515851193,
      1582436621,
      1591803298,
      2039522160,
      2071814471,
      2079247031,
      2080972498,
      2106953752,
      2121863487,
      2128775537,
      2132057084
    ],
    "filter_matches": [
      "rl"
    ],
    "rank": 19011,
    "citation_count": 169,
    "estimated_citation_count": 235,
    "publication_date": "2007-06-20",
    "found_in": 1,
    "transfer_experiment_type": [
      "r",
      "s_f"
    ],
    "transfer_experiment_subtype": [
      "multi"
    ],
    "transfer_data_type": [
      "pri"
    ],
    "transfer_performance_metrics": [
      "j",
      "tr"
    ],
    "implementation": [
      "Custom",
      "CS",
      "Pseudo",
      "Formulas",
      "Figures"
    ],
    "policy_type": [
      "Bayesian"
    ],
    "task_mappings": [
      "N/A"
    ],
    "autonomous_transfer": 0,
    "is_deep_rl": 0,
    "tags": [
      "2D",
      "Navigation",
      "Maze",
      "Grid"
    ],
    "allowed_learner": [
      "B"
    ],
    "country": [
      "USA"
    ],
    "uni": [
      "University of Massachusetts"
    ],
    "department": [
      "Electrical Engineering and Computer Sciences"
    ],
    "source_task_selection": [
      "all"
    ],
    "was_in_survey": [
      "taylor"
    ],
    "in_title": [
      "r",
      "l"
    ],
    "in_abs": [
      "r",
      "l"
    ],
    "in_contet": [
      "t",
      "r",
      "l",
      "k"
    ],
    "rejected_or_unpublished": null,
    "duplication_status": null,
    "paper_for_thesis": 0,
    "paper_available": 1,
    "behind_paywall": 0,
    "paywall_circumventable": 0,
    "author_names": [
      "Prasad Tadepalli",
      "Aaron Wilson",
      "Alan Fern",
      "Soumya Ray"
    ]
  },
  {
    "id": 2134153324,
    "title": "Generalizing plans to new environments in relational MDPs",
    "abst": "A longstanding goal in planning research is the ability to generalize plans developed for some set of environments to a new but similar environment, with minimal or no replanning. Such generalization can both reduce planning time and allow us to tackle larger domains than the ones tractable for direct planning. In this paper, we present an approach to the generalization problem based on a new framework of relational Markov Decision Processes (RMDPs). An RMDP can model a set of similar environments by representing objects as instances of different classes. In order to generalize plans to multiple environments, we define an approximate value function specified in terms of classes of objects and, in a multiagent setting, by classes of agents. This class-based approximate value function is optimized relative to a sampled subset of environments, and computed using an efficient linear programming method. We prove that a polynomial number of sampled environments suffices to achieve performance close to the performance achievable when optimizing over the entire space. Our experimental results show that our method generalizes plans successfully to new, significantly larger, environments, with minimal loss of performance relative to environment-specific planning. We demonstrate our approach on a real strategic computer war game.",
    "url": "http://www.ijcai.org/Past%20Proceedings/IJCAI-2003/PDF/144.pdf",
    "lang": "en",
    "authors": [
      1988556028,
      2167404190,
      2283966317,
      2290440925
    ],
    "fos": [
      2993270172,
      41008148,
      106189395,
      41045048,
      119857082,
      154945302,
      126255220,
      14646407,
      177148314
    ],
    "journals": [],
    "conferences": [
      2792991336
    ],
    "conference_series": [
      1203999783
    ],
    "references": [
      11088338,
      41895731,
      1515851193,
      1557798492,
      1560550898,
      1631187438,
      1654728867,
      1800916125,
      1967346767,
      2020149918,
      2040766536,
      2096600060,
      2096622112,
      2110111529,
      2121517924,
      2121863487,
      2134779831,
      2149385746,
      2158479468
    ],
    "filter_matches": [
      "per"
    ],
    "rank": 19050,
    "citation_count": 149,
    "estimated_citation_count": 226,
    "publication_date": "2003-08-09",
    "found_in": 1,
    "transfer_experiment_type": [
      "#"
    ],
    "transfer_experiment_subtype": [
      "diff-no"
    ],
    "transfer_data_type": [
      "Q"
    ],
    "transfer_performance_metrics": [
      "j"
    ],
    "implementation": [
      "Custom",
      "CS",
      "Figures",
      "Formulas",
      "Videos",
      "Theorem",
      "Lemma"
    ],
    "policy_type": [
      "CPLEX"
    ],
    "task_mappings": [
      "N/A"
    ],
    "autonomous_transfer": 0,
    "is_deep_rl": 0,
    "tags": [
      "RTS",
      "Freecraft",
      "Games",
      "VideoGames",
      "Simulation"
    ],
    "allowed_learner": [
      "LP"
    ],
    "country": [
      "USA"
    ],
    "uni": [
      "Stanford University"
    ],
    "department": [
      "Computer Science"
    ],
    "source_task_selection": [
      "h"
    ],
    "was_in_survey": [
      "taylor"
    ],
    "in_title": [],
    "in_abs": [],
    "in_contet": [
      "r",
      "l"
    ],
    "rejected_or_unpublished": null,
    "duplication_status": null,
    "paper_for_thesis": 0,
    "paper_available": 1,
    "behind_paywall": 0,
    "paywall_circumventable": 0,
    "author_names": [
      "Carlos Guestrin",
      "Daphne Koller"
    ]
  },
  {
    "id": 2089561656,
    "title": "State abstraction for programmable reinforcement learning agents",
    "abst": "Safe state abstraction in reinforcement learning allows an agent to ignore aspects of its current state that are irrelevant to its current decision, and therefore speeds up dynamic programming and learning. This paper explores safe state abstraction in hierarchical reinforcement learning, where learned behaviors must conform to a given partial, hierarchical program. Unlike previous approaches to this problem, our methods yield significant state abstraction while maintaining hierarchical optimality, i.e., optimality among all policies consistent with the partial program. We show how to achieve this for a partial programming language that is essentially Lisp augmented with nondeterministic constructs. We demonstrate our methods on two variants of Dietterich&#039;s taxi domain, showing how state abstraction and hierarchical optimality result in faster learning of better policies and enable the transfer of learned skills from one problem to another.",
    "url": "http://dl.acm.org/citation.cfm?id=894129",
    "lang": "en",
    "authors": [
      2128362942,
      2805045621
    ],
    "fos": [
      119857082,
      37404715,
      97541855,
      124304363,
      47932503,
      190883126,
      154945302,
      176181172,
      199190896,
      41008148
    ],
    "journals": [],
    "conferences": [
      2785605827
    ],
    "conference_series": [
      1184914352
    ],
    "references": [
      112321980,
      1488730473,
      1552562496,
      1650504995,
      1777239053,
      1982678075,
      2039153121,
      2107726111,
      2109910161,
      2121517924,
      2156067405,
      2158548602,
      2341171179
    ],
    "filter_matches": [
      "rl"
    ],
    "rank": 19113,
    "citation_count": 169,
    "estimated_citation_count": 278,
    "publication_date": "2002-07-28",
    "found_in": 1,
    "transfer_experiment_type": [
      "r",
      "s_i",
      "s_f"
    ],
    "transfer_experiment_subtype": [
      "same_all"
    ],
    "transfer_data_type": [
      "pi"
    ],
    "transfer_performance_metrics": [
      "tr"
    ],
    "implementation": [
      "Custom",
      "CS",
      "Figures",
      "Formulas",
      "Pseudo",
      "Tables",
      "Theorem"
    ],
    "policy_type": [
      "Q",
      "MAXQ",
      "Alisp"
    ],
    "task_mappings": [
      "N/A"
    ],
    "autonomous_transfer": 0,
    "is_deep_rl": 0,
    "tags": [
      "2D",
      "Grid",
      "Navigation",
      "Taxi",
      "Simulation"
    ],
    "allowed_learner": [
      "H"
    ],
    "country": [
      "USA"
    ],
    "uni": [
      "UC Berkley"
    ],
    "department": [
      "Computer Science"
    ],
    "source_task_selection": [
      "h"
    ],
    "was_in_survey": [
      "taylor"
    ],
    "in_title": [
      "r",
      "l"
    ],
    "in_abs": [
      "t",
      "r",
      "l"
    ],
    "in_contet": [
      "t",
      "r",
      "l"
    ],
    "rejected_or_unpublished": null,
    "duplication_status": null,
    "paper_for_thesis": 0,
    "paper_available": 1,
    "behind_paywall": 0,
    "paywall_circumventable": 0,
    "author_names": [
      "Stuart Russell",
      "David Andre"
    ]
  },
  {
    "id": 2166798247,
    "title": "Transfer learning in real-time strategy games using hybrid CBR/RL",
    "abst": "The goal of transfer learning is to use the knowledge acquired in a set of source tasks to improve performance in a related but previously unseen target task. In this paper, we present a multilayered architecture named CAse-Based Reinforcement Learner (CARL). It uses a novel combination of Case-Based Reasoning (CBR) and Reinforcement Learning (RL) to achieve transfer while playing against the Game AI across a variety of scenarios in MadRTSTM, a commercial Real Time Strategy game. Our experiments demonstrate that CARL not only performs well on individual tasks but also exhibits significant performance gains when allowed to transfer knowledge from previous tasks.",
    "url": "https://dl.acm.org/citation.cfm?id=1625275.1625444",
    "lang": "en",
    "authors": [
      2100123673,
      2106580818,
      2109056120,
      2126228213,
      2135226159,
      2251311143
    ],
    "fos": [
      97541855,
      28006648,
      2781170869,
      119857082,
      150899416,
      67203356,
      74678566,
      154945302,
      41008148
    ],
    "journals": [],
    "conferences": [
      2793510348
    ],
    "conference_series": [
      1203999783
    ],
    "references": [
      1487906362,
      1500151553,
      1515851193,
      1565097357,
      1592209052,
      2100677568,
      2126385963,
      2132057084,
      2140095144,
      2154549708,
      2312609093
    ],
    "filter_matches": [
      "tl"
    ],
    "rank": 19167,
    "citation_count": 110,
    "estimated_citation_count": 183,
    "publication_date": "2007-01-06",
    "found_in": 1,
    "transfer_experiment_type": [
      "#"
    ],
    "transfer_experiment_subtype": [
      "diff-no"
    ],
    "transfer_data_type": [
      "Q"
    ],
    "transfer_performance_metrics": [
      "j",
      "tr"
    ],
    "implementation": [
      "Custom",
      "CS",
      "Figures",
      "Tables",
      "Formulas"
    ],
    "policy_type": [
      "Q"
    ],
    "task_mappings": [
      "N/A"
    ],
    "autonomous_transfer": 0,
    "is_deep_rl": 0,
    "tags": [
      "RTS",
      "MadRTS",
      "Games",
      "VideoGames",
      "Simulation"
    ],
    "allowed_learner": [
      "TD",
      "CBR"
    ],
    "country": [
      "USA"
    ],
    "uni": [
      "Georgia Institute of Technology"
    ],
    "department": [
      "College of Computing"
    ],
    "source_task_selection": [
      "h"
    ],
    "was_in_survey": [
      "taylor"
    ],
    "in_title": [
      "t",
      "l",
      "rl"
    ],
    "in_abs": [
      "t",
      "r",
      "l",
      "k"
    ],
    "in_contet": [
      "t",
      "r",
      "l",
      "k"
    ],
    "rejected_or_unpublished": null,
    "duplication_status": null,
    "paper_for_thesis": 0,
    "paper_available": 1,
    "behind_paywall": 0,
    "paywall_circumventable": 0,
    "author_names": [
      "Michael P. Holmes",
      "Charles L. Isbell",
      "Ashwin Ram",
      "Juan Carlos Santamaria",
      "Arya Irani",
      "Manu Sharma"
    ]
  },
  {
    "id": 2795717084,
    "title": "StarCraft Micromanagement with Reinforcement Learning and Curriculum Transfer Learning",
    "abst": "Real-time strategy games have been an important field of game artificial intelligence in recent years. This paper presents a reinforcement learning and curriculum transfer learning method to control multiple units in StarCraft micromanagement. We define an efficient state representation, which breaks down the complexity caused by the large state space in the game environment. Then a parameter sharing multi-agent gradientdescent Sarsa({lambda}) (PS-MAGDS) algorithm is proposed to train the units. The learning policy is shared among our units to encourage cooperative behaviors. We use a neural network as a function approximator to estimate the action-value function, and propose a reward function to help units balance their move and attack. In addition, a transfer learning method is used to extend our model to more difficult scenarios, which accelerates the training process and improves the learning performance. In small scale scenarios, our units successfully learn to combat and defeat the built-in AI with 100% win rates. In large scale scenarios, curriculum transfer learning method is used to progressively train a group of units, and shows superior performance over some baseline methods in target scenarios. With reinforcement learning and curriculum transfer learning, our units are able to learn appropriate strategies in StarCraft micromanagement scenarios.",
    "url": "http://export.arxiv.org/abs/1804.00810",
    "lang": "en",
    "authors": [
      2097402029,
      2167395857,
      2574426535
    ],
    "fos": [
      2781329980,
      150899416,
      47177190,
      72434380,
      119857082,
      2989414621,
      97541855,
      41008148,
      50644808,
      154945302
    ],
    "journals": [
      2596500785
    ],
    "conferences": [],
    "conference_series": [],
    "references": [
      1542941925,
      1641379095,
      1658008008,
      1660754132,
      1777239053,
      1974221555,
      2026615874,
      2031748952,
      2041367235,
      2044504853,
      2097381042,
      2104733512,
      2107726111,
      2113913482,
      2121863487,
      2145339207,
      2155968351,
      2165698076,
      2173564293,
      2184201647,
      2296073425,
      2312229020,
      2338351427,
      2402402867,
      2405379562,
      2518713116,
      2526283374,
      2530887700,
      2535561795,
      2538000344,
      2550182557,
      2567015638,
      2576264401,
      2604279009,
      2623431351,
      2734333713,
      2750605955,
      2751516180,
      2756196406,
      2768629321,
      2770581177,
      2949608212,
      2950471160,
      2962938168,
      2963430173,
      2963477884,
      2963864421,
      2964043796
    ],
    "filter_matches": [
      "tr",
      "tl",
      "trl",
      "rl"
    ],
    "rank": 19179,
    "citation_count": 0,
    "estimated_citation_count": 0,
    "publication_date": "2018-04-03",
    "found_in": 1,
    "transfer_experiment_type": [
      "s_i",
      "s_f",
      "levels",
      "t"
    ],
    "transfer_experiment_subtype": [
      "multi"
    ],
    "transfer_data_type": [
      "curriculum"
    ],
    "transfer_performance_metrics": [
      "j",
      "tr",
      "tt",
      "ap"
    ],
    "implementation": [
      "Custom",
      "OSS",
      "bwapi",
      "Pseudo",
      "Figures",
      "Formulas",
      "Tables"
    ],
    "policy_type": [
      "PS-MAGDS",
      "SARSA"
    ],
    "task_mappings": [
      "N/A"
    ],
    "autonomous_transfer": 0,
    "is_deep_rl": 1,
    "tags": [
      "Simulation",
      "Games",
      "VideoGames",
      "StarCraft",
      "Micromanagement",
      "Curriculum",
      "MultiAgent"
    ],
    "allowed_learner": [
      "TD",
      "H",
      "MB"
    ],
    "country": [
      "China"
    ],
    "uni": [
      "Chinese Academy of Sciences",
      "State Key Laboratory of Management and Control for Complex Systems"
    ],
    "department": [
      "Institute of Automation"
    ],
    "source_task_selection": [
      "all"
    ],
    "was_in_survey": [
      "mag",
      "cur"
    ],
    "in_title": [
      "t",
      "r",
      "l"
    ],
    "in_abs": [
      "t",
      "r",
      "l"
    ],
    "in_contet": [
      "t",
      "r",
      "l",
      "k",
      "s"
    ],
    "rejected_or_unpublished": null,
    "duplication_status": null,
    "paper_for_thesis": 0,
    "paper_available": 1,
    "behind_paywall": 0,
    "paywall_circumventable": 0,
    "author_names": [
      "Yuanheng Zhu",
      "Dongbin Zhao",
      "Kun Shao"
    ]
  },
  {
    "id": 2126565096,
    "title": "Behavior transfer for value-function-based reinforcement learning",
    "abst": "Temporal difference (TD) learning methods [22] have become popular reinforcement learning techniques in recent years. TD methods have had some experimental successes and have been shown to exhibit some desirable properties in theory, but have often been found very slow in practice. A key feature of TD methods is that they represent policies in terms of value functions. In this paper we introduce behavior transfer, a novel approach to speeding up TD learning by transferring the learned value function from one task to a second related task. We present experimental results showing that autonomous learners are able to learn one multiagent task and then use behavior transfer to markedly reduce the total training time for a more complex task.",
    "url": "http://dl.acm.org/citation.cfm?doid=1082473.1082482",
    "lang": "en",
    "authors": [
      2147180669,
      2148762994
    ],
    "fos": [
      3018071011,
      97541855,
      154945302,
      196340769,
      3020055732,
      137293760,
      14646407,
      119857082,
      33954974,
      34413123,
      41008148
    ],
    "journals": [],
    "conferences": [
      2785532900
    ],
    "conference_series": [
      1168671587
    ],
    "references": [
      24477102,
      1495978126,
      1515851193,
      1579667258,
      1584313244,
      1585603966,
      1777239053,
      1799762961,
      2012036715,
      2089561656,
      2097113539,
      2104641222,
      2119567691,
      2119717200,
      2128353691,
      2128547596,
      2134153324,
      2161879907,
      2169659168,
      2198041288,
      2396715201,
      3020831056
    ],
    "filter_matches": [
      "tr",
      "tl",
      "trl",
      "rl"
    ],
    "rank": 19185,
    "citation_count": 77,
    "estimated_citation_count": 120,
    "publication_date": "2005-07-25",
    "found_in": 1,
    "transfer_experiment_type": [
      "v",
      "#"
    ],
    "transfer_experiment_subtype": [
      "diff-it"
    ],
    "transfer_data_type": [
      "Q"
    ],
    "transfer_performance_metrics": [
      "j",
      "tt"
    ],
    "implementation": [
      "Custom",
      "CS",
      "RoboCup",
      "Formulas",
      "Videos",
      "Figures",
      "Tables"
    ],
    "policy_type": [
      "SARSA",
      "CMAC",
      "SMDP"
    ],
    "task_mappings": [
      "sup"
    ],
    "autonomous_transfer": 0,
    "is_deep_rl": 0,
    "tags": [
      "2D",
      "Simulation",
      "RoboCup",
      "KeepAway",
      "MultiAgent",
      "4v3",
      "3v2"
    ],
    "allowed_learner": [
      "TD"
    ],
    "country": [
      "USA"
    ],
    "uni": [
      "The University of Texas"
    ],
    "department": [
      "Compouter Sciences"
    ],
    "source_task_selection": [
      "h"
    ],
    "was_in_survey": [
      "mag",
      "lazaric",
      "cur"
    ],
    "in_title": [
      "t",
      "r",
      "l"
    ],
    "in_abs": [
      "t",
      "r",
      "l"
    ],
    "in_contet": [
      "t",
      "r",
      "l",
      "k"
    ],
    "rejected_or_unpublished": null,
    "duplication_status": null,
    "paper_for_thesis": 0,
    "paper_available": 1,
    "behind_paywall": 0,
    "paywall_circumventable": 0,
    "author_names": [
      "Peter Stone",
      "Matthew D. Taylor"
    ]
  },
  {
    "id": 2950819666,
    "title": "Zero-shot Deep Reinforcement Learning Driving Policy Transfer for Autonomous Vehicles based on Robust Control",
    "abst": "Although deep reinforcement learning (deep RL) methods have lots of strengths that are favorable if applied to autonomous driving, real deep RL applications in autonomous driving have been slowed down by the modeling gap between the source (training) domain and the target (deployment) domain. Unlike current policy transfer approaches, which generally limit to the usage of uninterpretable neural network representations as the transferred features, we propose to transfer concrete kinematic quantities in autonomous driving. The proposed robust-control-based (RC) generic transfer architecture, which we call RL-RC, incorporates a transferable hierarchical RL trajectory planner and a robust tracking controller based on disturbance observer (DOB). The deep RL policies trained with known nominal dynamics model are transfered directly to the target domain, DOB-based robust tracking control is applied to tackle the modeling gap including the vehicle dynamics errors and the external disturbances such as side forces. We provide simulations validating the capability of the proposed method to achieve zero-shot transfer across multiple driving scenarios such as lane keeping, lane changing and obstacle avoidance.",
    "url": "http://export.arxiv.org/abs/1812.03216",
    "lang": "en",
    "authors": [
      243239543,
      2770030547,
      2896824728
    ],
    "fos": [
      6683253,
      50644808,
      65244806,
      79699506,
      31531917,
      79487989,
      126255220,
      13662910,
      33923547,
      39920418,
      97541855
    ],
    "journals": [
      2597173376
    ],
    "conferences": [],
    "conference_series": [],
    "references": [
      1544016679,
      1990083550,
      2025420234,
      2139137304,
      2173248099,
      2342840547,
      2343568200,
      2426267443,
      2529477964,
      2530944449,
      2563338958,
      2586431331,
      2736601468,
      2739211902,
      2767050701,
      2768925084,
      2963280855,
      2964043796
    ],
    "filter_matches": [
      "tr",
      "tl",
      "trl",
      "rl"
    ],
    "rank": 19239,
    "citation_count": 0,
    "estimated_citation_count": 0,
    "publication_date": "2018-12-07",
    "found_in": 1,
    "transfer_experiment_type": [
      "t",
      "s_i",
      "s_f",
      "s",
      "levels"
    ],
    "transfer_experiment_subtype": [
      "diff-no"
    ],
    "transfer_data_type": [
      "fea",
      "pi"
    ],
    "transfer_performance_metrics": [
      "ap",
      "tr",
      "j",
      "tt"
    ],
    "implementation": [
      "Custom",
      "CS",
      "Formulas",
      "Pseudo",
      "Figures",
      "Tables",
      "Videos"
    ],
    "policy_type": [
      "RL-RC"
    ],
    "task_mappings": [
      "N/A"
    ],
    "autonomous_transfer": 0,
    "is_deep_rl": 1,
    "tags": [
      "Robotics",
      "Autonomous",
      "Driving",
      "Vehicle",
      "Simulation"
    ],
    "allowed_learner": [
      "H",
      "TD"
    ],
    "country": [
      "USA"
    ],
    "uni": [
      "University of California",
      "Berkeley DeepDrive"
    ],
    "department": [
      "Mechanical Engineering"
    ],
    "source_task_selection": [
      "lib"
    ],
    "was_in_survey": [
      "mag"
    ],
    "in_title": [
      "t",
      "r",
      "l"
    ],
    "in_abs": [
      "t",
      "r",
      "l"
    ],
    "in_contet": [
      "t",
      "r",
      "l"
    ],
    "rejected_or_unpublished": null,
    "duplication_status": null,
    "paper_for_thesis": 0,
    "paper_available": 1,
    "behind_paywall": 0,
    "paywall_circumventable": 0,
    "author_names": [
      "Masayoshi Tomizuka",
      "Zhuo Xu",
      "Chen Tang"
    ]
  },
  {
    "id": 2795341696,
    "title": "Reinforcement learning for non-prehensile manipulation: Transfer from simulation to physical system",
    "abst": "Reinforcement learning has emerged as a promising methodology for training robot controllers. However, most results have been limited to simulation due to the need for a large number of samples and the lack of automated-yet-safe data collection methods. Model-based reinforcement learning methods provide an avenue to circumvent these challenges, but the traditional concern has been the mismatch between the simulator and the real world. Here, we show that control policies learned in simulation can successfully transfer to a physical system, composed of three Phantom robots pushing an object to various desired target positions. We use a modified form of the natural policy gradient algorithm for learning, applied to a carefully identified simulation model. The resulting policies, trained entirely in simulation, work well on the physical system without additional training. In addition, we show that training with an ensemble of models makes the learned policies more robust to modeling errors, thus compensating for difficulties in system identification.",
    "url": "https://au.arxiv.org/pdf/1803.10371",
    "lang": "en",
    "authors": [
      2024122153,
      2028747364,
      2035686109,
      2342102202,
      2569238406
    ],
    "fos": [
      133731056,
      133462117,
      104293457,
      90509273,
      127413603,
      136380597,
      97541855,
      116672817,
      119247159
    ],
    "journals": [
      2596519289
    ],
    "conferences": [],
    "conference_series": [],
    "references": [
      1555368087,
      1594783240,
      1676686791,
      1820821744,
      1964946446,
      1966784014,
      1975230295,
      1977655452,
      1998172110,
      2042408133,
      2087617385,
      2107726111,
      2119717200,
      2124695578,
      2130801532,
      2132602063,
      2140135625,
      2144351850,
      2158782408,
      2158796564,
      2165693128,
      2186601217,
      2198582666,
      2205975260,
      2209762605,
      2416477367,
      2529477964,
      2529601334,
      2529658650,
      2575705757,
      2615735215,
      2736601468,
      2757631751,
      2767050701,
      2949608212
    ],
    "filter_matches": [
      "tr",
      "tl",
      "trl",
      "rl"
    ],
    "rank": 19243,
    "citation_count": 5,
    "estimated_citation_count": 5,
    "publication_date": "2018-03-28",
    "found_in": 1,
    "transfer_experiment_type": [
      "t"
    ],
    "transfer_experiment_subtype": [
      "sim2real"
    ],
    "transfer_data_type": [
      "pi"
    ],
    "transfer_performance_metrics": [
      "j"
    ],
    "implementation": [
      "Custom",
      "CS",
      "Formulas",
      "Pseudo",
      "Figures",
      "Videos",
      "Tables"
    ],
    "policy_type": [
      "NPG",
      "PG"
    ],
    "task_mappings": [
      "N/A"
    ],
    "autonomous_transfer": 0,
    "is_deep_rl": 1,
    "tags": [
      "Robotics",
      "3D",
      "Arm",
      "Simulation",
      "RealWorld",
      "Push"
    ],
    "allowed_learner": [
      "TD"
    ],
    "country": [
      "USA"
    ],
    "uni": [
      "University of Washington",
      "Roboti LLC"
    ],
    "department": [
      "Industry",
      "Uni",
      "Colab"
    ],
    "source_task_selection": [
      "h"
    ],
    "was_in_survey": [
      "mag"
    ],
    "in_title": [
      "t",
      "r",
      "l"
    ],
    "in_abs": [
      "t",
      "r",
      "l"
    ],
    "in_contet": [
      "t",
      "r",
      "l"
    ],
    "rejected_or_unpublished": null,
    "duplication_status": null,
    "paper_for_thesis": 0,
    "paper_available": 1,
    "behind_paywall": 0,
    "paywall_circumventable": 0,
    "author_names": [
      "Svetoslav Kolev",
      "Emanuel Todorov",
      "Kendall Lowrey",
      "Aravind Rajeswaran",
      "Jeremy Dao"
    ]
  },
  {
    "id": 2153353285,
    "title": "Using advice to transfer knowledge acquired in one reinforcement learning task to another",
    "abst": "We present a method for transferring knowledge learned in one task to a related task. Our problem solvers employ reinforcement learning to acquire a model for one task. We then transform that learned model into advice for a new task. A human teacher provides a mapping from the old task to the new task to guide this knowledge transfer. Advice is incorporated into our problem solver using a knowledge-based support vector regression method that we previously developed. This advice-taking approach allows the problem solver to refine or even discard the transferred knowledge based on its subsequent experiences. We empirically demonstrate the effectiveness of our approach with two games from the RoboCup soccer simulator: KeepAway and BreakAway. Our results demonstrate that a problem solver learning to play BreakAway using advice extracted from KeepAway outperforms a problem solver learning without the benefit of such advice.",
    "url": "https://doi.org/10.1007/11564096_40",
    "lang": "en",
    "authors": [
      738944226,
      2047441381,
      2079278047,
      2120363087
    ],
    "fos": [
      3019612716,
      97541855,
      2776960227,
      34413123,
      2777220311,
      12267149,
      154945302,
      41008148,
      119857082
    ],
    "journals": [],
    "conferences": [],
    "conference_series": [
      2755314191
    ],
    "references": [
      24477102,
      193428430,
      1497976081,
      1515851193,
      1584120419,
      1594602740,
      1617610651,
      1976115983,
      2012036715,
      2121863487,
      2122982548,
      2126565096,
      2140584963,
      2141559645,
      2145739724,
      2155791599,
      2160644528,
      2165792602,
      3022194887
    ],
    "filter_matches": [
      "tr",
      "tl",
      "trl",
      "rl"
    ],
    "rank": 19290,
    "citation_count": 73,
    "estimated_citation_count": 125,
    "publication_date": "2005-10-03",
    "found_in": 1,
    "transfer_experiment_type": [
      "p"
    ],
    "transfer_experiment_subtype": [
      "diff-it"
    ],
    "transfer_data_type": [
      "advisor"
    ],
    "transfer_performance_metrics": [
      "ap",
      "tr"
    ],
    "implementation": [
      "Custom",
      "CS",
      "Pseudo",
      "Formulas",
      "Figures"
    ],
    "policy_type": [
      "SARSA"
    ],
    "task_mappings": [
      "sup"
    ],
    "autonomous_transfer": 0,
    "is_deep_rl": 0,
    "tags": [
      "2D",
      "RoboCup",
      "KeepAway",
      "BreakAway",
      "MultiAgent",
      "Simulation",
      "3v2",
      "2v1"
    ],
    "allowed_learner": [
      "TD"
    ],
    "country": [
      "USA"
    ],
    "uni": [
      "University of Wisconsin",
      "University of Minnesota"
    ],
    "department": [],
    "source_task_selection": [
      "h"
    ],
    "was_in_survey": [
      "taylor",
      "lazaric",
      "zhu",
      "mag"
    ],
    "in_title": [
      "t",
      "r",
      "l",
      "k"
    ],
    "in_abs": [
      "t",
      "r",
      "l",
      "k"
    ],
    "in_contet": [
      "t",
      "r",
      "l",
      "k",
      "s"
    ],
    "rejected_or_unpublished": null,
    "duplication_status": null,
    "paper_for_thesis": 0,
    "paper_available": 1,
    "behind_paywall": 0,
    "paywall_circumventable": 0,
    "author_names": [
      "Lisa Torrey",
      "Richard Maclin",
      "Jude W. Shavlik",
      "Trevor Walker"
    ]
  },
  {
    "id": 1506146479,
    "title": "Skill acquisition via transfer learning and advice taking",
    "abst": "We describe a reinforcement learning system that transfers skills from a previously learned source task to a related target task. The system uses inductive logic programming to analyze experience in the source task, and transfers rules for when to take actions. The target task learner accepts these rules through an advice-taking algorithm, which allows learners to benefit from outside guidance that may be imperfect. Our system accepts a human-provided mapping, which specifies the similarities between the source and target tasks and may also include advice about the differences between them. Using three tasks in the RoboCup simulated soccer domain, we demonstrate that this system can speed up reinforcement learning substantially.",
    "url": "https://experts.umn.edu/en/publications/skill-acquisition-via-transfer-learning-and-advice-taking",
    "lang": "en",
    "authors": [
      738944226,
      2047441381,
      2079278047,
      2120363087
    ],
    "fos": [
      154945302,
      68339613,
      150899416,
      34413123,
      132758656,
      2776960227,
      28006648,
      41008148,
      2779382394,
      97541855
    ],
    "journals": [],
    "conferences": [],
    "conference_series": [
      2755314191
    ],
    "references": [
      198956113,
      1515851193,
      1834252692,
      1987902506,
      2012036715,
      2100677568,
      2104046064,
      2121863487,
      2122982548,
      2124175081,
      2126565096,
      2140584963,
      2153353285,
      2155791599,
      3022194887
    ],
    "filter_matches": [
      "tl"
    ],
    "rank": 19301,
    "citation_count": 51,
    "estimated_citation_count": 78,
    "publication_date": "2006-09-18",
    "found_in": 1,
    "transfer_experiment_type": [
      "a",
      "r",
      "v"
    ],
    "transfer_experiment_subtype": [
      "diff-it"
    ],
    "transfer_data_type": [
      "rule",
      "advisor"
    ],
    "transfer_performance_metrics": [
      "j",
      "tr"
    ],
    "implementation": [
      "Custom",
      "CS",
      "Aleph",
      "Pseudo",
      "Tables",
      "Figures",
      "Formulas"
    ],
    "policy_type": [
      "Q"
    ],
    "task_mappings": [
      "sup"
    ],
    "autonomous_transfer": 0,
    "is_deep_rl": 0,
    "tags": [
      "2D",
      "RoboCup",
      "BreakAway",
      "Simulation",
      "MultiAgent",
      "4v3",
      "3v2",
      "MoveDownfield",
      "KeepAway"
    ],
    "allowed_learner": [
      "TD"
    ],
    "country": [
      "USA"
    ],
    "uni": [
      "University of Minnesota",
      "University of Wisconsin"
    ],
    "department": [
      "Computer Science"
    ],
    "source_task_selection": [
      "h"
    ],
    "was_in_survey": [
      "taylor"
    ],
    "in_title": [
      "t",
      "l",
      "s"
    ],
    "in_abs": [
      "t",
      "r",
      "l",
      "s"
    ],
    "in_contet": [
      "t",
      "r",
      "l",
      "s",
      "k"
    ],
    "rejected_or_unpublished": null,
    "duplication_status": null,
    "paper_for_thesis": 0,
    "paper_available": 1,
    "behind_paywall": 0,
    "paywall_circumventable": 0,
    "author_names": [
      "Lisa Torrey",
      "Richard Maclin",
      "Jude W. Shavlik",
      "Trevor Walker"
    ]
  },
  {
    "id": 2124695578,
    "title": "Transfer learning for reinforcement learning on a physical robot",
    "abst": "As robots become more widely available, many capabilities that were once only practical to develop and test in simulation are becoming feasible on real, physically grounded, robots. This newfound feasibility is important because simulators rarely represent the world with sufficient fidelity that developed behaviors will work as desired in the real world. However, development and testing on robots remains difficult and time consuming, so it is desirable to minimize the number of trials needed when developing robot behaviors. This paper focuses on reinforcement learning (RL) on physically grounded robots. A few noteworthy exceptions notwithstanding, RL has typically been done purely in simulation, or, at best, initially in simulation with the eventual learned behaviors run on a real robot. However, some recent RL methods exhibit sufficiently low sample complexity to enable learning entirely on robots. One such method is transfer learning for RL. The main contribution of this paper is the first empirical demonstration that transfer learning can significantly speed up and even improve asymptotic performance of RL done entirely on a physical robot. In addition, we show that transferring information learned in simulation can bolster additional learning on the robot.",
    "url": "https://www.researchgate.net/profile/Matthew_Taylor12/publication/228959234_Transfer_learning_for_reinforcement_learning_on_a_physical_robot/links/0fcfd51144740519c6000000.pdf?origin=publication_detail",
    "lang": "en",
    "authors": [
      2222275272
    ],
    "fos": [
      2779841105,
      188888258,
      97541855,
      2776459999,
      34413123,
      119857082,
      41008148,
      90509273,
      107457646,
      154945302,
      68339613,
      150899416
    ],
    "journals": [],
    "conferences": [
      2786712790
    ],
    "conference_series": [
      1168671587
    ],
    "references": [
      24477102,
      134361651,
      203338875,
      1487106624,
      1585603966,
      1601735911,
      1949804828,
      1996625075,
      2012036715,
      2096001037,
      2097381042,
      2110292307,
      2121863487,
      2124175081,
      2130105540,
      2133040789,
      2135043890,
      2139053308,
      2156493855
    ],
    "filter_matches": [
      "tr",
      "tl",
      "trl",
      "rl"
    ],
    "rank": 19303,
    "citation_count": 40,
    "estimated_citation_count": 66,
    "publication_date": "2010-12-01",
    "found_in": 1,
    "transfer_experiment_type": [
      "t"
    ],
    "transfer_experiment_subtype": [
      "sim2real",
      "multi"
    ],
    "transfer_data_type": [
      "Q"
    ],
    "transfer_performance_metrics": [
      "j",
      "tr",
      "ap"
    ],
    "implementation": [
      "Custom",
      "CS",
      "Formulas",
      "Figures",
      "Web",
      "Dead"
    ],
    "policy_type": [
      "Q",
      "SARSA"
    ],
    "task_mappings": [
      "sup"
    ],
    "autonomous_transfer": 0,
    "is_deep_rl": 0,
    "tags": [
      "3D",
      "Robotics",
      "KeepAway",
      "RoboCup",
      "Simulation",
      "RealWorld",
      "MultiAgent"
    ],
    "allowed_learner": [
      "TD"
    ],
    "country": [
      "USA"
    ],
    "uni": [
      "The University of Texas",
      "University of Southern California"
    ],
    "department": [
      "Computer Science"
    ],
    "source_task_selection": [],
    "was_in_survey": [
      "mag"
    ],
    "in_title": [
      "t",
      "r",
      "l"
    ],
    "in_abs": [
      "t",
      "r",
      "l"
    ],
    "in_contet": [
      "t",
      "r",
      "l",
      "k"
    ],
    "rejected_or_unpublished": null,
    "duplication_status": null,
    "paper_for_thesis": 0,
    "paper_available": 1,
    "behind_paywall": 0,
    "paywall_circumventable": 0,
    "author_names": [
      "Samuel Barrett and Peter Stone"
    ]
  },
  {
    "id": 2122982548,
    "title": "Giving advice about preferred actions to reinforcement learners via knowledge-based kernel regression",
    "abst": "We present a novel formulation for providing advice to a reinforcement learner that employs support-vector regression as its function approximator. Our new method extends a recent advice-giving technique, called Knowledge-Based Kernel Regression (KBKR), that accepts advice concerning a single action of a reinforcement learner. In KBKR, users can say that in some set of states, an action&#039;s value should be greater than some linear expression of the current state. In our new technique, which we call Preference KBKR (Pref-KBKR), the user can provide advice in a more natural manner by recommending that some action is preferred over another in the specified set of states. Specifying preferences essentially means that users are giving advice about policies rather than Q values, which is a more natural way for humans to present advice. We present the motivation for preference advice and a proof of the correctness of our extension to KBKR. In addition, we show empirical results that our method can make effective use of advice on a novel reinforcement-learning task, based on the RoboCup simulator, which we call Breakaway. Our work demonstrates the significant potential of advice-giving techniques for addressing complex reinforcement learning problems, while further demonstrating the use of support-vector regression for reinforcement learning.",
    "url": "https://experts.umn.edu/en/publications/giving-advice-about-preferred-actions-to-reinforcement-learners-v",
    "lang": "en",
    "authors": [
      738944226,
      2047441381,
      2079278047,
      2120363087,
      2464448550
    ],
    "fos": [
      41008148,
      97541855,
      67203356,
      55439883,
      154945302,
      200695384,
      83546350,
      119857082
    ],
    "journals": [],
    "conferences": [
      2785841295
    ],
    "conference_series": [
      1184914352
    ],
    "references": [
      193428430,
      1497976081,
      1511354083,
      1515851193,
      1584120419,
      1617610651,
      1976115983,
      2093404847,
      2121863487,
      2134289401,
      2140584963,
      2141559645,
      2145739724,
      2155791599,
      3022194887
    ],
    "filter_matches": [
      "apa"
    ],
    "rank": 19317,
    "citation_count": 82,
    "estimated_citation_count": 118,
    "publication_date": "2005-07-09",
    "found_in": 1,
    "transfer_experiment_type": [
      "a",
      "r",
      "v"
    ],
    "transfer_experiment_subtype": [
      "diff-it"
    ],
    "transfer_data_type": [
      "rule",
      "advisor"
    ],
    "transfer_performance_metrics": [
      "j",
      "tr"
    ],
    "implementation": [
      "Custom",
      "CS",
      "Formulas",
      "Figures"
    ],
    "policy_type": [
      "Q"
    ],
    "task_mappings": [
      "sup"
    ],
    "autonomous_transfer": 0,
    "is_deep_rl": 0,
    "tags": [
      "2D",
      "RoboCup",
      "BreakAway",
      "Simulation",
      "MultiAgent",
      "3v2"
    ],
    "allowed_learner": [
      "TD"
    ],
    "country": [
      "USA"
    ],
    "uni": [
      "University of Minnesota",
      "University of Wisconsin"
    ],
    "department": [
      "Computer Science"
    ],
    "source_task_selection": [
      "h"
    ],
    "was_in_survey": [
      "taylor"
    ],
    "in_title": [
      "r",
      "l",
      "k"
    ],
    "in_abs": [
      "r",
      "l",
      "k"
    ],
    "in_contet": [
      "r",
      "l",
      "k"
    ],
    "rejected_or_unpublished": null,
    "duplication_status": null,
    "paper_for_thesis": 0,
    "paper_available": 1,
    "behind_paywall": 0,
    "paywall_circumventable": 0,
    "author_names": [
      "Lisa Torrey",
      "Richard Maclin",
      "Jude W. Shavlik",
      "Trevor Walker"
    ]
  },
  {
    "id": 2004030284,
    "title": "Transfer of samples in batch reinforcement learning",
    "abst": "The main objective of transfer in reinforcement learning is to reduce the complexity of learning the solution of a target task by effectively reusing the knowledge retained from solving a set of source tasks. In this paper, we introduce a novel algorithm that transfers samples (i.e., tuples &#x2329;s, a, s&#039;, r&#x232A;) from source to target tasks. Under the assumption that tasks have similar transition models and reward functions, we propose a method to select samples from the source tasks that are mostly similar to the target task, and, then, to use them as input for batch reinforcement-learning algorithms. As a result, the number of samples an agent needs to collect from the target task to learn its solution is reduced. We empirically show that, following the proposed approach, the transfer of samples is effective in reducing the learning complexity, even when some source tasks are significantly different from the target task.",
    "url": "https://core.ac.uk/display/108628857",
    "lang": "en",
    "authors": [
      98988277,
      131065259,
      2103877068
    ],
    "fos": [
      8038995,
      41008148,
      119857082,
      97541855,
      28006648,
      2983510059,
      199190896,
      206588197,
      154945302,
      47932503,
      118930307
    ],
    "journals": [],
    "conferences": [
      80229498
    ],
    "conference_series": [
      1180662882
    ],
    "references": [
      131458779,
      1492014007,
      1582256513,
      2090170171,
      2109102709,
      2110292307,
      2120346334,
      2131831090,
      2132057084,
      2133040789,
      2568646110
    ],
    "filter_matches": [
      "tr",
      "tl",
      "trl",
      "rl"
    ],
    "rank": 19361,
    "citation_count": 82,
    "estimated_citation_count": 126,
    "publication_date": "2008-07-05",
    "found_in": 1,
    "transfer_experiment_type": [
      "s_i",
      "s_f",
      "s"
    ],
    "transfer_experiment_subtype": [
      "multi"
    ],
    "transfer_data_type": [
      "I"
    ],
    "transfer_performance_metrics": [
      "j",
      "tt"
    ],
    "implementation": [
      "Custom",
      "CS",
      "Pseudo",
      "Formulas",
      "Figures",
      "Tables"
    ],
    "policy_type": [
      "FQI"
    ],
    "task_mappings": [
      "N/A"
    ],
    "autonomous_transfer": 0,
    "is_deep_rl": 0,
    "tags": [
      "2D",
      "Navigation",
      "Obstacle",
      "Samples",
      "Simulation"
    ],
    "allowed_learner": [
      "TD",
      "batch"
    ],
    "country": [
      "Italy"
    ],
    "uni": [
      "Politecnico di Milano"
    ],
    "department": [
      "IIT-Lab"
    ],
    "source_task_selection": [
      "lib"
    ],
    "was_in_survey": [
      "lazaric",
      "zhu",
      "bone",
      "mag",
      "cur"
    ],
    "in_title": [
      "t",
      "r",
      "l"
    ],
    "in_abs": [
      "t",
      "r",
      "l",
      "k"
    ],
    "in_contet": [
      "t",
      "r",
      "l",
      "k"
    ],
    "rejected_or_unpublished": null,
    "duplication_status": null,
    "paper_for_thesis": 0,
    "paper_available": 1,
    "behind_paywall": 1,
    "paywall_circumventable": 0,
    "author_names": [
      "Andrea Bonarini",
      "Alessandro Lazaric",
      "Marcello Restelli"
    ]
  },
  {
    "id": 2154328025,
    "title": "Transfer via inter-task mappings in policy search reinforcement learning",
    "abst": "The ambitious goal of transfer learning is to accelerate learning on a target task after training on a different, but related, source task. While many past transfer methods have focused on transferring value-functions, this paper presents a method for transferring policies across tasks with different state and action spaces. In particular, this paper utilizes transfer via inter-task mappings for policy search methods (TVITM-PS) to construct a transfer functional that translates a population of neural network policies trained via policy search from a source task to a target task. Empirical results in robot soccer Keepaway and Server Job Scheduling show that TVITM-PS can markedly reduce learning time when full inter-task mappings are available. The results also demonstrate that TVITMPS still succeeds when given only incomplete inter-task mappings. Furthermore, we present a novel method for learning such mappings when they are not available, and give results showing they perform comparably to hand-coded mappings.",
    "url": "http://www.cs.utexas.edu/users/ai-lab/?taylor:ijcaams07",
    "lang": "en",
    "authors": [
      2042571382,
      2147180669,
      2148762994
    ],
    "fos": [
      97541855,
      77075516,
      111873713,
      90509273,
      50644808,
      150899416,
      2908647359,
      154945302,
      111226992,
      119857082,
      41008148
    ],
    "journals": [],
    "conferences": [
      2786151659
    ],
    "conference_series": [
      1168671587
    ],
    "references": [
      24477102,
      36691172,
      1515851193,
      1570448133,
      1670263352,
      1760981091,
      2104641222,
      2111935653,
      2116339921,
      2119053738,
      2119567691,
      2124290836,
      2125614502,
      2126565096,
      2134153324,
      2153353285,
      2169659168,
      2198041288,
      2966207845
    ],
    "filter_matches": [
      "tr",
      "tl",
      "trl",
      "rl"
    ],
    "rank": 19368,
    "citation_count": 81,
    "estimated_citation_count": 123,
    "publication_date": "2007-05-14",
    "found_in": 1,
    "transfer_experiment_type": [
      "a",
      "v"
    ],
    "transfer_experiment_subtype": [
      "diff-it",
      "lit"
    ],
    "transfer_data_type": [
      "pi"
    ],
    "transfer_performance_metrics": [
      "tt",
      "j",
      "tr"
    ],
    "implementation": [
      "Custom",
      "CS",
      "Pseudo",
      "Formulas",
      "Figures",
      "Tables"
    ],
    "policy_type": [
      "TVITM-PS"
    ],
    "task_mappings": [
      "sup"
    ],
    "autonomous_transfer": 0,
    "is_deep_rl": 0,
    "tags": [
      "Simulation",
      "RoboCup",
      "KeepAway",
      "MultiAgent",
      "3v2",
      "4v3"
    ],
    "allowed_learner": [
      "PS",
      "all"
    ],
    "country": [
      "USA"
    ],
    "uni": [
      "The University of Texas"
    ],
    "department": [
      "Computer Science"
    ],
    "source_task_selection": [
      "h"
    ],
    "was_in_survey": [
      "taylor",
      "silva",
      "bone",
      "taylor-intertask",
      "lazaric",
      "mag",
      "cur"
    ],
    "in_title": [
      "t",
      "r",
      "l"
    ],
    "in_abs": [
      "t",
      "r",
      "l"
    ],
    "in_contet": [
      "t",
      "r",
      "l",
      "k"
    ],
    "rejected_or_unpublished": null,
    "duplication_status": null,
    "paper_for_thesis": 0,
    "paper_available": 1,
    "behind_paywall": 0,
    "paywall_circumventable": 0,
    "author_names": [
      "Shimon Whiteson",
      "Peter Stone",
      "Matthew D. Taylor"
    ]
  },
  {
    "id": 2110292307,
    "title": "Transferring instances for model-based reinforcement learning",
    "abst": "Reinforcement learningagents typically require a significant amount of data before performing well on complex tasks. Transfer learningmethods have made progress reducing sample complexity, but they have primarily been applied to model-free learning methods, not more data-efficient model-based learning methods. This paper introduces timbrel , a novel method capable of transferring information effectively into a model-based reinforcement learning algorithm. We demonstrate that timbrel can significantly improve the sample efficiency and asymptotic performance of a model-based algorithm when learning in a continuous state space. Additionally, we conduct experiments to test the limits of timbrel &#039;s effectiveness.",
    "url": "https://rd.springer.com/chapter/10.1007/978-3-540-87481-2_32",
    "lang": "en",
    "authors": [
      2142875091,
      2147180669,
      2148762994
    ],
    "fos": [
      188888258,
      119857082,
      58973888,
      97541855,
      28006648,
      8038995,
      199190896,
      41008148,
      24138899,
      154945302,
      77967617
    ],
    "journals": [],
    "conferences": [],
    "conference_series": [
      2755314191
    ],
    "references": [
      36691172,
      1496855202,
      1505937442,
      1515851193,
      1533597678,
      1585546214,
      1585603966,
      1601974704,
      1949804828,
      2004030284,
      2022775778,
      2041367235,
      2042357378,
      2048226872,
      2098723043,
      2104641222,
      2113913482,
      2114537044,
      2117341272,
      2117629901,
      2119567691,
      2133040789,
      2133120480,
      2145983895,
      2153192722,
      2153353285,
      2154328025,
      2158150115,
      2162768030,
      2169743339,
      2334782222,
      2341171179,
      2914746235,
      3011120880,
      3023407077
    ],
    "filter_matches": [
      "tr",
      "tl",
      "trl",
      "rl"
    ],
    "rank": 19368,
    "citation_count": 68,
    "estimated_citation_count": 113,
    "publication_date": "2008-09-15",
    "found_in": 1,
    "transfer_experiment_type": [
      "a",
      "v"
    ],
    "transfer_experiment_subtype": [
      "diff-it",
      "lit"
    ],
    "transfer_data_type": [
      "I"
    ],
    "transfer_performance_metrics": [
      "ap",
      "tr",
      "j"
    ],
    "implementation": [
      "Custom",
      "CS",
      "Pseudo",
      "Web",
      "Dead",
      "Figures",
      "Formulas",
      "Tables"
    ],
    "policy_type": [
      "SARSA",
      "R-Max",
      "TIMBREL"
    ],
    "task_mappings": [
      "sup",
      "exp"
    ],
    "autonomous_transfer": 0,
    "is_deep_rl": 0,
    "tags": [
      "2D",
      "3D",
      "2Dto3D",
      "MountainCar",
      "MountainCar3D",
      "Simulation",
      "ClassicControl"
    ],
    "allowed_learner": [
      "MB",
      "all"
    ],
    "country": [
      "USA"
    ],
    "uni": [
      "The University of Texas"
    ],
    "department": [
      "Computer Science"
    ],
    "source_task_selection": [
      "h"
    ],
    "was_in_survey": [
      "taylor",
      "lazaric",
      "taylor-intertask",
      "bone",
      "mag"
    ],
    "in_title": [
      "t",
      "r",
      "l"
    ],
    "in_abs": [
      "t",
      "r",
      "l"
    ],
    "in_contet": [
      "t",
      "r",
      "l",
      "k"
    ],
    "rejected_or_unpublished": null,
    "duplication_status": null,
    "paper_for_thesis": 0,
    "paper_available": 1,
    "behind_paywall": 0,
    "paywall_circumventable": 0,
    "author_names": [
      "Nicholas K. Jong",
      "Peter Stone",
      "Matthew D. Taylor"
    ]
  },
  {
    "id": 2106953752,
    "title": "General game learning using knowledge transfer",
    "abst": "We present a reinforcement learning game player that can interact with a General Game Playing system and transfer knowledge learned in one game to expedite learning in many other games. We use the technique of value-function transfer where general features are extracted from the state space of a previous game and matched with the completely different state space of a new game. To capture the underlying similarity of vastly disparate state spaces arising from different games, we use a game-tree lookahead structure for features. We show that such feature-based value function transfer learns superior policies faster than a reinforcement learning agent that does not use knowledge transfer. Furthermore, knowledge transfer using lookahead features can capture opponent-specific value-functions, i.e. can exploit an opponent&#039;s weaknesses to learn faster than a reinforcement learner that uses lookahead with minimax (pessimistic) search against the same opponent.",
    "url": "http://ijcai.org/Proceedings/07/Papers/107.pdf",
    "lang": "en",
    "authors": [
      2147180669,
      2162022237
    ],
    "fos": [
      167573328,
      73795354,
      41008148,
      154945302,
      202556891,
      2780617750,
      14642086,
      119857082,
      102234262,
      47175762,
      170828538
    ],
    "journals": [],
    "conferences": [
      2793510348
    ],
    "conference_series": [
      1203999783
    ],
    "references": [
      47250057,
      1515851193,
      2041367235,
      2099587183,
      2111572265,
      2121863487,
      2126565096,
      2145943363,
      2964331425
    ],
    "filter_matches": [
      "tl"
    ],
    "rank": 19380,
    "citation_count": 79,
    "estimated_citation_count": 103,
    "publication_date": "2007-01-06",
    "found_in": 1,
    "transfer_experiment_type": [
      "a",
      "v"
    ],
    "transfer_experiment_subtype": [
      "diff-no"
    ],
    "transfer_data_type": [
      "fea"
    ],
    "transfer_performance_metrics": [
      "ap",
      "j",
      "tr"
    ],
    "implementation": [
      "Custom",
      "CS",
      "Formulas",
      "Figures",
      "Tables"
    ],
    "policy_type": [
      "Q"
    ],
    "task_mappings": [
      "N/A"
    ],
    "autonomous_transfer": 0,
    "is_deep_rl": 0,
    "tags": [
      "Games",
      "GGP",
      "BoardGames",
      "Connect3",
      "CaptureGo",
      "Othello",
      "Simulation",
      "Games"
    ],
    "allowed_learner": [
      "TD"
    ],
    "country": [
      "USA"
    ],
    "uni": [
      "The University of Texas"
    ],
    "department": [
      "Computer Science"
    ],
    "source_task_selection": [
      "h"
    ],
    "was_in_survey": [
      "taylor"
    ],
    "in_title": [
      "t",
      "l",
      "k"
    ],
    "in_abs": [
      "t",
      "r",
      "l",
      "k"
    ],
    "in_contet": [
      "t",
      "r",
      "l",
      "k"
    ],
    "rejected_or_unpublished": null,
    "duplication_status": null,
    "paper_for_thesis": 0,
    "paper_available": 1,
    "behind_paywall": 0,
    "paywall_circumventable": 0,
    "author_names": [
      "Peter Stone",
      "Bikramjit Banerjee"
    ]
  },
  {
    "id": 2158150115,
    "title": "Autonomous transfer for reinforcement learning",
    "abst": "Recent work in transfer learning has succeeded in making reinforcement learning algorithms more efficient by incorporating knowledge from previous tasks. However, such methods typically must be provided either a full model of the tasks or an explicit relation mapping one task into the other. An autonomous agent may not have access to such high-level information, but would be able to analyze its experience to find similarities between tasks. In this paper we introduce Modeling Approximate State Transitions by Exploiting Regression (MASTER), a method for automatically learning a mapping from one task to another through an agent&#039;s experience. We empirically demonstrate that such learned relationships can significantly improve the speed of a reinforcement learning algorithm in a series of Mountain Car tasks. Additionally, we demonstrate that our method may also assist with the difficult problem of task selection for transfer.",
    "url": "https://dl.acm.org/citation.cfm?id=1402427",
    "lang": "en",
    "authors": [
      2147180669,
      2148762994,
      2160943816
    ],
    "fos": [
      154945302,
      28006648,
      97541855,
      119857082,
      188888258,
      41008148,
      150899416,
      77075516,
      199190896,
      58973888,
      8038995
    ],
    "journals": [],
    "conferences": [
      2787693658
    ],
    "conference_series": [
      1168671587
    ],
    "references": [
      36691172,
      203338875,
      1515851193,
      1533597678,
      1550698229,
      1570448133,
      1585603966,
      2031727428,
      2098723043,
      2099587183,
      2111935653,
      2113913482,
      2114013702,
      2122982548,
      2123995443,
      2126385963,
      2133040789,
      2153353285,
      2154328025,
      2164114810,
      2166798247,
      2396715201,
      2966207845,
      3020831056
    ],
    "filter_matches": [
      "tr",
      "tl",
      "trl",
      "rl"
    ],
    "rank": 19387,
    "citation_count": 80,
    "estimated_citation_count": 121,
    "publication_date": "2008-05-12",
    "found_in": 1,
    "transfer_experiment_type": [
      "a",
      "v",
      "t"
    ],
    "transfer_experiment_subtype": [
      "lit",
      "diff-no"
    ],
    "transfer_data_type": [
      "I",
      "Q"
    ],
    "transfer_performance_metrics": [
      "j",
      "tt"
    ],
    "implementation": [
      "Custom",
      "CS",
      "Formulas",
      "Pseudo",
      "Figures",
      "Tables",
      "Web",
      "Dead"
    ],
    "policy_type": [
      "Q"
    ],
    "task_mappings": [
      "exp"
    ],
    "autonomous_transfer": 1,
    "is_deep_rl": 0,
    "tags": [
      "2D",
      "3D",
      "2Dto3D",
      "MountainCar",
      "MountainCar3D",
      "Simulation",
      "Handbrake",
      "ClassicControl"
    ],
    "allowed_learner": [
      "TD"
    ],
    "country": [
      "USA"
    ],
    "uni": [
      "The University of Texas"
    ],
    "department": [
      "Computer Science"
    ],
    "source_task_selection": [
      "h"
    ],
    "was_in_survey": [
      "lazaric",
      "bone",
      "cur",
      "mag"
    ],
    "in_title": [
      "t",
      "r",
      "l"
    ],
    "in_abs": [
      "t",
      "r",
      "l",
      "k"
    ],
    "in_contet": [
      "t",
      "r",
      "l",
      "k"
    ],
    "rejected_or_unpublished": null,
    "duplication_status": null,
    "paper_for_thesis": 0,
    "paper_available": 1,
    "behind_paywall": 0,
    "paywall_circumventable": 0,
    "author_names": [
      "Peter Stone",
      "Matthew D. Taylor",
      "Gregory Kuhlmann"
    ]
  },
  {
    "id": 2913485808,
    "title": "Universal Successor Features for Transfer Reinforcement Learning",
    "abst": "Transfer in Reinforcement Learning (RL) refers to the idea of applying knowledge gained from previous tasks to solving related tasks. Learning a universal value function (Schaul et al., 2015), which generalizes over goals and states, has previously been shown to be useful for transfer. However, successor features are believed to be more suitable than values for transfer (Dayan, 1993; Barreto et al.,2017), even though they cannot directly generalize to new goals. In this paper, we propose (1) Universal Successor Features (USFs) to capture the underlying dynamics of the environment while allowing generalization to unseen goals and (2) a flexible end-to-end model of USFs that can be trained by interacting with the environment. We show that learning USFs is compatible with any RL algorithm that learns state values using a temporal difference method. Our experiments in a simple gridworld and with two MuJoCo environments show that USFs can greatly accelerate training when learning multiple tasks and can effectively transfer knowledge to new tasks.",
    "url": "http://arxiv.org/pdf/2001.04025.pdf",
    "lang": null,
    "authors": [
      161269817,
      2098772737,
      2798256004,
      2957660614
    ],
    "fos": [
      196340769,
      75306776,
      154945302,
      2778375133,
      119857082,
      97541855,
      2908926286,
      33923547
    ],
    "journals": [
      2597173376
    ],
    "conferences": [],
    "conference_series": [],
    "references": [
      567721252,
      2056354534,
      2097381042,
      2124175081,
      2132622533,
      2145339207,
      2158782408,
      2417089653,
      2551887912,
      2787757704,
      2789008106,
      2804673281,
      2913340405,
      2962717849,
      2963864421,
      2964001908,
      2964043796,
      2964161785
    ],
    "filter_matches": [
      "tr",
      "tl",
      "trl",
      "rl"
    ],
    "rank": 19417,
    "citation_count": 0,
    "estimated_citation_count": 0,
    "publication_date": "2018-09-27",
    "found_in": 1,
    "transfer_experiment_type": [
      "s_i",
      "s_f",
      "levels",
      "r"
    ],
    "transfer_experiment_subtype": [
      "multi"
    ],
    "transfer_data_type": [
      "fea"
    ],
    "transfer_performance_metrics": [
      "tt"
    ],
    "implementation": [
      "Custom",
      "CS",
      "Formulas",
      "Figures",
      "Pseudo",
      "Gym"
    ],
    "policy_type": [
      "DQN",
      "DDPG",
      "USF"
    ],
    "task_mappings": [
      "N/A"
    ],
    "autonomous_transfer": 0,
    "is_deep_rl": 1,
    "tags": [
      "2D",
      "Navigation",
      "Grid",
      "Reacher",
      "FetchReach",
      "Simulation"
    ],
    "allowed_learner": [
      "TD"
    ],
    "country": [
      "USA",
      "Canada"
    ],
    "uni": [
      "University of Alberta",
      "University of Montreal"
    ],
    "department": [
      "Computer Science"
    ],
    "source_task_selection": [
      "all"
    ],
    "was_in_survey": [
      "zhu",
      "mag"
    ],
    "in_title": [
      "t",
      "r",
      "l"
    ],
    "in_abs": [
      "t",
      "r",
      "l",
      "k"
    ],
    "in_contet": [
      "t",
      "r",
      "l",
      "k"
    ],
    "rejected_or_unpublished": 2,
    "duplication_status": null,
    "paper_for_thesis": 0,
    "paper_available": 1,
    "behind_paywall": 0,
    "paywall_circumventable": 0,
    "author_names": [
      "Yoshua Bengio",
      "Junfeng Wen",
      "Chen Ma",
      "Dylan R. Ashley"
    ]
  },
  {
    "id": 2784831250,
    "title": "Cross-Domain Transfer in Reinforcement Learning using Target Apprentice",
    "abst": "In this paper, we present a new approach to Transfer Learning (TL) in Reinforcement Learning (RL) for cross-domain tasks. Many of the available techniques approach the transfer architecture as a method of speeding up the target task learning. We propose to adapt and reuse the mapped source task optimal-policy directly in related domains. We show the optimal policy from a related source task can be near optimal in target domain provided an adaptive policy accounts for the model error between target and source. The main benefit of this policy augmentation is generalizing policies across multiple related domains without having to re-learn the new tasks. Our results show that this architecture leads to better sample efficiency in the transfer, reducing sample complexity of target task learning to target apprentice learning.",
    "url": "http://export.arxiv.org/pdf/1801.06920",
    "lang": "en",
    "authors": [
      2098179893,
      2110746766
    ],
    "fos": [
      150899416,
      119857082,
      97541855,
      107806365,
      2987555995,
      123657996,
      154945302,
      41008148,
      206588197,
      177148314,
      179024874
    ],
    "journals": [
      2596500785
    ],
    "conferences": [],
    "conference_series": [],
    "references": [
      64698994,
      110451278,
      1492014007,
      1499408472,
      1848094219,
      1991753499,
      1996625075,
      2019324135,
      2078676277,
      2097381042,
      2098723043,
      2106953752,
      2127107099,
      2128905965,
      2133040789,
      2145339207,
      2156493855,
      2164114810
    ],
    "filter_matches": [
      "tr",
      "tl",
      "trl",
      "rl"
    ],
    "rank": 19430,
    "citation_count": 1,
    "estimated_citation_count": 1,
    "publication_date": "2018-01-22",
    "found_in": 1,
    "transfer_experiment_type": [
      "v",
      "a",
      "t"
    ],
    "transfer_experiment_subtype": [
      "diff-no",
      "lit"
    ],
    "transfer_data_type": [
      "I",
      "advisor"
    ],
    "transfer_performance_metrics": [
      "j",
      "tt"
    ],
    "implementation": [
      "Custom",
      "CS",
      "Formulas",
      "Figures",
      "Pseudo",
      "Theorem"
    ],
    "policy_type": [
      "FQI",
      "Q",
      "UMA",
      "TA"
    ],
    "task_mappings": [
      "Ma",
      "exp"
    ],
    "autonomous_transfer": 1,
    "is_deep_rl": 0,
    "tags": [
      "2D",
      "Simulation",
      "CartPole",
      "Bicycle",
      "Navigation",
      "Grid",
      "ClassicControl"
    ],
    "allowed_learner": [
      "TD",
      "batch"
    ],
    "country": [
      "USA"
    ],
    "uni": [
      "University of Illinois"
    ],
    "department": [
      "Department of Agriculture and Biological Engineering and Coordinated Science Lab"
    ],
    "source_task_selection": [
      "h"
    ],
    "was_in_survey": [
      "mag"
    ],
    "in_title": [
      "t",
      "r",
      "l"
    ],
    "in_abs": [
      "t",
      "r",
      "l"
    ],
    "in_contet": [
      "t",
      "r",
      "l",
      "k",
      "s"
    ],
    "rejected_or_unpublished": null,
    "duplication_status": null,
    "paper_for_thesis": 0,
    "paper_available": 1,
    "behind_paywall": 0,
    "paywall_circumventable": 0,
    "author_names": [
      "Girish Joshi",
      "Girish Chowdhary"
    ]
  },
  {
    "id": 2123995443,
    "title": "Graph-Based Domain Mapping for Transfer Learning in General Games",
    "abst": "A general game player is an agent capable of taking as input a description of a game&#039;s rules in a formal language and proceeding to play without any subsequent human input. To do well, an agent should learn from experience with past games and transfer the learned knowledge to new problems. We introduce a graph-based method for identifying previously encountered games and prove its robustness formally. We then describe how the same basic approach can be used to identify similar but non-identical games. We apply this technique to automate domain mapping for value function transfer and speed up reinforcement learning on variants of previously played games. Our approach is fully implemented with empirical results in the general game playing system.",
    "url": "http://www.cs.utexas.edu/users/ai-lab/?kuhlmann:ecml07",
    "lang": "en",
    "authors": [
      2147180669,
      2160943816
    ],
    "fos": [
      95940807,
      97541855,
      170828538,
      14642086,
      2780617750,
      102234262,
      73795354,
      167573328,
      154945302,
      41008148
    ],
    "journals": [],
    "conferences": [],
    "conference_series": [
      2755314191
    ],
    "references": [
      95993446,
      135031542,
      1515851193,
      1548156140,
      1984290203,
      2077329922,
      2099587183,
      2106953752,
      2110630796,
      2111572265,
      2121863487,
      2145943363,
      2151259087,
      2154328025,
      2312609093,
      2396715201,
      2911296969,
      3020831056
    ],
    "filter_matches": [
      "tl"
    ],
    "rank": 19443,
    "citation_count": 46,
    "estimated_citation_count": 69,
    "publication_date": "2007-09-17",
    "found_in": 1,
    "transfer_experiment_type": [
      "a",
      "v"
    ],
    "transfer_experiment_subtype": [
      "lit"
    ],
    "transfer_data_type": [
      "Q"
    ],
    "transfer_performance_metrics": [
      "j",
      "tr"
    ],
    "implementation": [
      "Custom",
      "CS",
      "Pseudo",
      "Figures",
      "Formulas",
      "Tables",
      "Theorem"
    ],
    "policy_type": [
      "Q",
      "RuleGraph"
    ],
    "task_mappings": [
      "T"
    ],
    "autonomous_transfer": 0,
    "is_deep_rl": 0,
    "tags": [
      "GGP",
      "Simulation",
      "Games",
      "BoardGames",
      "MiniChess"
    ],
    "allowed_learner": [
      "TD"
    ],
    "country": [
      "USA"
    ],
    "uni": [
      "The University of Texas"
    ],
    "department": [
      "Computer Science"
    ],
    "source_task_selection": [
      "h"
    ],
    "was_in_survey": [
      "taylor"
    ],
    "in_title": [
      "t",
      "l"
    ],
    "in_abs": [
      "t",
      "r",
      "l",
      "k"
    ],
    "in_contet": [
      "t",
      "r",
      "l",
      "k"
    ],
    "rejected_or_unpublished": null,
    "duplication_status": null,
    "paper_for_thesis": 0,
    "paper_available": 1,
    "behind_paywall": 0,
    "paywall_circumventable": 0,
    "author_names": [
      "Peter Stone",
      "Gregory Kuhlmann"
    ]
  },
  {
    "id": 2949876402,
    "title": "Grounding Language for Transfer in Deep Reinforcement Learning",
    "abst": "In this paper, we explore the utilization of natural language to drive transfer for reinforcement learning (RL). Despite the wide-spread application of deep RL techniques, learning generalized policy representations that work across domains remains a challenging problem. We demonstrate that textual descriptions of environments provide a compact intermediate channel to facilitate effective policy transfer. Specifically, by learning to ground the meaning of text to the dynamics of the environment such as transitions and rewards, an autonomous agent can effectively bootstrap policy learning on a new domain given its description. We employ a model-based RL approach consisting of a differentiable planning module, a model-free component and a factorized state representation to effectively use entity descriptions. Our model outperforms prior work on both transfer and multi-task scenarios in a variety of different environments. For instance, we achieve up to 14% and 11.5% absolute improvement over previously existing models in terms of average and initial rewards, respectively.",
    "url": "http://export.arxiv.org/abs/1708.00133",
    "lang": "en",
    "authors": [
      340253981,
      2080327427,
      2337405891
    ],
    "fos": [
      154945302,
      195324797,
      202615002,
      119857082,
      2776731479,
      168993435,
      2989414621,
      97541855,
      13687954,
      156996364,
      41008148
    ],
    "journals": [
      2596401190
    ],
    "conferences": [],
    "conference_series": [],
    "references": [],
    "filter_matches": [
      "tr",
      "tl",
      "trl",
      "rl"
    ],
    "rank": 19468,
    "citation_count": 0,
    "estimated_citation_count": 0,
    "publication_date": "2017-08-01",
    "found_in": 1,
    "transfer_experiment_type": [
      "t",
      "s"
    ],
    "transfer_experiment_subtype": [
      "multi"
    ],
    "transfer_data_type": [
      "pi"
    ],
    "transfer_performance_metrics": [
      "ap",
      "tr"
    ],
    "implementation": [
      "OSS",
      "Custom",
      "Pseudo",
      "Formulas",
      "Figures",
      "Tables"
    ],
    "policy_type": [
      "DQN",
      "AMN",
      "VIN"
    ],
    "task_mappings": [
      "sup",
      "text"
    ],
    "autonomous_transfer": 0,
    "is_deep_rl": 1,
    "tags": [
      "GVGAI",
      "Simulation",
      "Games",
      "MonteCarlo"
    ],
    "allowed_learner": [
      "TD"
    ],
    "country": [
      "USA"
    ],
    "uni": [
      "Princeton University",
      "Massachusetts Institute of Technology"
    ],
    "department": [
      "Computer Science",
      "Artificial Intelligence Laboratory"
    ],
    "source_task_selection": [
      "lib"
    ],
    "was_in_survey": [
      "mag"
    ],
    "in_title": [
      "t",
      "r",
      "l"
    ],
    "in_abs": [
      "t",
      "r",
      "l"
    ],
    "in_contet": [
      "t",
      "r",
      "l",
      "k"
    ],
    "rejected_or_unpublished": null,
    "duplication_status": null,
    "paper_for_thesis": 0,
    "paper_available": 1,
    "behind_paywall": 0,
    "paywall_circumventable": 0,
    "author_names": [
      "Tommi S. Jaakkola",
      "Regina Barzilay",
      "Karthik Narasimhan"
    ]
  },
  {
    "id": 2132057084,
    "title": "Transfer in variable-reward hierarchical reinforcement learning",
    "abst": "Transfer learning seeks to leverage previously learned tasks to achieve faster learning in a new task. In this paper, we consider transfer learning in the context of related but distinct Reinforcement Learning (RL) problems. In particular, our RL problems are derived from Semi-Markov Decision Processes (SMDPs) that share the same transition dynamics but have different reward functions that are linear in a set of reward features. We formally define the transfer learning problem in the context of RL as learning an efficient algorithm to solve any SMDP drawn from a fixed distribution after experiencing a finite number of them. Furthermore, we introduce an online algorithm to solve this problem, Variable-Reward Reinforcement Learning (VRRL), that compactly stores the optimal value functions for several SMDPs, and uses them to optimally initialize the value function for a new SMDP. We generalize our method to a hierarchical RL setting where the different SMDPs share the same task hierarchy. Our experimental results in a simplified real-time strategy domain show that significant transfer learning occurs in both flat and hierarchical settings. Transfer is especially effective in the hierarchical setting where the overall value functions are decomposed into subtask value functions which are more widely amenable to transfer across different SMDPs.",
    "url": "https://doi.org/10.1007/s10994-008-5061-y",
    "lang": "en",
    "authors": [
      1993564419,
      2139785505,
      2150653897,
      2156930872
    ],
    "fos": [
      52692508,
      148764684,
      150899416,
      196921405,
      31170391,
      119857082,
      33923547,
      14646407,
      97541855,
      159886148,
      154945302
    ],
    "journals": [
      62148650
    ],
    "conferences": [],
    "conference_series": [],
    "references": [
      1549353711,
      1557798492,
      1576539603,
      1607304230,
      1936091723,
      1998649829,
      1999874108,
      2004883573,
      2028357975,
      2071279638,
      2089561656,
      2097113539,
      2098723043,
      2109910161,
      2117428849,
      2121517924,
      2128905965,
      2134153324,
      2134779831,
      2136202932,
      2156493855,
      2168359464,
      2334782222
    ],
    "filter_matches": [
      "tr",
      "tl",
      "trl",
      "rl",
      "rlreward",
      "trreward",
      "tlrreward"
    ],
    "rank": 19578,
    "citation_count": 66,
    "estimated_citation_count": 106,
    "publication_date": "2008-12-01",
    "found_in": 1,
    "transfer_experiment_type": [
      "r"
    ],
    "transfer_experiment_subtype": [
      "multi"
    ],
    "transfer_data_type": [
      "pi"
    ],
    "transfer_performance_metrics": [
      "tr"
    ],
    "implementation": [
      "Custom",
      "CS",
      "Formulas",
      "Pseudo",
      "Figures",
      "Theorem"
    ],
    "policy_type": [
      "MAXQ",
      "VRHRL"
    ],
    "task_mappings": [
      "N/A"
    ],
    "autonomous_transfer": 0,
    "is_deep_rl": 0,
    "tags": [
      "RTS",
      "2D",
      "Grid",
      "Simulation"
    ],
    "allowed_learner": [
      "H",
      "MB"
    ],
    "country": [
      "USA"
    ],
    "uni": [
      "Oregon State"
    ],
    "department": [
      "Electrical Engineering and Computer Science"
    ],
    "source_task_selection": [
      "lib"
    ],
    "was_in_survey": [
      "taylor",
      "lazaric",
      "zhu",
      "mag"
    ],
    "in_title": [
      "t",
      "r",
      "l"
    ],
    "in_abs": [
      "t",
      "r",
      "l"
    ],
    "in_contet": [
      "t",
      "r",
      "l"
    ],
    "rejected_or_unpublished": null,
    "duplication_status": null,
    "paper_for_thesis": 0,
    "paper_available": 1,
    "behind_paywall": 0,
    "paywall_circumventable": 0,
    "author_names": [
      "Prasad Tadepalli",
      "Alan Fern",
      "Neville Mehta",
      "Sriraam Natarajan"
    ]
  },
  {
    "id": 36691172,
    "title": "Using Homomorphisms to transfer options across continuous reinforcement learning domains",
    "abst": "We examine the problem of Transfer in Reinforcement Learning and present a method to utilize knowledge acquired in one Markov Decision Process (MDP) to bootstrap learning in a more complex but related MDP. We build on work in model minimization in Reinforcement Learning to define relationships between state-action pairs of the two MDPs. Our main contribution in this work is to provide a way to compactly represent such mappings using relationships between state variables in the two domains. We use these functions to transfer a learned policy in the first domain into an option in the new domain, and apply intra-option learning methods to bootstrap learning in the new domain. We first evaluate our approach in the well known Blocksworld domain. We then demonstrate that our approach to transfer is viable in a complex domain with a continuous state space by evaluating it in the Robosoccer Keepaway domain.",
    "url": "https://dl.acm.org/citation.cfm?id=1597538.1597618",
    "lang": "en",
    "authors": [
      2102570927,
      2666828694
    ],
    "fos": [
      199190896,
      41008148,
      119857082,
      58973888,
      8038995,
      106189395,
      28006648,
      77075516,
      154945302,
      188116033,
      126255220,
      97541855
    ],
    "journals": [],
    "conferences": [
      2786207413
    ],
    "conference_series": [
      1184914352
    ],
    "references": [
      1494114146,
      1515851193,
      1534331386,
      1564393562,
      1688218840,
      2104641222,
      2109910161,
      2121863487,
      2126565096,
      2170400507
    ],
    "filter_matches": [
      "tr",
      "tl",
      "trl",
      "rl"
    ],
    "rank": 19603,
    "citation_count": 56,
    "estimated_citation_count": 85,
    "publication_date": "2006-07-16",
    "found_in": 1,
    "transfer_experiment_type": [
      "a",
      "v"
    ],
    "transfer_experiment_subtype": [
      "lit"
    ],
    "transfer_data_type": [
      "N/A"
    ],
    "transfer_performance_metrics": [
      "ap",
      "j",
      "tr"
    ],
    "implementation": [
      "Custom",
      "CS",
      "Pseudo",
      "Figures",
      "Formulas"
    ],
    "policy_type": [
      "Q",
      "IntraOptionQ",
      "CMAC"
    ],
    "task_mappings": [
      "Ma",
      "svg",
      "exp"
    ],
    "autonomous_transfer": 0,
    "is_deep_rl": 0,
    "tags": [
      "2D",
      "Navigation",
      "Blocksworld",
      "KeepAway",
      "RoboCup",
      "Simulation",
      "MultiAgent",
      "3v2",
      "4v3"
    ],
    "allowed_learner": [
      "all",
      "H"
    ],
    "country": [
      "USA"
    ],
    "uni": [
      "University of Michigan"
    ],
    "department": [
      "Computer Science and Engineering"
    ],
    "source_task_selection": [
      "h"
    ],
    "was_in_survey": [
      "taylor",
      "taylor-intertask",
      "lazaric",
      "cur",
      "mag"
    ],
    "in_title": [
      "t",
      "r",
      "l"
    ],
    "in_abs": [
      "t",
      "r",
      "l",
      "k"
    ],
    "in_contet": [
      "t",
      "r",
      "l",
      "k",
      "s"
    ],
    "rejected_or_unpublished": null,
    "duplication_status": null,
    "paper_for_thesis": 0,
    "paper_available": 1,
    "behind_paywall": 0,
    "paywall_circumventable": 0,
    "author_names": [
      "Satinder Singh",
      "Vishal Soni"
    ]
  },
  {
    "id": 1848094219,
    "title": "Unsupervised cross-domain transfer in policy gradient reinforcement learning via manifold alignment",
    "abst": "The success of applying policy gradient reinforcement learning (RL) to difficult control tasks hinges crucially on the ability to determine a sensible initialization for the policy. Transfer learning methods tackle this problem by reusing knowledge gleaned from solving other related tasks. In the case of multiple task domains, these algorithms require an inter-task mapping to facilitate knowledge transfer across domains. However, there are currently no general methods to learn an inter-task mapping without requiring either background knowledge that is not typically present in RL settings, or an expensive analysis of an exponential number of inter-task mappings in the size of the state and action spaces.This paper introduces an autonomous framework that uses unsupervised manifold alignment to learn intertask mappings and effectively transfer samples between different task domains. Empirical results on diverse dynamical systems, including an application to quadrotor control, demonstrate its effectiveness for cross-domain transfer in the context of policy gradient RL.",
    "url": "https://dl.acm.org/citation.cfm?id=2886521.2886669",
    "lang": "en",
    "authors": [
      2125814586,
      2148762994,
      2215740056,
      2313896761
    ],
    "fos": [
      119857082,
      41008148,
      2776960227,
      150899416,
      206588197,
      97541855,
      79379906,
      153120616,
      2992168537,
      154945302,
      114466953
    ],
    "journals": [],
    "conferences": [
      2787475599
    ],
    "conference_series": [
      1184914352
    ],
    "references": [
      64698994,
      110451278,
      204815387,
      1492014007,
      1626155273,
      1977655452,
      1986014385,
      1993170675,
      2090679187,
      2091714857,
      2097381042,
      2098723043,
      2106953752,
      2114537044,
      2127107099,
      2133040789,
      2145983511,
      2154328025,
      2155027007,
      2156493855,
      2158150115,
      2172968643
    ],
    "filter_matches": [
      "tr",
      "tl",
      "trl",
      "rl"
    ],
    "rank": 19618,
    "citation_count": 31,
    "estimated_citation_count": 31,
    "publication_date": "2015-01-25",
    "found_in": 1,
    "transfer_experiment_type": [
      "t",
      "a",
      "v"
    ],
    "transfer_experiment_subtype": [
      "diff-no",
      "lit"
    ],
    "transfer_data_type": [
      "I",
      "pi",
      "manifold"
    ],
    "transfer_performance_metrics": [
      "j",
      "tt",
      "ap",
      "tr"
    ],
    "implementation": [
      "Custom",
      "CS",
      "Formulas",
      "Figures",
      "Pseudo"
    ],
    "policy_type": [
      "PG",
      "MAXDT-PG"
    ],
    "task_mappings": [
      "exp"
    ],
    "autonomous_transfer": 1,
    "is_deep_rl": 0,
    "tags": [
      "2D",
      "CartPole",
      "SimpleMass",
      "CartPoleThreeLink",
      "Simulation",
      "ClassicControl"
    ],
    "allowed_learner": [
      "TD"
    ],
    "country": [
      "USA"
    ],
    "uni": [
      "University of Pennsylvania",
      "Olin College of Engineering",
      "Washington State University"
    ],
    "department": [],
    "source_task_selection": [
      "h"
    ],
    "was_in_survey": [
      "mag",
      "zhu"
    ],
    "in_title": [
      "t",
      "r",
      "l"
    ],
    "in_abs": [
      "t",
      "r",
      "l"
    ],
    "in_contet": [
      "t",
      "r",
      "l",
      "k"
    ],
    "rejected_or_unpublished": null,
    "duplication_status": null,
    "paper_for_thesis": 0,
    "paper_available": 1,
    "behind_paywall": 0,
    "paywall_circumventable": 0,
    "author_names": [
      "Eric Eaton",
      "Matthew D. Taylor",
      "Haitham Bou Ammar",
      "Paul Ruvolo"
    ]
  },
  {
    "id": 110451278,
    "title": "Reinforcement learning transfer via sparse coding",
    "abst": "Although reinforcement learning (RL) has been successfully deployed in a variety of tasks, learning speed remains a fundamental problem for applying RL in complex environments. Transfer learning aims to ameliorate this shortcoming by speeding up learning through the adaptation of previously learned behaviors in similar tasks. Transfer techniques often use an inter-task mapping, which determines how a pair of tasks are related. Instead of relying on a hand-coded inter-task mapping, this paper proposes a novel transfer learning method capable of autonomously creating an inter-task mapping by using a novel combination of sparse coding, sparse projection learning and sparse Gaussian processes. We also propose two new transfer algorithms (TrLSPI and TrFQI) based on least squares policy iteration and fitted-Q-iteration. Experiments not only show successful transfer of information between similar tasks, inverted pendulum to cart pole, but also between two very different domains: mountain car to cart pole. This paper empirically shows that the learned inter-task mapping can be successfully used to (1) improve the performance of a learned policy on a fixed number of environmental samples, (2) reduce the learning times needed by the algorithms to converge to a policy on a fixed number of samples, and (3) converge faster to a near-optimal policy given a large number of samples.",
    "url": "https://cris.maastrichtuniversity.nl/portal/en/publications/reinforcement-learning-transfer-via-sparse-coding(55f157dc-7588-4af7-9d75-bf299bd3e19e).html",
    "lang": "en",
    "authors": [
      1982051687,
      2027870957,
      2142969897,
      2148762994,
      2215740056
    ],
    "fos": [
      77637269,
      119857082,
      28006648,
      41008148,
      192921069,
      154945302,
      58973888,
      77075516,
      97541855,
      150899416,
      199190896
    ],
    "journals": [],
    "conferences": [
      2787266288
    ],
    "conference_series": [
      1168671587
    ],
    "references": [
      24477102,
      203338875,
      1515851193,
      1626155273,
      2004030284,
      2012036715,
      2027197817,
      2079247031,
      2097381042,
      2098723043,
      2099768828,
      2113606819,
      2121863487,
      2122922389,
      2123995443,
      2130005627,
      2133040789,
      2153353285,
      2154328025,
      2158150115,
      2165698076
    ],
    "filter_matches": [
      "tr",
      "tl",
      "trl",
      "rl"
    ],
    "rank": 19660,
    "citation_count": 36,
    "estimated_citation_count": 60,
    "publication_date": "2012-06-04",
    "found_in": 1,
    "transfer_experiment_type": [
      "t",
      "a",
      "v"
    ],
    "transfer_experiment_subtype": [
      "diff-no",
      "lit"
    ],
    "transfer_data_type": [
      "Q",
      "I"
    ],
    "transfer_performance_metrics": [
      "tt",
      "tr"
    ],
    "implementation": [
      "Custom",
      "CS",
      "Formulas",
      "Figures",
      "Pseudo"
    ],
    "policy_type": [
      "LSPI",
      "FQI"
    ],
    "task_mappings": [
      "exp"
    ],
    "autonomous_transfer": 0,
    "is_deep_rl": 0,
    "tags": [
      "2D",
      "Simulation",
      "MountainCar",
      "InvertedPendulum",
      "CartPole",
      "ClassicControl"
    ],
    "allowed_learner": [
      "TD",
      "batch"
    ],
    "country": [
      "Netherlands",
      "USA"
    ],
    "uni": [
      "Maastricht University",
      "Lafayette College"
    ],
    "department": [],
    "source_task_selection": [
      "h"
    ],
    "was_in_survey": [
      "zhu",
      "mag"
    ],
    "in_title": [
      "t",
      "r",
      "l"
    ],
    "in_abs": [
      "t",
      "r",
      "l"
    ],
    "in_contet": [
      "t",
      "r",
      "l",
      "k"
    ],
    "rejected_or_unpublished": null,
    "duplication_status": 1,
    "paper_for_thesis": 126751897,
    "paper_available": 1,
    "behind_paywall": 0,
    "paywall_circumventable": 0,
    "author_names": [
      "Kurt Driessens",
      "Karl Tuyls",
      "Gerhard Weiss",
      "Matthew D. Taylor",
      "Haitham Bou Ammar"
    ]
  },
  {
    "id": 2098723043,
    "title": "Value-function-based transfer for reinforcement learning using structure mapping",
    "abst": "Transfer learning concerns applying knowledge learned in one task (the source) to improve learning another related task (the target). In this paper, we use structure mapping, a psychological and computational theory about analogy making, to find mappings between the source and target tasks and thus construct the transfer functional automatically. Our structure mapping algorithm is a specialized and optimized version of the structure mapping engine and uses heuristic search to find the best maximal mapping. The algorithm takes as input the source and target task specifications represented as qualitative dynamic Bayes networks, which do not need probability information. We apply this method to the Keepaway task from RoboCup simulated soccer and compare the result from automated transfer to that from handcoded transfer.",
    "url": "https://www.aaai.org/Library/AAAI/2006/aaai06-066.php",
    "lang": "en",
    "authors": [
      2104164346,
      2147180669
    ],
    "fos": [
      41008148,
      24858836,
      154945302,
      97541855,
      77075516,
      119857082,
      207201462,
      28006648,
      173801870,
      150899416,
      50648415
    ],
    "journals": [],
    "conferences": [
      2786207413
    ],
    "conference_series": [
      1184914352
    ],
    "references": [
      1517018472,
      1554015367,
      1576539603,
      1989486129,
      2020294948,
      2026161499,
      2061504687,
      2121863487,
      2128905965,
      2134153324,
      2143891888,
      2145454741,
      2145805610,
      2169659168,
      2170400507,
      2911283634,
      2914656440
    ],
    "filter_matches": [
      "tr",
      "tl",
      "trl",
      "rl"
    ],
    "rank": 19662,
    "citation_count": 66,
    "estimated_citation_count": 92,
    "publication_date": "2006-07-16",
    "found_in": 1,
    "transfer_experiment_type": [
      "a",
      "v"
    ],
    "transfer_experiment_subtype": [
      "lit"
    ],
    "transfer_data_type": [
      "N/A"
    ],
    "transfer_performance_metrics": [],
    "implementation": [
      "Custom",
      "CS",
      "Pseudo",
      "Formulas",
      "Figures",
      "Tables"
    ],
    "policy_type": [
      "QDBN"
    ],
    "task_mappings": [
      "exp"
    ],
    "autonomous_transfer": 0,
    "is_deep_rl": 0,
    "tags": [
      "2D",
      "RoboCup",
      "BreakAway",
      "Simulation",
      "Games",
      "MultiAgent",
      "Qualitative Dynamic Bayes Network",
      "3v2",
      "4v3"
    ],
    "allowed_learner": [
      "all"
    ],
    "country": [
      "USA"
    ],
    "uni": [
      "The University of Texas"
    ],
    "department": [
      "Computer Science"
    ],
    "source_task_selection": [
      "h"
    ],
    "was_in_survey": [
      "taylor",
      "taylor-intertask",
      "mag"
    ],
    "in_title": [
      "t",
      "r",
      "l"
    ],
    "in_abs": [
      "t",
      "r",
      "l",
      "k"
    ],
    "in_contet": [
      "t",
      "r",
      "l",
      "k"
    ],
    "rejected_or_unpublished": null,
    "duplication_status": null,
    "paper_for_thesis": 0,
    "paper_available": 1,
    "behind_paywall": 0,
    "paywall_circumventable": 0,
    "author_names": [
      "Yaxin Liu",
      "Peter Stone"
    ]
  },
  {
    "id": 203338875,
    "title": "An experts algorithm for transfer learning",
    "abst": "A long-lived agent continually faces new tasks in its environment. Such an agent may be able to use knowledge learned in solving earlier tasks to produce candidate policies for its current task. There may, however, be multiple reasonable policies suggested by prior experience, and the agent must choose between them potentially without any a priori knowledge about their applicability to its current situation. We present an &quot;experts&quot; algorithm for efficiently choosing amongst candidate policies in solving an unknown Markov decision process task. We conclude with the results of experiments on two domains in which we generate candidate policies from solutions to related tasks and use our experts algorithm to choose amongst them.",
    "url": "https://dl.acm.org/citation.cfm?id=1625275.1625448",
    "lang": null,
    "authors": [
      2090935536,
      2102570927
    ],
    "fos": [
      75553542,
      150899416,
      154945302,
      41008148,
      106189395,
      119857082,
      11413529
    ],
    "journals": [],
    "conferences": [
      2793510348
    ],
    "conference_series": [
      1203999783
    ],
    "references": [
      79394677,
      1516061453,
      1517018472,
      1970041563,
      1979675141,
      1998498767,
      2009551863,
      2011277999,
      2028357975,
      2077902449,
      2098339418,
      2105507006,
      2126565096,
      2169659168,
      2489939061
    ],
    "filter_matches": [
      "tl"
    ],
    "rank": 19737,
    "citation_count": 42,
    "estimated_citation_count": 58,
    "publication_date": "2007-01-06",
    "found_in": 1,
    "transfer_experiment_type": [
      "a",
      "v"
    ],
    "transfer_experiment_subtype": [
      "lit"
    ],
    "transfer_data_type": [
      "N/A"
    ],
    "transfer_performance_metrics": [
      "j"
    ],
    "implementation": [
      "Custom",
      "CS",
      "Pseudo",
      "Formulas",
      "Figures",
      "Theorem"
    ],
    "policy_type": [
      "SARSA",
      "AtEasel"
    ],
    "task_mappings": [
      "Ma",
      "svg",
      "exp"
    ],
    "autonomous_transfer": 0,
    "is_deep_rl": 0,
    "tags": [
      "2D",
      "RoboCup",
      "BreakAway",
      "Simulation",
      "MultiAgent"
    ],
    "allowed_learner": [
      "all"
    ],
    "country": [
      "USA"
    ],
    "uni": [
      "University of Michigan"
    ],
    "department": [
      "Computer Science and Engineering"
    ],
    "source_task_selection": [
      "h"
    ],
    "was_in_survey": [
      "taylor"
    ],
    "in_title": [
      "t",
      "l"
    ],
    "in_abs": [
      "t",
      "l",
      "k"
    ],
    "in_contet": [
      "t",
      "r",
      "l",
      "k"
    ],
    "rejected_or_unpublished": null,
    "duplication_status": null,
    "paper_for_thesis": 0,
    "paper_available": 1,
    "behind_paywall": 0,
    "paywall_circumventable": 0,
    "author_names": [
      "Erik Talvitie",
      "Satinder Singh"
    ]
  },
  {
    "id": 2281071090,
    "title": "Unsupervised energy prediction in a Smart Grid context using reinforcement cross-building transfer learning",
    "abst": "Abstract In a future Smart Grid context, increasing challenges in managing the stochastic local energy supply and demand are expected. This increased the need of more accurate energy prediction methods in order to support further complex decision-making processes. Although many methods aiming to predict the energy consumption exist, all these require labelled data, such as historical or simulated data. Still, such datasets are not always available under the emerging Smart Grid transition and complex people behaviour. Our approach goes beyond the state-of-the-art energy prediction methods in that it does not require labelled data. Firstly, two reinforcement learning algorithms are investigated in order to model the building energy consumption. Secondly, as a main theoretical contribution, a Deep Belief Network (DBN) is incorporated into each of these algorithms, making them suitable for continuous states. Thirdly, the proposed methods yield a cross-building transfer that can target new behaviour of existing buildings (due to changes in their structure or installations), as well as completely new types of buildings. The methods are developed in the MATLAB&#x00AE; environment and tested on a real database recorded over seven years, with hourly resolution. Experimental results demonstrate that the energy prediction accuracy in terms of RMSE has been significantly improved in 91.42% of the cases after using a DBN for automatically extracting high-level features from the unlabelled data, compared to the equivalent methods without the DBN pre-processing.",
    "url": "http://www.sciencedirect.com/science/article/pii/S0378778816300305",
    "lang": "en",
    "authors": [
      1970372426,
      2052779821,
      2106581526,
      2956138870
    ],
    "fos": [
      154945302,
      10558101,
      67203356,
      97541855,
      150899416,
      97385483,
      3017484106,
      124101348,
      139945424,
      119857082,
      2780165032,
      41008148
    ],
    "journals": [
      6846437
    ],
    "conferences": [],
    "conference_series": [],
    "references": [
      32403112,
      44815768,
      97016928,
      142185896,
      1219860795,
      1515851193,
      1557517019,
      1564017552,
      1980618555,
      1980746328,
      1980972971,
      1985336195,
      1997334587,
      2002070806,
      2022771152,
      2023460138,
      2026980178,
      2033065921,
      2042131079,
      2051607409,
      2061434834,
      2071151783,
      2072128103,
      2081060955,
      2096018174,
      2100495367,
      2101651041,
      2101786389,
      2116064496,
      2117341272,
      2119567691,
      2125526403,
      2136922672,
      2137570937,
      2145339207,
      2145805610,
      2161257182,
      2166481425,
      2181867278,
      2182304831
    ],
    "filter_matches": [
      "tr",
      "tl",
      "trl",
      "rl"
    ],
    "rank": 19743,
    "citation_count": 40,
    "estimated_citation_count": 52,
    "publication_date": "2016-03-15",
    "found_in": 1,
    "transfer_experiment_type": [
      "t"
    ],
    "transfer_experiment_subtype": [
      "same_all"
    ],
    "transfer_data_type": [
      "Q"
    ],
    "transfer_performance_metrics": [
      "ap",
      "tr"
    ],
    "implementation": [
      "Custom",
      "CS",
      "Pseudo",
      "Formulas",
      "Figures",
      "Tables"
    ],
    "policy_type": [
      "SARSA",
      "DBN",
      "Q"
    ],
    "task_mappings": [
      "N/A"
    ],
    "autonomous_transfer": 0,
    "is_deep_rl": 1,
    "tags": [
      "Deep Belief Network",
      "Power",
      "Efficiency",
      "SmartGrid",
      "RBM",
      "Simulation"
    ],
    "allowed_learner": [
      "TD"
    ],
    "country": [
      "Netherlands"
    ],
    "uni": [
      "Technical University of Eindhoven"
    ],
    "department": [
      "Electrical Engineering"
    ],
    "source_task_selection": [
      "h"
    ],
    "was_in_survey": [
      "mag"
    ],
    "in_title": [
      "t",
      "r",
      "l"
    ],
    "in_abs": [
      "t",
      "r",
      "l"
    ],
    "in_contet": [
      "t",
      "r",
      "l",
      "k"
    ],
    "rejected_or_unpublished": null,
    "duplication_status": null,
    "paper_for_thesis": 0,
    "paper_available": 1,
    "behind_paywall": 0,
    "paywall_circumventable": 0,
    "author_names": [
      "Madeleine Gibescu",
      "WL Wil Kling",
      "Elena Mocanu",
      "HP Phuong Nguyen"
    ]
  },
  {
    "id": 2963983495,
    "title": "Theoretically-grounded policy advice from multiple teachers in reinforcement learning settings with applications to negative transfer",
    "abst": "Policy advice is a transfer learning method where a student agent is able to learn faster via advice from a teacher. However, both this and other reinforcement learning transfer methods have little theoretical analysis. This paper formally defines a setting where multiple teacher agents can provide advice to a student and introduces an algorithm to leverage both autonomous exploration and teacher&#039;s advice. Our regret bounds justify the intuition that good teachers help while bad teachers hurt. Using our formalization, we are also able to quantify, for the first time, when negative transfer can occur within such a reinforcement learning setting.",
    "url": "https://dl.acm.org/citation.cfm?id=3060945",
    "lang": "en",
    "authors": [
      2112607260,
      2148762994,
      2215740056
    ],
    "fos": [
      81931697,
      150899416,
      132010649,
      41008148,
      145420912,
      50817715,
      119857082,
      154945302,
      2779178101,
      97541855
    ],
    "journals": [],
    "conferences": [
      2321817494
    ],
    "conference_series": [
      1203999783
    ],
    "references": [
      158722652,
      950880443,
      1505937442,
      1515851193,
      1573527757,
      1583155004,
      1586504939,
      1658094677,
      1662803991,
      1850488217,
      1969685488,
      1986014385,
      2031727428,
      2097381042,
      2098441518,
      2119567691,
      2159459871,
      2171578145,
      2290053245,
      2489939061,
      2735506162,
      2949677831
    ],
    "filter_matches": [
      "tr",
      "tl",
      "trl",
      "rl"
    ],
    "rank": 19778,
    "citation_count": 2,
    "estimated_citation_count": 2,
    "publication_date": "2016-07-09",
    "found_in": 1,
    "transfer_experiment_type": [],
    "transfer_experiment_subtype": [
      "same_all"
    ],
    "transfer_data_type": [
      "advisor"
    ],
    "transfer_performance_metrics": [
      "j",
      "tt"
    ],
    "implementation": [
      "Custom",
      "CS",
      "Formulas",
      "Figures",
      "Pseudo",
      "Theorem",
      "Lemma"
    ],
    "policy_type": [
      "Q"
    ],
    "task_mappings": [
      "N/A"
    ],
    "autonomous_transfer": 0,
    "is_deep_rl": 0,
    "tags": [
      "2D",
      "Navigation",
      "Simulation",
      "Grid",
      "CombinationLock",
      "Maze",
      "BlockDude"
    ],
    "allowed_learner": [
      "TD"
    ],
    "country": [
      "USA"
    ],
    "uni": [
      "Washington State University",
      "Princeton University"
    ],
    "department": [],
    "source_task_selection": [],
    "was_in_survey": [
      "mag",
      "silva"
    ],
    "in_title": [
      "t",
      "r",
      "l"
    ],
    "in_abs": [
      "t",
      "r",
      "l"
    ],
    "in_contet": [
      "t",
      "r",
      "l",
      "k"
    ],
    "rejected_or_unpublished": null,
    "duplication_status": null,
    "paper_for_thesis": 0,
    "paper_available": 1,
    "behind_paywall": 0,
    "paywall_circumventable": 0,
    "author_names": [
      "Yusen Zhan",
      "Matthew D. Taylor",
      "Haitham Bou Ammar"
    ]
  },
  {
    "id": 2165792602,
    "title": "Improving action selection in MDP&#039;s via knowledge transfer",
    "abst": "Temporal-difference reinforcement learning (RL) has been successfully applied in several domains with large state sets. Large action sets, however, have received considerably less attention. This paper demonstrates the use of knowledge transfer between related tasks to accelerate learning with large action sets. We introduce action transfer, a technique that extracts the actions from the (near-)optimal solution to the first task and uses them in place of the full action set when learning any subsequent tasks. When optimal actions make up a small fraction of the domain&#039;s action set, action transfer can substantially reduce the number of actions and thus the complexity of the problem. However, action transfer between dissimilar tasks can be detrimental. To address this difficulty, we contribute randomized task perturbation (RTP), an enhancement to action transfer that makes it robust to unrepresentative source tasks. We motivate RTP action transfer with a detailed theoretical analysis featuring a formalism of related tasks and a bound on the suboptimality of action transfer. The empirical results in this paper show the potential of RTP action transfer to substantially expand the applicability of RL to problems with large action sets.",
    "url": "http://www.cs.utexas.edu/users/ai-lab/?AAAI05-actions",
    "lang": "en",
    "authors": [
      183355381,
      2147180669
    ],
    "fos": [
      97541855,
      119857082,
      38706069,
      2776960227,
      162696548,
      166109690,
      154945302,
      41008148
    ],
    "journals": [],
    "conferences": [
      2785841295
    ],
    "conference_series": [
      1184914352
    ],
    "references": [
      1515851193,
      1631187438,
      1800916125,
      2041367235,
      2117341272,
      2121517924,
      2121863487,
      2134153324,
      2154549708,
      2155791599,
      2160279936,
      3011120880
    ],
    "filter_matches": [
      "ias"
    ],
    "rank": 19780,
    "citation_count": 62,
    "estimated_citation_count": 93,
    "publication_date": "2005-07-09",
    "found_in": 1,
    "transfer_experiment_type": [
      "s_f",
      "t"
    ],
    "transfer_experiment_subtype": [
      "same_all"
    ],
    "transfer_data_type": [
      "A"
    ],
    "transfer_performance_metrics": [
      "tr"
    ],
    "implementation": [
      "Custom",
      "CS",
      "Pseudo",
      "Formulas",
      "Figures",
      "Lemma",
      "Theorem"
    ],
    "policy_type": [
      "Q",
      "RTP"
    ],
    "task_mappings": [
      "N/A"
    ],
    "autonomous_transfer": 0,
    "is_deep_rl": 0,
    "tags": [
      "2D",
      "Grid",
      "Navigation",
      "Simulation"
    ],
    "allowed_learner": [
      "TD"
    ],
    "country": [
      "USA"
    ],
    "uni": [
      "The University of Texas"
    ],
    "department": [
      "Computer Science"
    ],
    "source_task_selection": [
      "mod"
    ],
    "was_in_survey": [
      "taylor"
    ],
    "in_title": [
      "k",
      "t"
    ],
    "in_abs": [
      "t",
      "r",
      "l",
      "k"
    ],
    "in_contet": [
      "t",
      "r",
      "l",
      "k"
    ],
    "rejected_or_unpublished": null,
    "duplication_status": null,
    "paper_for_thesis": 0,
    "paper_available": 1,
    "behind_paywall": 0,
    "paywall_circumventable": 0,
    "author_names": [
      "Peter Stone"
    ]
  },
  {
    "id": 2235081654,
    "title": "Autonomous cross-domain knowledge transfer in lifelong policy gradient reinforcement learning",
    "abst": "Online multi-task learning is an important capability for lifelong learning agents, enabling them to acquire models for diverse tasks over time and rapidly learn new tasks by building upon prior experience. However, recent progress toward lifelong reinforcement learning (RL) has been limited to learning from within a single task domain. For truly versatile lifelong learning, the agent must be able to autonomously transfer knowledge between different task domains. A few methods for cross-domain transfer have been developed, but these methods are computationally inefficient for scenarios where the agent must learn tasks consecutively.In this paper, we develop the first cross-domain lifelong RL framework. Our approach efficiently optimizes a shared repository of transferable knowledge and learns projection matrices that specialize that knowledge to different task domains. We provide rigorous theoretical guarantees on the stability of this approach, and empirically evaluate its performance on diverse dynamical systems. Our results show that the proposed method can learn effectively from interleaved task domains and rapidly acquire high performance in new domains.",
    "url": "http://www.seas.upenn.edu/~eeaton/papers/BouAmmar2015Autonomous.pdf",
    "lang": "en",
    "authors": [
      2125814586,
      2144772322,
      2215740056,
      2313896761
    ],
    "fos": [
      28006648,
      79379906,
      154945302,
      108771440,
      207685749,
      41008148,
      3020089516,
      97541855
    ],
    "journals": [],
    "conferences": [],
    "conference_series": [
      2755283090
    ],
    "references": [
      110451278,
      1519626139,
      1560550898,
      1942758450,
      1969074599,
      1996342763,
      2012392077,
      2097381042,
      2098432798,
      2099618002,
      2103285838,
      2106008664,
      2114235770,
      2114537044,
      2119717200,
      2123327324,
      2126635028,
      2134197408,
      2135046866,
      2135316123,
      2153988437,
      2154328025,
      2155027007,
      2158150115,
      2169743339,
      2181867278,
      2186054958,
      2903077804
    ],
    "filter_matches": [
      "tr",
      "tl",
      "trl",
      "rl"
    ],
    "rank": 19783,
    "citation_count": 24,
    "estimated_citation_count": 24,
    "publication_date": "2015-07-25",
    "found_in": 1,
    "transfer_experiment_type": [
      "a",
      "p",
      "r",
      "t",
      "v"
    ],
    "transfer_experiment_subtype": [
      "lit",
      "multi"
    ],
    "transfer_data_type": [
      "sub",
      "fea",
      "pi"
    ],
    "transfer_performance_metrics": [
      "j"
    ],
    "implementation": [
      "Custom",
      "CS",
      "Formulas",
      "Theorem",
      "Figures"
    ],
    "policy_type": [
      "Cross-Domain",
      "PG-Ella",
      "PG"
    ],
    "task_mappings": [
      "exp"
    ],
    "autonomous_transfer": 1,
    "is_deep_rl": 0,
    "tags": [
      "Cross-Domain",
      "Lifelong",
      "PolicyGradient",
      "SimpleMass",
      "DoubleMass",
      "CartPole",
      "DoubleCartPole",
      "Bicycle",
      "Helicpoter",
      "Simulation",
      "ClassicControl"
    ],
    "allowed_learner": [
      "TD"
    ],
    "country": [
      "USA"
    ],
    "uni": [
      "University of Pennsylvania",
      "Olin College of Engineering"
    ],
    "department": [
      "Computer Science"
    ],
    "source_task_selection": [
      "all"
    ],
    "was_in_survey": [
      "mag"
    ],
    "in_title": [
      "t",
      "r",
      "l",
      "k"
    ],
    "in_abs": [
      "t",
      "r",
      "l",
      "k"
    ],
    "in_contet": [
      "t",
      "r",
      "l",
      "k"
    ],
    "rejected_or_unpublished": null,
    "duplication_status": null,
    "paper_for_thesis": 0,
    "paper_available": 1,
    "behind_paywall": 0,
    "paywall_circumventable": 0,
    "author_names": [
      "Eric Eaton",
      "Jos&#x00E9; Marcio Luna",
      "Haitham Bou Ammar",
      "Paul Ruvolo"
    ]
  },
  {
    "id": 2117629901,
    "title": "A comparison of direct and model-based reinforcement learning",
    "abst": "This paper compares direct reinforcement learning (no explicit model) and model-based reinforcement learning on a simple task: pendulum swing up. We find that in this task model-based approaches support reinforcement learning from smaller amounts of training data and efficient handling of changing goals.",
    "url": "https://dblp.uni-trier.de/db/conf/icra/icra1997.html#AtkesonS97a",
    "lang": "en",
    "authors": [
      2161070301,
      2649737133
    ],
    "fos": [
      2994509759,
      17500928,
      90509273,
      196340769,
      199190896,
      51632099,
      133731056,
      154945302,
      65244806,
      110639684,
      119857082,
      127413603,
      97541855
    ],
    "journals": [],
    "conferences": [],
    "conference_series": [
      1163902177
    ],
    "references": [
      1491843047,
      1585546214,
      1595634327,
      1597173708,
      1616818660,
      1689445748,
      1966195676,
      1972586294,
      1990005421,
      2002971752,
      2009533501,
      2013232999,
      2048226872,
      2080759927,
      2083143894,
      2105038027,
      2107726111,
      2116039916,
      2118426468,
      2119717200,
      2121832485,
      2124175081,
      2131398727,
      2131600418,
      2147766102,
      2463510513
    ],
    "filter_matches": [
      "rl"
    ],
    "rank": 19864,
    "citation_count": 161,
    "estimated_citation_count": 243,
    "publication_date": "1997-04-20",
    "found_in": 1,
    "transfer_experiment_type": [
      "r"
    ],
    "transfer_experiment_subtype": [
      "same_all"
    ],
    "transfer_data_type": [
      "model"
    ],
    "transfer_performance_metrics": [
      "j",
      "ap",
      "tr"
    ],
    "implementation": [
      "Custom",
      "CS",
      "Formulas",
      "Figures"
    ],
    "policy_type": [
      "Q",
      "CMAC"
    ],
    "task_mappings": [
      "N/A"
    ],
    "autonomous_transfer": 0,
    "is_deep_rl": 0,
    "tags": [
      "Robotics",
      "Simulation"
    ],
    "allowed_learner": [
      "MB"
    ],
    "country": [
      "USA"
    ],
    "uni": [
      "Georgia Institute of Technology"
    ],
    "department": [
      "College of Computing"
    ],
    "source_task_selection": [
      "all"
    ],
    "was_in_survey": [
      "taylor"
    ],
    "in_title": [
      "r",
      "l"
    ],
    "in_abs": [
      "r",
      "l"
    ],
    "in_contet": [
      "t",
      "r",
      "l"
    ],
    "rejected_or_unpublished": null,
    "duplication_status": null,
    "paper_for_thesis": 0,
    "paper_available": 1,
    "behind_paywall": 1,
    "paywall_circumventable": 1,
    "author_names": [
      "Christopher G. Atkeson",
      "J.C. Santamaria"
    ]
  },
  {
    "id": 2141559023,
    "title": "An automated measure of MDP similarity for transfer in reinforcement learning",
    "abst": "Transfer learning can improve the reinforcement learning of a new task by allowing the agent to reuse knowledge acquired from other source tasks. Despite their success, transfer learning methods rely on having relevant source tasks; transfer from inappropriate tasks can inhibit performance on the new task. For fully autonomous transfer, it is critical to have a method for automatically choosing relevant source tasks, which requires a similarity measure between Markov Decision Processes (MDPs). This issue has received little attention, and is therefore still a largely open problem. This paper presents a data-driven automated similarity measure for MDPs. This novel measure is a significant step toward autonomous reinforcement learning transfer, allowing agents to: (1) characterize when transfer will be useful and, (2) automatically select tasks to use for transfer. The proposed measure is based on the reconstruction error of a restricted Boltzmann machine that attempts to model the behavioral dynamics of the two MDPs being compared. Empirical results illustrate that this measure is correlated with the performance of transfer and therefore can be used to identify similar source tasks for transfer learning.",
    "url": "http://www.seas.upenn.edu/~eeaton/papers/BouAmmar2014Automated.pdf",
    "lang": "en",
    "authors": [
      1982051687,
      1984519118,
      2027870957,
      2125814586,
      2142969897,
      2148762994,
      2215740056
    ],
    "fos": [
      77075516,
      58973888,
      97541855,
      28006648,
      119857082,
      106189395,
      150899416,
      2776517306,
      41008148,
      199190896,
      154945302
    ],
    "journals": [],
    "conferences": [
      3685113
    ],
    "conference_series": [
      1184914352
    ],
    "references": [
      97016928,
      110451278,
      1506146479,
      1626155273,
      1995688924,
      2072128103,
      2097381042,
      2099866409,
      2104228245,
      2110292307,
      2116064496,
      2133040789,
      2181867278,
      2964331425
    ],
    "filter_matches": [
      "tr",
      "tl",
      "trl",
      "rl"
    ],
    "rank": 19870,
    "citation_count": 30,
    "estimated_citation_count": 30,
    "publication_date": "2014-06-18",
    "found_in": 1,
    "transfer_experiment_type": [
      "t"
    ],
    "transfer_experiment_subtype": [
      "multi"
    ],
    "transfer_data_type": [
      "Q"
    ],
    "transfer_performance_metrics": [
      "j"
    ],
    "implementation": [
      "Custom",
      "CS",
      "Pseudo",
      "Formulas",
      "Figures"
    ],
    "policy_type": [
      "SARSA",
      "Q",
      "FQI",
      "LSPI"
    ],
    "task_mappings": [
      "N/A"
    ],
    "autonomous_transfer": 0,
    "is_deep_rl": 1,
    "tags": [
      "2D",
      "Simulation",
      "CartPole",
      "InvertedPendulum",
      "MountainCar",
      "RBDist",
      "ClassicControl"
    ],
    "allowed_learner": [
      "TD",
      "batch"
    ],
    "country": [
      "USA",
      "Netherlands",
      "UK"
    ],
    "uni": [
      "University of Pennsylvania",
      "Washington State University",
      "Technical University of Eindhoven",
      "University of Maastricht",
      "University of Liverpool"
    ],
    "department": [],
    "source_task_selection": [
      "lib",
      "h"
    ],
    "was_in_survey": [
      "mag"
    ],
    "in_title": [
      "t",
      "r",
      "l"
    ],
    "in_abs": [
      "t",
      "r",
      "l",
      "k"
    ],
    "in_contet": [
      "t",
      "r",
      "l",
      "k"
    ],
    "rejected_or_unpublished": null,
    "duplication_status": null,
    "paper_for_thesis": 0,
    "paper_available": 1,
    "behind_paywall": 0,
    "paywall_circumventable": 0,
    "author_names": [
      "Kurt Driessens",
      "Decebal Constantin Mocanu",
      "Karl Tuyls",
      "Eric Eaton",
      "Gerhard Weiss",
      "Matthew D. Taylor",
      "Haitham Bou Ammar"
    ]
  },
  {
    "id": 2604618034,
    "title": "Knowledge Transfer for Deep Reinforcement Learning with Hierarchical Experience Replay.",
    "abst": null,
    "url": "https://dr.ntu.edu.sg/handle/10220/42453",
    "lang": "en",
    "authors": [
      2120836466,
      2605054084
    ],
    "fos": [
      41008148,
      47932503,
      2776960227,
      97541855,
      119857082,
      154945302
    ],
    "journals": [],
    "conferences": [
      2786197818
    ],
    "conference_series": [
      1184914352
    ],
    "references": [
      1515851193,
      1595483645,
      1602154927,
      1690739335,
      1821462560,
      2048226872,
      2134797427,
      2145339207,
      2155968351,
      2159420891,
      2165698076,
      2174786457,
      2201581102,
      2290104316,
      2294370754,
      2402040300,
      2584377191,
      2950708852,
      2963454359,
      2964161785
    ],
    "filter_matches": [
      "tr",
      "tl",
      "trl",
      "rl"
    ],
    "rank": 19870,
    "citation_count": 23,
    "estimated_citation_count": 23,
    "publication_date": "2017-01-01",
    "found_in": 1,
    "transfer_experiment_type": [
      "t",
      "s_i",
      "s_f",
      "levels",
      "s"
    ],
    "transfer_experiment_subtype": [
      "multi"
    ],
    "transfer_data_type": [
      "fea",
      "pi_dyn",
      "distil"
    ],
    "transfer_performance_metrics": [
      "ap"
    ],
    "implementation": [
      "Custom",
      "CS",
      "Formulas",
      "Figures",
      "Tables"
    ],
    "policy_type": [
      "DQN",
      "AMN",
      "DIST"
    ],
    "task_mappings": [
      "N/A"
    ],
    "autonomous_transfer": 0,
    "is_deep_rl": 1,
    "tags": [
      "2D",
      "Simulation",
      "Games",
      "VideoGames",
      "Atari"
    ],
    "allowed_learner": [
      "TD",
      "H"
    ],
    "country": [
      "Singapore"
    ],
    "uni": [
      "Nanyang Technological University"
    ],
    "department": [
      "Computer Science and Engineering"
    ],
    "source_task_selection": [
      "lib"
    ],
    "was_in_survey": [
      "mag",
      "zhu"
    ],
    "in_title": [
      "t",
      "r",
      "l",
      "k"
    ],
    "in_abs": [
      "t",
      "r",
      "l",
      "k"
    ],
    "in_contet": [
      "t",
      "r",
      "l",
      "k"
    ],
    "rejected_or_unpublished": null,
    "duplication_status": null,
    "paper_for_thesis": 0,
    "paper_available": 1,
    "behind_paywall": 0,
    "paywall_circumventable": 0,
    "author_names": [
      "Sinno Jialin Pan",
      "Haiyan Yin"
    ]
  },
  {
    "id": 2972430483,
    "title": "Transfer of Temporal Logic Formulas in Reinforcement Learning.",
    "abst": "Transferring high-level knowledge from a source task to a target task is an effective way to expedite reinforcement learning (RL). For example, propositional logic and first-order logic have been used as representations of such knowledge. We study the transfer of knowledge between tasks in which the timing of the events matters. We call such tasks temporal tasks. We concretize similarity between temporal tasks through a notion of logical transferability, and develop a transfer learning approach between different yet similar temporal tasks. We first propose an inference technique to extract metric interval temporal logic (MITL) formulas in sequential disjunctive normal form from labeled trajectories collected in RL of the two tasks. If logical transferability is identified through this inference, we construct a timed automaton for each sequential conjunctive subformula of the inferred MITL formulas from both tasks. We perform RL on the extended state which includes the locations and clock valuations of the timed automata for the source task. We then establish mappings between the corresponding components (clocks, locations, etc.) of the timed automata from the two tasks, and transfer the extended Q-functions based on the established mappings. Finally, we perform RL on the extended state for the target task, starting with the transferred extended Q-functions. Our results in two case studies show, depending on how similar the source task and the target task are, that the sampling efficiency for the target task can be improved by up to one order of magnitude by performing RL in the extended state space, and further improved by up to another order of magnitude using the transferred extended Q-functions.",
    "url": "https://arxiv.org/abs/1909.04256",
    "lang": "en",
    "authors": [
      2166645686,
      2315370604
    ],
    "fos": [
      154945302,
      30788636,
      41008148,
      72434380,
      97541855,
      119857082,
      150899416,
      80444323,
      2776214188,
      136726353,
      69562738,
      25016198
    ],
    "journals": [
      2596500785
    ],
    "conferences": [],
    "conference_series": [],
    "references": [
      1499017198,
      1561730144,
      1775249002,
      1965167605,
      1978100913,
      2023808162,
      2049696538,
      2077450865,
      2101508170,
      2105649996,
      2134845968,
      2145113795,
      2164114810,
      2312067050,
      2339807279,
      2474788619,
      2524638160,
      2531762107,
      2567705466,
      2574264966,
      2740302738,
      2741248519,
      2796085096,
      2807161818,
      2807270760,
      2808386811,
      2887933419,
      2889713213,
      2963575966,
      2963778636,
      2972698871
    ],
    "filter_matches": [
      "tr",
      "tl",
      "trl",
      "rl"
    ],
    "rank": 19883,
    "citation_count": 0,
    "estimated_citation_count": 0,
    "publication_date": "2019-09-10",
    "found_in": 1,
    "transfer_experiment_type": [
      "s_i",
      "s_f",
      "levels"
    ],
    "transfer_experiment_subtype": [
      "multi"
    ],
    "transfer_data_type": [
      "Q"
    ],
    "transfer_performance_metrics": [
      "tt",
      "tr",
      "ap"
    ],
    "implementation": [
      "Custom",
      "CS",
      "Pseudo",
      "Formulas",
      "Figures"
    ],
    "policy_type": [
      "Q"
    ],
    "task_mappings": [
      "N/A"
    ],
    "autonomous_transfer": 0,
    "is_deep_rl": 0,
    "tags": [
      "2D",
      "Navigation",
      "Grid",
      "Timed",
      "Simulation"
    ],
    "allowed_learner": [
      "TD"
    ],
    "country": [
      "USA"
    ],
    "uni": [
      "The University of Texas"
    ],
    "department": [
      "Computational Engineering and Sciences",
      "Aerospace Engineering and Engineering Mechanics"
    ],
    "source_task_selection": [
      "h"
    ],
    "was_in_survey": [
      "mag"
    ],
    "in_title": [
      "t",
      "r",
      "l"
    ],
    "in_abs": [
      "t",
      "r",
      "l",
      "k"
    ],
    "in_contet": [
      "t",
      "r",
      "l",
      "k"
    ],
    "rejected_or_unpublished": null,
    "duplication_status": null,
    "paper_for_thesis": 0,
    "paper_available": 1,
    "behind_paywall": 0,
    "paywall_circumventable": 0,
    "author_names": [
      "Ufuk Topcu",
      "Zhe Xu"
    ]
  },
  {
    "id": 1974043469,
    "title": "Learning domain structure through probabilistic policy reuse in reinforcement learning",
    "abst": "Policy Reuse is a transfer learning approach to improve a reinforcement learner with guidance from previously learned similar policies. The method uses the past policies as a probabilistic bias where the learner chooses among the exploitation of the ongoing learned policy, the exploration of random unexplored actions, and the exploitation of past policies. In this work, we demonstrate that Policy Reuse further contributes to the learning of the structure of a domain. Interestingly and almost as a side effect, Policy Reuse identifies classes of similar policies revealing a basis of core-policies of the domain. We demonstrate theoretically that, under a set of conditions to be satisfied, reusing such a set of core-policies allows us to bound the minimal expected gain received while learning a new policy. In general, Policy Reuse contributes to the overall goal of lifelong reinforcement learning, as (i) it incrementally builds a policy library; (ii) it provides a mechanism to reuse past policies; and (iii) it learns an abstract domain structure in terms of core-policies of the domain.",
    "url": "https://paperity.org/p/3969703/learning-domain-structure-through-probabilistic-policy-reuse-in-reinforcement-learning",
    "lang": "en",
    "authors": [
      2108671403,
      2128007263
    ],
    "fos": [
      154945302,
      139502532,
      97541855,
      124101348,
      56739046,
      67203356,
      119857082,
      47932503,
      49937458,
      3019959826,
      41008148,
      206588197,
      150899416
    ],
    "journals": [
      2480581173
    ],
    "conferences": [],
    "conference_series": [],
    "references": [
      26772505,
      36691172,
      69737162,
      114278598,
      1028811162,
      1255659923,
      1258105458,
      1486747874,
      1489563120,
      1494114146,
      1504212531,
      1512137381,
      1556824961,
      1557798492,
      1585546346,
      1598052524,
      1696410204,
      1853223271,
      1964150045,
      2012036715,
      2014512216,
      2056584142,
      2090170171,
      2091633639,
      2097113539,
      2097381042,
      2103626435,
      2104641222,
      2107726111,
      2109910161,
      2114451917,
      2114580749,
      2121517924,
      2121863487,
      2122451452,
      2122982548,
      2128118446,
      2128905965,
      2131746053,
      2133040789,
      2137375617,
      2153668164,
      2156493855,
      2159666783,
      2161571887,
      2164114810,
      2165792602,
      2519453022,
      3008729385,
      3011120880
    ],
    "filter_matches": [
      "rl"
    ],
    "rank": 19885,
    "citation_count": 20,
    "estimated_citation_count": 20,
    "publication_date": "2013-03-01",
    "found_in": 1,
    "transfer_experiment_type": [
      "s_i",
      "s_f"
    ],
    "transfer_experiment_subtype": [
      "multi"
    ],
    "transfer_data_type": [
      "pi"
    ],
    "transfer_performance_metrics": [
      "tr"
    ],
    "implementation": [
      "Custom",
      "CS",
      "Pseudo",
      "Formulas",
      "Figures"
    ],
    "policy_type": [
      "Q"
    ],
    "task_mappings": [
      "N/A"
    ],
    "autonomous_transfer": 0,
    "is_deep_rl": 0,
    "tags": [
      "2D",
      "Grid",
      "Navigation",
      "Simulation"
    ],
    "allowed_learner": [
      "TD"
    ],
    "country": [
      "USA"
    ],
    "uni": [
      "Carnegie Mellon University"
    ],
    "department": [],
    "source_task_selection": [
      "lib"
    ],
    "was_in_survey": [
      "taylor"
    ],
    "in_title": [
      "r",
      "l"
    ],
    "in_abs": [
      "l"
    ],
    "in_contet": [
      "t",
      "r",
      "l",
      "k"
    ],
    "rejected_or_unpublished": null,
    "duplication_status": null,
    "paper_for_thesis": 0,
    "paper_available": 1,
    "behind_paywall": 0,
    "paywall_circumventable": 0,
    "author_names": [
      "Manuela Veloso",
      "Fernando Fern&#x00E1;ndez"
    ]
  },
  {
    "id": 2807652507,
    "title": "Context-Aware Indoor VLC/RF Heterogeneous Network Selection: Reinforcement Learning With Knowledge Transfer",
    "abst": "For the converged use of LTE, WLAN, and visible light communication in indoor scenarios, fine-grained and intelligent network selection is essential for ensuring high user quality of experience. To tackle the challenges associated with dynamic environments and complicated service requirements, we propose a context-aware solution for indoor network selection. Specifically, three-level contextual information is revealed and exploited in both the utility and algorithm designs. In particular, the contextual information about the asymmetric downlink-uplink features of network performance is used to design a fine-grained utility model. A context-aware learning algorithm sensitive to traffic type-location-time information is proposed. The time-location dependent periodic changing rule of load statistical distributions is further used to realize efficient online network selection via knowledge transfer. The simulation results show that the proposed algorithm can achieve much better performance with faster convergence speed than traditional reinforcement learning.",
    "url": "https://dblp.uni-trier.de/db/journals/access/access6.html#DuWSW18",
    "lang": null,
    "authors": [
      2434761485,
      2789354429,
      2805562297,
      2806686749
    ],
    "fos": [
      158207573,
      2776960227,
      2779333187,
      41008148,
      71923881,
      157764524,
      51675839,
      203274722,
      120314980,
      97541855
    ],
    "journals": [
      2485537415
    ],
    "conferences": [],
    "conference_series": [],
    "references": [
      203338875,
      1533619695,
      1998027131,
      2009654461,
      2019657309,
      2037405583,
      2055548259,
      2079229848,
      2087045149,
      2099837357,
      2107726111,
      2122496159,
      2128145324,
      2128639451,
      2141539428,
      2162598825,
      2186351516,
      2272996903,
      2562947506,
      2566056037,
      2578555287,
      2963335382
    ],
    "filter_matches": [
      "tr",
      "tl",
      "trl",
      "rl"
    ],
    "rank": 19913,
    "citation_count": 11,
    "estimated_citation_count": 11,
    "publication_date": "2018-01-01",
    "found_in": 1,
    "transfer_experiment_type": [
      "t"
    ],
    "transfer_experiment_subtype": [
      "same_all"
    ],
    "transfer_data_type": [
      "Q"
    ],
    "transfer_performance_metrics": [
      "tt",
      "tr"
    ],
    "implementation": [
      "Custom",
      "CS",
      "Pseudo",
      "Formulas",
      "Figures"
    ],
    "policy_type": [
      "Q"
    ],
    "task_mappings": [
      "exp"
    ],
    "autonomous_transfer": 0,
    "is_deep_rl": 0,
    "tags": [
      "Indoor Network Selection",
      "Network",
      "Contextaware",
      "Simulation",
      "MonteCarlo"
    ],
    "allowed_learner": [
      "TD"
    ],
    "country": [
      "China"
    ],
    "uni": [
      "National University of Defense Technology",
      "National Digital Switching System Engineering and Technological Research Center"
    ],
    "department": [
      "Information and Communcation"
    ],
    "source_task_selection": [
      "lib"
    ],
    "was_in_survey": [
      "mag"
    ],
    "in_title": [
      "t",
      "r",
      "l",
      "k"
    ],
    "in_abs": [
      "t",
      "r",
      "l",
      "k"
    ],
    "in_contet": [
      "t",
      "r",
      "l",
      "k"
    ],
    "rejected_or_unpublished": null,
    "duplication_status": null,
    "paper_for_thesis": 0,
    "paper_available": 1,
    "behind_paywall": 0,
    "paywall_circumventable": 0,
    "author_names": [
      "Zhiyong Du",
      "Youming Sun",
      "Chunxi Wang",
      "Guofeng Wu"
    ]
  },
  {
    "id": 2259258048,
    "title": "Multiagent Reinforcement Learning With Sparse Interactions by Negotiation and Knowledge Transfer",
    "abst": "Reinforcement learning has significant applications for multiagent systems, especially in unknown dynamic environments. However, most multiagent reinforcement learning (MARL) algorithms suffer from such problems as exponential computation complexity in the joint state-action space, which makes it difficult to scale up to realistic multiagent problems. In this paper, a novel algorithm named negotiation-based MARL with sparse interactions (NegoSIs) is presented. In contrast to traditional sparse-interaction-based MARL algorithms, NegoSI adopts the equilibrium concept and makes it possible for agents to select the nonstrict equilibrium-dominating strategy profile (nonstrict EDSP) or meta equilibrium for their joint actions. The presented NegoSI algorithm consists of four parts: 1) the equilibrium-based framework for sparse interactions; 2) the negotiation for the equilibrium set; 3) the minimum variance method for selecting one joint action; and 4) the knowledge transfer of local ${Q}$ -values. In this integrated algorithm, three techniques, i.e., unshared value functions, equilibrium solutions, and sparse interactions are adopted to achieve privacy protection, better coordination and lower computational complexity, respectively. To evaluate the performance of the presented NegoSI algorithm, two groups of experiments are carried out regarding three criteria: 1) steps of each episode; 2) rewards of each episode; and 3) average runtime. The first group of experiments is conducted using six grid world games and shows fast convergence and high scalability of the presented algorithm. Then in the second group of experiments NegoSI is applied to an intelligent warehouse problem and simulated results demonstrate the effectiveness of the presented NegoSI algorithm compared with other state-of-the-art MARL algorithms.",
    "url": "https://www.ncbi.nlm.nih.gov/pubmed/27046917",
    "lang": "en",
    "authors": [
      2107182554,
      2437276002,
      2950737215,
      2992037913
    ],
    "fos": [
      41008148,
      179799912,
      154945302,
      159886148,
      74222875,
      119857082,
      126255220,
      97541855,
      48044578,
      187691185,
      71923881,
      41550386
    ],
    "journals": [
      76152103
    ],
    "conferences": [],
    "conference_series": [],
    "references": [
      11035765,
      25696899,
      67301991,
      103885025,
      168445157,
      1490582271,
      1519783625,
      1542941925,
      1868540347,
      1969785549,
      1982828967,
      1987725948,
      1991799203,
      2008809493,
      2012612381,
      2035003264,
      2035608030,
      2036103676,
      2054339854,
      2059859034,
      2067050450,
      2070963703,
      2071710911,
      2087823764,
      2094667998,
      2096018174,
      2096622112,
      2096690894,
      2099618002,
      2103076989,
      2104602264,
      2118318536,
      2120846115,
      2121863487,
      2124152208,
      2128814508,
      2129799459,
      2136644454,
      2137894103,
      2138965998,
      2147492008,
      2147750403,
      2153427071,
      2160041839,
      2164637474,
      2170396314,
      2397927371,
      3011120880
    ],
    "filter_matches": [
      "tr",
      "tl",
      "trl",
      "rl"
    ],
    "rank": 19955,
    "citation_count": 11,
    "estimated_citation_count": 11,
    "publication_date": "2017-05-01",
    "found_in": 1,
    "transfer_experiment_type": [
      "s",
      "levels"
    ],
    "transfer_experiment_subtype": [
      "multi"
    ],
    "transfer_data_type": [
      "Q"
    ],
    "transfer_performance_metrics": [
      "tt",
      "tr"
    ],
    "implementation": [
      "Custom",
      "CS",
      "Formulas",
      "Figures",
      "Pseudo",
      "Tables"
    ],
    "policy_type": [
      "Q",
      "CQ",
      "NegoQV",
      "ILVFT",
      "NegoSI"
    ],
    "task_mappings": [
      "N/A"
    ],
    "autonomous_transfer": 0,
    "is_deep_rl": 0,
    "tags": [
      "2D",
      "Simulation",
      "Navigation",
      "Grid",
      "Warehouse",
      "MultiAgent"
    ],
    "allowed_learner": [
      "TD"
    ],
    "country": [
      "China",
      "USA"
    ],
    "uni": [
      "Nanjing University",
      "University of Michigan"
    ],
    "department": [
      "Control and Systems Engineering",
      "State Key Laboratory for Novel Software Technology"
    ],
    "source_task_selection": [
      "h"
    ],
    "was_in_survey": [
      "silva",
      "mag"
    ],
    "in_title": [
      "t",
      "r",
      "l",
      "k"
    ],
    "in_abs": [
      "t",
      "r",
      "l",
      "k"
    ],
    "in_contet": [
      "t",
      "r",
      "l",
      "k"
    ],
    "rejected_or_unpublished": null,
    "duplication_status": null,
    "paper_for_thesis": 0,
    "paper_available": 1,
    "behind_paywall": 0,
    "paywall_circumventable": 0,
    "author_names": [
      "Chunlin Chen",
      "Luowei Zhou",
      "Pei Yang",
      "Yang Gao"
    ]
  },
  {
    "id": 1533597678,
    "title": "Transfer Learning in Reinforcement Learning Problems Through Partial Policy Recycling",
    "abst": "We investigate the relation between transfer learning in reinforcement learning with function approximation and supervised learning with concept drift. We present a new incremental relational regression tree algorithm that is capable of dealing with concept drift through tree restructuring and show that it enables a Q-learner to transfer knowledge from one task to another by recycling those parts of the generalized Q-function that still hold interesting information for the new task. We illustrate the performance of the algorithm in several experiments.",
    "url": "https://doi.org/10.1007/978-3-540-74958-5_70",
    "lang": "en",
    "authors": [
      1982051687,
      2016312802,
      2168801554
    ],
    "fos": [
      112972136,
      58973888,
      97541855,
      199190896,
      154945302,
      24138899,
      41008148,
      77967617,
      28006648,
      119857082,
      8038995
    ],
    "journals": [],
    "conferences": [],
    "conference_series": [
      2755314191
    ],
    "references": [
      30507863,
      107556075,
      1487385582,
      1506146479,
      1640646391,
      2014512216,
      2022775778,
      2031727428,
      2070183171,
      2099587183,
      2106953752,
      2133632100,
      2153353285,
      2154328025,
      2154441654,
      2396715201,
      2503474909,
      3020831056
    ],
    "filter_matches": [
      "tr",
      "tl",
      "trl",
      "rl"
    ],
    "rank": 19959,
    "citation_count": 49,
    "estimated_citation_count": 84,
    "publication_date": "2007-09-17",
    "found_in": 1,
    "transfer_experiment_type": [
      "#"
    ],
    "transfer_experiment_subtype": [
      "diff-no"
    ],
    "transfer_data_type": [
      "Q"
    ],
    "transfer_performance_metrics": [
      "ap",
      "j",
      "tt",
      "tr"
    ],
    "implementation": [
      "Custom",
      "CS",
      "Figures"
    ],
    "policy_type": [
      "TgR"
    ],
    "task_mappings": [
      "N/A"
    ],
    "autonomous_transfer": 0,
    "is_deep_rl": 0,
    "tags": [
      "Bongard",
      "Blocksworld",
      "TicTacToe",
      "2D",
      "Navigation",
      "BoardGames",
      "Simulation",
      "Games"
    ],
    "allowed_learner": [
      "RRL"
    ],
    "country": [
      "Netherlands"
    ],
    "uni": [
      "K.U.Leuven"
    ],
    "department": [
      "Computer Science"
    ],
    "source_task_selection": [],
    "was_in_survey": [
      "taylor",
      "bone",
      "mag"
    ],
    "in_title": [
      "t",
      "r",
      "l"
    ],
    "in_abs": [
      "t",
      "r",
      "l",
      "k"
    ],
    "in_contet": [
      "t",
      "r",
      "l",
      "k"
    ],
    "rejected_or_unpublished": null,
    "duplication_status": null,
    "paper_for_thesis": 0,
    "paper_available": 1,
    "behind_paywall": 0,
    "paywall_circumventable": 0,
    "author_names": [
      "Kurt Driessens",
      "Tom Croonenborghs",
      "Jan Ramon"
    ]
  },
  {
    "id": 2156493855,
    "title": "Relational macros for transfer in reinforcement learning",
    "abst": "We describe an application of inductive logic programming to transfer learning. Transfer learning is the use of knowledge learned in a source task to improve learning in a related target task. The tasks we work with are in reinforcement-learning domains. Our approach transfers relational macros, which are finite-state machines in which the transition conditions and the node actions are represented by first-order logical clauses. We use inductive logic programming to learn a macro that characterizes successful behavior in the source task, and then use the macro for decision-making in the early learning stages of the target task. Through experiments in the RoboCup simulated soccer domain, we show that Relational Macro Transfer via Demonstration (RMT-D) from a source task can provide a substantial head start in the target task.",
    "url": "https://experts.umn.edu/en/publications/relational-macros-for-transfer-in-reinforcement-learning",
    "lang": "en",
    "authors": [
      738944226,
      2047441381,
      2079278047,
      2120363087
    ],
    "fos": [
      119857082,
      97541855,
      154945302,
      28006648,
      150899416,
      41008148,
      2779382394,
      166955791,
      77075516,
      177877439,
      50033165
    ],
    "journals": [],
    "conferences": [],
    "conference_series": [
      1200006883
    ],
    "references": [
      36691172,
      115717799,
      143164768,
      1487385582,
      1490954610,
      1506146479,
      1510402218,
      1512137381,
      1515851193,
      1976115983,
      2056102643,
      2100677568,
      2111572265,
      2114537044,
      2121517924,
      2121863487,
      2122982548,
      2128905965,
      2133632477,
      2153353285,
      2155791599,
      2164114810,
      2521075165,
      3011120880
    ],
    "filter_matches": [
      "tr",
      "tl",
      "trl",
      "rl"
    ],
    "rank": 19991,
    "citation_count": 39,
    "estimated_citation_count": 69,
    "publication_date": "2007-06-19",
    "found_in": 1,
    "transfer_experiment_type": [
      "a",
      "r",
      "v"
    ],
    "transfer_experiment_subtype": [
      "diff-it"
    ],
    "transfer_data_type": [
      "relational-macros"
    ],
    "transfer_performance_metrics": [
      "j",
      "tr"
    ],
    "implementation": [
      "Custom",
      "CS",
      "Pseudo",
      "Figures",
      "Tables"
    ],
    "policy_type": [
      "Q",
      "RMT-D"
    ],
    "task_mappings": [
      "sup"
    ],
    "autonomous_transfer": 0,
    "is_deep_rl": 0,
    "tags": [
      "2D",
      "RoboCup",
      "BreakAway",
      "Simulation",
      "Games",
      "MultiAgent",
      "2v1",
      "3v2",
      "4v3"
    ],
    "allowed_learner": [
      "TD",
      "RRL"
    ],
    "country": [
      "USA"
    ],
    "uni": [
      "University of Minnesota",
      "University of Wisconsin"
    ],
    "department": [
      "Computer Science"
    ],
    "source_task_selection": [
      "h"
    ],
    "was_in_survey": [
      "taylor",
      "taylor-intertask",
      "bone",
      "mag"
    ],
    "in_title": [
      "t",
      "r",
      "l"
    ],
    "in_abs": [
      "t",
      "r",
      "l",
      "k"
    ],
    "in_contet": [
      "t",
      "r",
      "l",
      "k",
      "s"
    ],
    "rejected_or_unpublished": null,
    "duplication_status": null,
    "paper_for_thesis": 0,
    "paper_available": 1,
    "behind_paywall": 0,
    "paywall_circumventable": 0,
    "author_names": [
      "Lisa Torrey",
      "Richard Maclin",
      "Jude W. Shavlik",
      "Trevor Walker"
    ]
  },
  {
    "id": 2766253973,
    "title": "Heuristically Accelerated Reinforcement Learning by Means of Case-Based Reasoning and Transfer Learning",
    "abst": "Reinforcement Learning (RL) is a well-known technique for learning the solutions of control problems from the interactions of an agent in its domain. However, RL is known to be inefficient in problems of the real-world where the state space and the set of actions grow up fast. Recently, heuristics, case-based reasoning (CBR) and transfer learning have been used as tools to accelerate the RL process. This paper investigates a class of algorithms called Transfer Learning Heuristically Accelerated Reinforcement Learning (TLHARL) that uses CBR as heuristics within a transfer learning setting to accelerate RL. The main contributions of this work are the proposal of a new TLHARL algorithm based on the traditional RL algorithm Q(&#x03BB;) and the application of TLHARL on two distinct real-robot domains: a robot soccer with small-scale robots and the humanoid-robot stability learning. Experimental results show that our proposed method led to a significant improvement of the learning rate in both domains.",
    "url": "https://researchnow.flinders.edu.au/en/publications/heuristically-accelerated-reinforcement-learning-by-means-of-case",
    "lang": "en",
    "authors": [
      189649061,
      2124955968,
      2138742265,
      2138885720,
      2224319168
    ],
    "fos": [
      173801870,
      127413603,
      119857082,
      199190896,
      150899416,
      90509273,
      72434380,
      154945302,
      127705205,
      97541855,
      20162079
    ],
    "journals": [
      91329792
    ],
    "conferences": [],
    "conference_series": [],
    "references": [
      90468634,
      158722652,
      605348272,
      1424441257,
      1500349428,
      1502099479,
      1540685400,
      1559035773,
      1799762961,
      1912083604,
      1982636603,
      1985697265,
      1986014385,
      1986614398,
      1993277309,
      2013232999,
      2027919246,
      2031727428,
      2039804956,
      2041521369,
      2052293776,
      2062989416,
      2074466695,
      2091118421,
      2097381042,
      2098441518,
      2100677568,
      2100726628,
      2106953752,
      2110292307,
      2113913482,
      2121863487,
      2124175081,
      2140034347,
      2151340488,
      2160644528,
      2161879907,
      2165698076,
      2168939893,
      2174786457,
      2295582178,
      2395579298,
      2403040169,
      2503983428,
      2516542501,
      2524979071,
      2534140487,
      2547858766,
      2565773700,
      2585821313,
      2913340405,
      2949600457,
      3011120880
    ],
    "filter_matches": [
      "tr",
      "tl",
      "trl",
      "rl"
    ],
    "rank": 20102,
    "citation_count": 8,
    "estimated_citation_count": 8,
    "publication_date": "2018-08-01",
    "found_in": 1,
    "transfer_experiment_type": [
      "t"
    ],
    "transfer_experiment_subtype": [
      "sim2real"
    ],
    "transfer_data_type": [
      "pi",
      "Q",
      "cases"
    ],
    "transfer_performance_metrics": [
      "j",
      "tt"
    ],
    "implementation": [
      "Custom",
      "CS",
      "Formulas",
      "Figures",
      "Pseudo"
    ],
    "policy_type": [
      "Q",
      "HAQL",
      "TLHAQL"
    ],
    "task_mappings": [
      "N/A"
    ],
    "autonomous_transfer": 0,
    "is_deep_rl": 0,
    "tags": [
      "Robotics",
      "Simulation",
      "RealWorld",
      "RoboCup",
      "Kilobot",
      "Acrobot",
      "MultiAgent",
      "ClassicControl"
    ],
    "allowed_learner": [
      "CBR",
      "TD"
    ],
    "country": [
      "Brazil",
      "Spain"
    ],
    "uni": [
      "Technological Institute of Aeronautics",
      "Spanish National Research Council",
      "Centro Universitrio da FEI"
    ],
    "department": [
      "Artificial Intelligence Research Institute"
    ],
    "source_task_selection": [
      "h"
    ],
    "was_in_survey": [
      "mag"
    ],
    "in_title": [
      "t",
      "r",
      "l"
    ],
    "in_abs": [
      "t",
      "r",
      "l"
    ],
    "in_contet": [
      "t",
      "r",
      "l",
      "k",
      "s"
    ],
    "rejected_or_unpublished": null,
    "duplication_status": null,
    "paper_for_thesis": 0,
    "paper_available": 1,
    "behind_paywall": 0,
    "paywall_circumventable": 0,
    "author_names": [
      "Ramon L&#x00F3;pez de M&#x00E1;ntaras",
      "Reinaldo A. C. Bianchi",
      "Paulo E. Santos",
      "Luiz A. Celiberto",
      "Isaac J. Silva"
    ]
  },
  {
    "id": 24477102,
    "title": "Training and tracking in robotics",
    "abst": "We explore the use of learning schemes in training and adapting performance on simple coordination tasks. The tasks are 1-D pole balancing. Several programs incorporating learning have already achieved this (1, S, 8): the problem is to move a cart along a short piece of track to at to keep a pole balanced on its end; the pole is hinged to the cart at its bottom, and the cart is moved either to the left or to the right by a force of constant magnitude. The form of the task considered here, after (3), involves a genuinely difficult credit-assignment problem. We use a learning scheme previously developed and analysed (1, 7) to achieve performance through reinforcement, and extend it to include changing and new requirements. For example, the length or mast of the pole can change, the bias of the force, its strength, and so on; and the system can be tasked to avoid certain regions altogether. In this way we explore the learning system&#039;s ability to adapt to changes and to profit from a selected training sequence, both of which are of obvious utility in practical robotics applications.The results described here were obtained using a computer simulation of the pole-balancing problem. A movie will be shown of the performance of the system under the various requirements and tasks.",
    "url": "https://dblp.uni-trier.de/db/conf/ijcai/ijcai85.html#SelfridgeSB85",
    "lang": null,
    "authors": [
      1997216218,
      2120464764,
      2305505430
    ],
    "fos": [
      119857082,
      154945302,
      67203356,
      2777275308,
      2986914688,
      34413123,
      41008148
    ],
    "journals": [],
    "conferences": [
      2793897755
    ],
    "conference_series": [
      1203999783
    ],
    "references": [
      38913427,
      1488252886,
      1970185999,
      2091565802,
      2138178898,
      2895239407
    ],
    "filter_matches": [
      "trainingtrackingrobotics"
    ],
    "rank": 20103,
    "citation_count": 98,
    "estimated_citation_count": 149,
    "publication_date": "1985-08-18",
    "found_in": 1,
    "transfer_experiment_type": [
      "t"
    ],
    "transfer_experiment_subtype": [
      "same_all"
    ],
    "transfer_data_type": [
      "Q"
    ],
    "transfer_performance_metrics": [
      "tt"
    ],
    "implementation": [
      "Custom",
      "CS",
      "Formulas"
    ],
    "policy_type": [
      "Q"
    ],
    "task_mappings": [
      "N/A"
    ],
    "autonomous_transfer": 0,
    "is_deep_rl": 0,
    "tags": [
      "CartPole",
      "Simulation",
      "ClassicControl"
    ],
    "allowed_learner": [
      "TD"
    ],
    "country": [
      "UK"
    ],
    "uni": [
      "G T E Laba",
      "University of Massachusetts"
    ],
    "department": [
      "Computer Science"
    ],
    "source_task_selection": [
      "h"
    ],
    "was_in_survey": [
      "taylor"
    ],
    "in_title": [],
    "in_abs": [
      "r",
      "l"
    ],
    "in_contet": [
      "l"
    ],
    "rejected_or_unpublished": null,
    "duplication_status": null,
    "paper_for_thesis": 0,
    "paper_available": 1,
    "behind_paywall": 0,
    "paywall_circumventable": 0,
    "author_names": [
      "Andrew G. Barto",
      "Richard S. Sutton"
    ]
  },
  {
    "id": 1584313244,
    "title": "Vision-Based Behavior Acquisition For A Shooting Robot By Using A Reinforcement Learning",
    "abst": "We propose a method which acquires a purposive behavior for a mobile robot to shoot a ball into the goal by using a vision-based reinforcement learning. A mobile robot (an agent) does not need to know any parameters of the 3-D environment or its kinematics/dynamics. Information about the changes of the environment is only the image captured from a single TV camera mounted on the robot. An action-value function in terms of state is to be learned. Image positions of a ball and a goal are used as a state variable which shows the effect of an action previously taken. After the learning process, the robot tries to carry a ball near the goal and to shoot it. Both computer simulation and real robot experiments are shown, and discussion on the role of vision in the context of the vision-based reinforcement learning is given.",
    "url": "http://ci.nii.ac.jp/naid/110003299723",
    "lang": "ja",
    "authors": [
      229000411,
      1973789311,
      2079479084,
      2111067488
    ],
    "fos": [
      39920418,
      88044701,
      188116033,
      90509273,
      31972630,
      97541855,
      19966478,
      77967617,
      188888258,
      154945302,
      41008148
    ],
    "journals": [],
    "conferences": [],
    "conference_series": [],
    "references": [
      169977351,
      207822505,
      969698184,
      1557517019,
      1594201624,
      1966089223,
      2050797564,
      2059039791,
      2061361125,
      2097856935,
      2149276032,
      2341171179,
      3011120880
    ],
    "filter_matches": [
      "rl"
    ],
    "rank": 20112,
    "citation_count": 27,
    "estimated_citation_count": 27,
    "publication_date": "1994-01-21",
    "found_in": 1,
    "transfer_experiment_type": [
      "s_i"
    ],
    "transfer_experiment_subtype": [
      "same_all"
    ],
    "transfer_data_type": [
      "Q"
    ],
    "transfer_performance_metrics": [
      "tt"
    ],
    "implementation": [
      "Custom",
      "CS",
      "Figures",
      "Formulas",
      "Pseudo",
      "Tables"
    ],
    "policy_type": [
      "Q"
    ],
    "task_mappings": [
      "N/A"
    ],
    "autonomous_transfer": 0,
    "is_deep_rl": 0,
    "tags": [
      "Robotics",
      "Navigation",
      "RealWorld"
    ],
    "allowed_learner": [
      "TD"
    ],
    "country": [
      "Japan"
    ],
    "uni": [
      "Osaka University"
    ],
    "department": [
      "Mechanical Engineering"
    ],
    "source_task_selection": [
      "h"
    ],
    "was_in_survey": [
      "taylor"
    ],
    "in_title": [
      "r",
      "l"
    ],
    "in_abs": [
      "r",
      "l"
    ],
    "in_contet": [
      "r",
      "l"
    ],
    "rejected_or_unpublished": null,
    "duplication_status": null,
    "paper_for_thesis": 0,
    "paper_available": 1,
    "behind_paywall": 0,
    "paywall_circumventable": 0,
    "author_names": [
      "Sukoya Tawaratsumida",
      "Koh Hosoda",
      "Minoru Asada",
      "Shoichi Noda"
    ]
  },
  {
    "id": 2960705509,
    "title": "Assessing Transferability from Simulation to Reality for Reinforcement Learning",
    "abst": "Learning robot control policies from physics simulations is of great interest to the robotics community as it may render the learning process faster, cheaper, and safer by alleviating the need for expensive real-world experiments. However, the direct transfer of learned behavior from simulation to reality is a major challenge. Optimizing a policy on a slightly faulty simulator can easily lead to the maximization of the &#039;Simulation Optimization Bias&#039; (SOB). In this case, the optimizer exploits modeling errors of the simulator such that the resulting behavior can potentially damage the robot. We tackle this challenge by applying domain randomization, i.e., randomizing the parameters of the physics simulations during learning. We propose an algorithm called Simulation-based Policy Optimization with Transferability Assessment (SPOTA) which uses an estimator of the SOB to formulate a stopping criterion for training. The introduced estimator quantifies the over-fitting to the set of domains experienced while training. Our experimental results in two different environments show that the new simulation-based policy search algorithm is able to learn a control policy exclusively from a randomized simulator, which can be applied directly to real system without any additional training on the latter.",
    "url": "https://export.arxiv.org/pdf/1907.04685",
    "lang": "en",
    "authors": [
      2102101654,
      2899241740,
      2916835219
    ],
    "fos": [
      185429906,
      2776654903,
      119857082,
      2776330181,
      90509273,
      97541855,
      133731056,
      127413603,
      65401140,
      125583679,
      154945302,
      34413123
    ],
    "journals": [
      2596519289
    ],
    "conferences": [],
    "conference_series": [],
    "references": [
      1481659984,
      1507985183,
      1522301498,
      1832379062,
      1966784014,
      1969074599,
      1978161072,
      1990277704,
      1995878217,
      2000953623,
      2001685400,
      2076635261,
      2117897510,
      2145339207,
      2173248099,
      2205975260,
      2534269850,
      2595845486,
      2602963933,
      2605102758,
      2621667117,
      2623289472,
      2736601468,
      2766447205,
      2767050701,
      2773691349,
      2885163910,
      2898758998,
      2962749646,
      2962899390,
      2962957005,
      2963504951,
      2963614114,
      2963846183,
      2963864421,
      2964001908
    ],
    "filter_matches": [
      "tr",
      "tl",
      "trl",
      "rl"
    ],
    "rank": 20118,
    "citation_count": 1,
    "estimated_citation_count": 1,
    "publication_date": "2019-07-10",
    "found_in": 1,
    "transfer_experiment_type": [
      "s_i",
      "s_f",
      "t"
    ],
    "transfer_experiment_subtype": [
      "sim2real",
      "multi"
    ],
    "transfer_data_type": [
      "pi"
    ],
    "transfer_performance_metrics": [
      "ap"
    ],
    "implementation": [
      "Custom",
      "CS",
      "Formulas",
      "Figures",
      "Pseudo",
      "Tables",
      "Videos"
    ],
    "policy_type": [
      "SPOTA",
      "EPOpt",
      "PPO"
    ],
    "task_mappings": [
      "N/A"
    ],
    "autonomous_transfer": 0,
    "is_deep_rl": 1,
    "tags": [
      "3D",
      "Robotics",
      "BallBalancer",
      "CartPole",
      "SimulationOptimizationBias",
      "Simulation",
      "RealWorld",
      "ClassicControl"
    ],
    "allowed_learner": [
      "TD",
      "PS"
    ],
    "country": [
      "Germany"
    ],
    "uni": [
      "Technische Universitt Darmstadt",
      "Honda Research Institute Europe",
      "Max Planck Institute"
    ],
    "department": [
      "Intelligent Autonomous Systems Group",
      "Industry"
    ],
    "source_task_selection": [
      "h"
    ],
    "was_in_survey": [
      "mag"
    ],
    "in_title": [
      "t",
      "r",
      "l"
    ],
    "in_abs": [
      "t"
    ],
    "in_contet": [
      "t",
      "r",
      "l"
    ],
    "rejected_or_unpublished": null,
    "duplication_status": null,
    "paper_for_thesis": 0,
    "paper_available": 1,
    "behind_paywall": 0,
    "paywall_circumventable": 0,
    "author_names": [
      "Jan Peters",
      "Fabio Muratore",
      "Michael Gienger"
    ]
  },
  {
    "id": 1607318605,
    "title": "Proto-transfer Learning in Markov Decision Processes Using Spectral Methods",
    "abst": "In this paper we introduce proto-transfer leaning, a new framework for transfer learning. We explore solutions to transfer learning within reinforcement learning through the use of spectral methods. Proto-value functions (PVFs) are basis functions computed from a spectral analysis of random walks on the state space graph. They naturally lead to the ability to transfer knowledge and representation between related tasks or domains. We investigate task transfer by using the same PVFs in Markov decision processes (MDPs) with different rewards functions. Additionally, our experiments in domain transfer explore applying the Nystrom method for interpolation of PVFs between MDPs of different sizes. 1. Problem Statement The aim of transfer learning is to reuse behavior by using the knowledge learned about one domain or task to accelerate learning in a related domain or task. In this paper we explore solutions to transfer learning within reinforcement learning (Sutton &amp; Barto, 1998) through spectral methods. The new framework of proto-transfer learning transfers representations from one domain to another. This transfer entails the reuse of eigenvectors learned from one graph on another. We explore how to transfer knowledge learned on the source graph to a similar graph by modifying the eigenvectors of the Laplacian of the source domain to be reused for the target domain. Proto-value functions (PVFs) are a natural abstraction since they condense a domain by automatically learning an embedding of the Appearing in the ICML-06 Workshop on Structural Knowledge Transfer for Machine Learning, Pittsburgh, PA, 2006. Copyright 2006 by the author(s)/owner(s). state space based on its topology (Mahadevan, 2005). PVFs lead to the ability to transfer knowledge about domains and tasks, since they are constructed without taking reward into account. We define task transfer as the problem of transferring knowledge when the state space remains the same and only the reward differs. For task transfer, taskindependent basis functions, such as PVFs, can be reused from one task to the next without modification. Domain transfer refers to the more challenging problem of the state space changing. This change in state space can be a change in topology (i.e. obstacles moving to different locations) or a change in scale (i.e. a smaller or larger domain of the same shape). For domain transfer, the basis functions may need to be modified to reflect the changes in the state space. (Foster &amp; Dayan, 2002) study the task transfer problem by applying unsupervised, mixture model, learning methods to a collection of optimal value functions of different tasks in order to decompose and extract the underlying structure. In this paper, we investigate task transfer in discrete domains by reusing PVFs in MDPs with different reward functions. For domain transfer, we apply the Nystrom extension for interpolation of PVFs between MDPs of different sizes (Mahadevan et al., 2006). Previous work has accelerated learning when transferring behaviors between tasks and domains (Taylor et al., 2005), but we transfer representation and reuse knowledge to learn comparably on a new task or domain.",
    "url": "http://scholarworks.umass.edu/cgi/viewcontent.cgi?article=1149&amp;context=cs_faculty_pubs",
    "lang": "en",
    "authors": [
      2010118303,
      2133778237
    ],
    "fos": [
      2777965961,
      97541855,
      72434380,
      150899416,
      106189395,
      33923547,
      154945302,
      28006648,
      2776960227,
      8038995
    ],
    "journals": [],
    "conferences": [],
    "conference_series": [],
    "references": [
      194200989,
      658559791,
      1515851193,
      1598748993,
      2006373179,
      2032618685,
      2128905965,
      2130005627,
      2143958939,
      2271263738
    ],
    "filter_matches": [
      "tl"
    ],
    "rank": 20149,
    "citation_count": 40,
    "estimated_citation_count": 61,
    "publication_date": "2006-01-01",
    "found_in": 1,
    "transfer_experiment_type": [
      "r",
      "s_i",
      "s_f"
    ],
    "transfer_experiment_subtype": [
      "same_all"
    ],
    "transfer_data_type": [
      "pvf"
    ],
    "transfer_performance_metrics": [
      "tt"
    ],
    "implementation": [
      "Custom",
      "CS",
      "Pseudo",
      "Figures",
      "Formulas",
      "Tables"
    ],
    "policy_type": [
      "Q",
      "PVF"
    ],
    "task_mappings": [
      "N/A"
    ],
    "autonomous_transfer": 0,
    "is_deep_rl": 0,
    "tags": [
      "2D",
      "Grid",
      "Navigation",
      "Simulation"
    ],
    "allowed_learner": [
      "batch"
    ],
    "country": [
      "USA"
    ],
    "uni": [
      "University of Massachusetts"
    ],
    "department": [
      "Computer Science"
    ],
    "source_task_selection": [
      "h"
    ],
    "was_in_survey": [
      "taylor"
    ],
    "in_title": [],
    "in_abs": [
      "t",
      "r",
      "l"
    ],
    "in_contet": [
      "t",
      "r",
      "l"
    ],
    "rejected_or_unpublished": null,
    "duplication_status": null,
    "paper_for_thesis": 0,
    "paper_available": 1,
    "behind_paywall": 0,
    "paywall_circumventable": 0,
    "author_names": [
      "Kimberly Ferguson",
      "Sridhar Mahadevan"
    ]
  },
  {
    "id": 1612195517,
    "title": "Relativized options: choosing the right transformation",
    "abst": "Relativized options combine model minimization methods and a hierarchical reinforcement learning framework to derive compact reduced representations of a related family of tasks. Relativized options are defined without an absolute frame of reference, and an option&#039;s policy is transformed suitably based on the circumstances under which the option is invoked. In earlier work we addressed the issue of learning the option policy online. In this article we develop an algorithm for choosing, from among a set of candidate transformations, the right transformation for each member of the family of tasks.",
    "url": "https://works.bepress.com/andrew_barto/6/",
    "lang": "en",
    "authors": [
      1956086128,
      1997216218
    ],
    "fos": [
      119857082,
      97541855,
      56397880,
      41008148,
      74992021,
      154945302,
      147764199
    ],
    "journals": [],
    "conferences": [
      2785245801
    ],
    "conference_series": [
      1180662882
    ],
    "references": [
      79394677,
      151521611,
      1533853869,
      1534331386,
      1545378070,
      1598052524,
      1988217924,
      2001729196,
      2058735307,
      2059677035,
      2097815751,
      2107628283,
      2109910161,
      2121517924,
      2143435603,
      2158548602,
      2159599017,
      2168342951
    ],
    "filter_matches": [
      "opt"
    ],
    "rank": 20165,
    "citation_count": 37,
    "estimated_citation_count": 54,
    "publication_date": "2003-08-21",
    "found_in": 1,
    "transfer_experiment_type": [
      "t",
      "s_i",
      "s_f"
    ],
    "transfer_experiment_subtype": [
      "same_all"
    ],
    "transfer_data_type": [
      "options"
    ],
    "transfer_performance_metrics": [
      "tr"
    ],
    "implementation": [
      "Custom",
      "CS",
      "Formulas",
      "Figures",
      "Web",
      "Dead"
    ],
    "policy_type": [
      "Q",
      "SMDP-Q"
    ],
    "task_mappings": [
      "N/A"
    ],
    "autonomous_transfer": 0,
    "is_deep_rl": 0,
    "tags": [
      "2D",
      "Grid",
      "Navigation",
      "Collect",
      "Simulation"
    ],
    "allowed_learner": [
      "TD",
      "H"
    ],
    "country": [
      "USA"
    ],
    "uni": [
      "University of Massachusetts"
    ],
    "department": [
      "Computer Science"
    ],
    "source_task_selection": [
      "h"
    ],
    "was_in_survey": [
      "taylor"
    ],
    "in_title": [],
    "in_abs": [
      "l"
    ],
    "in_contet": [
      "t",
      "r",
      "l"
    ],
    "rejected_or_unpublished": null,
    "duplication_status": null,
    "paper_for_thesis": 0,
    "paper_available": 1,
    "behind_paywall": 0,
    "paywall_circumventable": 0,
    "author_names": [
      "Balaraman Ravindran",
      "Andrew G. Barto"
    ]
  },
  {
    "id": 2534140487,
    "title": "Using Transfer Learning to Speed-Up Reinforcement Learning: A Cased-Based Approach",
    "abst": "Reinforcement Learning (RL) is a well-known technique for the solution of problems where agents need to act with success in an unknown environment, learning through trial and error. However, this technique is not efficient enough to be used in applications with real world demands due to the time that the agent needs to learn. This paper investigates the use of Transfer Learning (TL) between agents to speed up the well-known Q-learning Reinforcement Learning algorithm. The new approach presented here allows the use of cases in a case base as heuristics to speed up the Q-learning algorithm, combining Case-Based Reasoning (CBR) and Heuristically Accelerated Reinforcement Learning (HARL) techniques. A set of empirical evaluations were conducted in the Mountain Car Problem Domain, where the actions learned during the solution of the 2D version of the problem can be used to speed up the learning of the policies for its 3D version. The experiments were made comparing the Q-learning Reinforcement Learning algorithm, the HAQL Heuristic Accelerated Reinforcement Learning (HARL) algorithm and the TL-HAQL algorithm, proposed here. The results show that the use of a case-base for transfer learning can lead to a significant improvement in the performance of the agent, making it learn faster than using either RL or HARL methods alone.",
    "url": "https://www.iiia.csic.es/es/publications/using-transfer-learning-speed-reinforcement-learning-case-based-approach",
    "lang": "en",
    "authors": [
      189649061,
      2124955968,
      2138885720,
      2512040638
    ],
    "fos": [
      28006648,
      154945302,
      199190896,
      112972136,
      58973888,
      77967617,
      119857082,
      41008148,
      97541855,
      24138899,
      8038995
    ],
    "journals": [],
    "conferences": [],
    "conference_series": [
      2760758913
    ],
    "references": [
      10379689,
      28595787,
      90468634,
      111328409,
      1490954610,
      1537916387,
      1592209052,
      1799762961,
      1817398672,
      1912083604,
      1993277309,
      2044217135,
      2076766268,
      2095182442,
      2097381042,
      2098723043,
      2100726628,
      2110292307,
      2121863487,
      2126385963,
      2133013156,
      2133040789,
      2151340488,
      2158150115,
      2160644528,
      2163808368,
      2166798247,
      2168939893,
      2913340405,
      3011120880
    ],
    "filter_matches": [
      "tr",
      "tl",
      "trl",
      "rl"
    ],
    "rank": 20190,
    "citation_count": 16,
    "estimated_citation_count": 16,
    "publication_date": "2010-10-23",
    "found_in": 1,
    "transfer_experiment_type": [
      "v",
      "t"
    ],
    "transfer_experiment_subtype": [
      "diff-it"
    ],
    "transfer_data_type": [
      "cases",
      "pi",
      "Q"
    ],
    "transfer_performance_metrics": [
      "j",
      "tt"
    ],
    "implementation": [
      "Custom",
      "CS",
      "Formulas",
      "Figures",
      "Pseudo",
      "Tables"
    ],
    "policy_type": [
      "Q",
      "HAQL",
      "TL-HAQL"
    ],
    "task_mappings": [
      "sup"
    ],
    "autonomous_transfer": 0,
    "is_deep_rl": 1,
    "tags": [
      "2D",
      "3D",
      "2Dto3D",
      "MountainCar",
      "Simulation",
      "ClassicControl"
    ],
    "allowed_learner": [
      "CBR",
      "TD"
    ],
    "country": [
      "Brazil",
      "Spain"
    ],
    "uni": [
      "Technological Institute of Aeronautics",
      "Spanish National Research Council",
      "Centro Universitrio da FEI"
    ],
    "department": [
      "Artificial Intelligence Research Institute"
    ],
    "source_task_selection": [
      "h"
    ],
    "was_in_survey": [
      "mag"
    ],
    "in_title": [
      "t",
      "r",
      "l"
    ],
    "in_abs": [
      "t",
      "r",
      "l"
    ],
    "in_contet": [
      "t",
      "r",
      "l",
      "k"
    ],
    "rejected_or_unpublished": null,
    "duplication_status": null,
    "paper_for_thesis": 0,
    "paper_available": 1,
    "behind_paywall": 0,
    "paywall_circumventable": 0,
    "author_names": [
      "Ramon L&#x00F3;pez de M&#x00E1;ntaras",
      "Reinaldo A. C. Bianchi",
      "Luiz A. Celiberto",
      "Jackson P. Matsuura"
    ]
  },
  {
    "id": 2036103676,
    "title": "Accelerating Multiagent Reinforcement Learning by Equilibrium Transfer",
    "abst": "An important approach in multiagent reinforcement learning (MARL) is equilibrium-based MARL, which adopts equilibrium solution concepts in game theory and requires agents to play equilibrium strategies at each state. However, most existing equilibrium-based MARL algorithms cannot scale due to a large number of computationally expensive equilibrium computations (e.g., computing Nash equilibria is PPAD-hard) during learning. For the first time, this paper finds that during the learning process of equilibrium-based MARL, the one-shot games corresponding to each state&#x2019;s successive visits often have the same or similar equilibria (for some states more than 90% of games corresponding to successive visits have similar equilibria). Inspired by this observation, this paper proposes to use equilibrium transfer to accelerate equilibrium-based MARL. The key idea of equilibrium transfer is to reuse previously computed equilibria when each agent has a small incentive to deviate. By introducing transfer loss and transfer condition, a novel framework called equilibrium transfer-based MARL is proposed. We prove that although equilibrium transfer brings transfer loss, equilibrium-based MARL algorithms can still converge to an equilibrium policy under certain assumptions. Experimental results in widely used benchmarks (e.g., grid world game, soccer game, and wall game) show that the proposed framework: 1) not only significantly accelerates equilibrium-based MARL (up to 96.7% reduction in learning time), but also achieves higher average rewards than algorithms without equilibrium transfer and 2) scales significantly better than algorithms without equilibrium transfer when the state/action space grows and the number of agents increases.",
    "url": "http://or.nsfc.gov.cn/handle/00001903-5/480512",
    "lang": "zh_chs",
    "authors": [
      2103293838,
      2169236860,
      2992037913
    ],
    "fos": [
      164407509,
      33923547,
      202556891,
      154945302,
      46814582,
      107012843,
      201364048,
      126255220,
      144237770,
      157034889,
      175983524,
      119857082,
      11343654
    ],
    "journals": [
      76152103
    ],
    "conferences": [],
    "conference_series": [],
    "references": [
      6043852,
      11035765,
      29099680,
      1502765764,
      1519783625,
      1542941925,
      1868540347,
      1968902782,
      1974740629,
      1978375026,
      1980737627,
      1991799203,
      2002373723,
      2008393562,
      2010526786,
      2029250042,
      2072256588,
      2089415692,
      2092710777,
      2097381042,
      2097780422,
      2099618002,
      2109100253,
      2112632840,
      2120846115,
      2121863487,
      2124152208,
      2130463867,
      2135017242,
      2136934807,
      2138965998,
      2145067550,
      2153427071,
      2164637474,
      2263562440,
      2404646363,
      2406194061
    ],
    "filter_matches": [
      "tr",
      "tl",
      "trl",
      "rl"
    ],
    "rank": 20220,
    "citation_count": 19,
    "estimated_citation_count": 19,
    "publication_date": "2015-07-01",
    "found_in": 1,
    "transfer_experiment_type": [
      "r",
      "s",
      "#",
      "v"
    ],
    "transfer_experiment_subtype": [
      "multi",
      "multiagent"
    ],
    "transfer_data_type": [
      "equilibrium"
    ],
    "transfer_performance_metrics": [
      "j",
      "tr",
      "tt"
    ],
    "implementation": [
      "Custom",
      "CS",
      "Pseudo",
      "Formulas",
      "Theorem",
      "Lemma",
      "Figures",
      "Tables"
    ],
    "policy_type": [
      "NashQ",
      "CEQ"
    ],
    "task_mappings": [],
    "autonomous_transfer": 0,
    "is_deep_rl": 0,
    "tags": [
      "2D",
      "Grid",
      "Navigation",
      "Simulation",
      "SoccerGame",
      "WallGame",
      "MultiGrid",
      "Equilibrium",
      "MultiAgent"
    ],
    "allowed_learner": [
      "TD"
    ],
    "country": [
      "China",
      "Singapore"
    ],
    "uni": [
      "Nanjing University",
      "Nanyang Technological University"
    ],
    "department": [
      "Computer Science",
      "Computer Engineering"
    ],
    "source_task_selection": [],
    "was_in_survey": [
      "mag",
      "silva"
    ],
    "in_title": [
      "t",
      "r",
      "l"
    ],
    "in_abs": [
      "t",
      "r",
      "l"
    ],
    "in_contet": [
      "t",
      "r",
      "l",
      "k"
    ],
    "rejected_or_unpublished": null,
    "duplication_status": null,
    "paper_for_thesis": 0,
    "paper_available": 1,
    "behind_paywall": 0,
    "paywall_circumventable": 0,
    "author_names": [
      "Yujing Hu",
      "Bo An",
      "Yang Gao"
    ]
  },
  {
    "id": 2907543530,
    "title": "Deep Reinforcement Learning with Knowledge Transfer for Online Rides Order Dispatching",
    "abst": "Ride dispatching is a central operation task on a ride-sharing platform to continuously match drivers to trip-requesting passengers. In this work, we model the ride dispatching problem as a Markov Decision Process and propose learning solutions based on deep Q-networks with action search to optimize the dispatching policy for drivers on ride-sharing platforms. We train and evaluate dispatching agents for this challenging decision task using real-world spatio-temporal trip data from the DiDi ride-sharing platform. A large-scale dispatching system typically supports many geographical locations with diverse demand-supply settings. To increase learning adaptability and efficiency, we propose a new transfer learning method Correlated Feature Progressive Transfer, along with two existing methods, enabling knowledge transfer in both spatial and temporal spaces. Through an extensive set of experiments, we demonstrate the learning and optimization capabilities of our deep reinforcement learning algorithms. We further show that dispatching policies learned by transferring knowledge from a source city to target cities or across temporal space within the same city significantly outperform those without transfer learning.",
    "url": "https://dblp.uni-trier.de/db/conf/icdm/icdm2018.html#WangQTYZ18",
    "lang": null,
    "authors": [
      2104242642,
      2803506069,
      2907012863,
      2907282905,
      2948300206
    ],
    "fos": [
      154945302,
      106189395,
      177606310,
      119857082,
      41008148,
      150899416,
      97541855,
      2776960227
    ],
    "journals": [],
    "conferences": [],
    "conference_series": [
      1183478919
    ],
    "references": [
      101796584,
      156001215,
      1585855589,
      1988580225,
      1991874986,
      2053913299,
      2061069829,
      2065195978,
      2097381042,
      2100495367,
      2121863487,
      2145339207,
      2155968351,
      2165698076,
      2174786457,
      2237537322,
      2426267443,
      2560647685,
      2739083961,
      2740302738,
      2742211145,
      2757649114,
      2808810245,
      2963049866,
      2963199420
    ],
    "filter_matches": [
      "tr",
      "tl",
      "trl",
      "rl"
    ],
    "rank": 20225,
    "citation_count": 13,
    "estimated_citation_count": 13,
    "publication_date": "2018-11-01",
    "found_in": 1,
    "transfer_experiment_type": [
      "t",
      "s_i",
      "s_f",
      "levels"
    ],
    "transfer_experiment_subtype": [
      "multi"
    ],
    "transfer_data_type": [
      "fea"
    ],
    "transfer_performance_metrics": [
      "j",
      "tr",
      "tt",
      "ap"
    ],
    "implementation": [
      "Custom",
      "CS",
      "Formulas",
      "Figures",
      "Pseudo"
    ],
    "policy_type": [
      "DQN"
    ],
    "task_mappings": [
      "N/A"
    ],
    "autonomous_transfer": 0,
    "is_deep_rl": 1,
    "tags": [
      "Simulation",
      "RideSharing",
      "Dispatch",
      "MultiAgent"
    ],
    "allowed_learner": [
      "TD"
    ],
    "country": [
      "USA",
      "China"
    ],
    "uni": [
      "Washington State University",
      "DiDi Research America",
      "DiDi Chuxing"
    ],
    "department": [
      "Industry",
      "Uni",
      "Colab"
    ],
    "source_task_selection": [
      "h"
    ],
    "was_in_survey": [
      "mag"
    ],
    "in_title": [
      "t",
      "r",
      "l",
      "k"
    ],
    "in_abs": [
      "t",
      "r",
      "l",
      "k"
    ],
    "in_contet": [
      "t",
      "r",
      "l",
      "k"
    ],
    "rejected_or_unpublished": null,
    "duplication_status": null,
    "paper_for_thesis": 0,
    "paper_available": 1,
    "behind_paywall": 1,
    "paywall_circumventable": 1,
    "author_names": [
      "Zhiwei Qin",
      "Jieping Ye",
      "Hongtu Zhu",
      "Zhaodong Wang",
      "Xiaocheng Tang"
    ]
  },
  {
    "id": 2014512216,
    "title": "Transfer of Experience Between Reinforcement Learning Environments with Progressive Difficulty",
    "abst": "This paper describes an extension to reinforcement learning (RL), in which a standard RL algorithm is augmented with a mechanism for transferring experience gained in one problem to new but related problems. In this approach, named Progressive RL, an agent acquires experience of operating in a simple environment through experimentation, and then engages in a period of introspection, during which it rationalises the experience gained and formulates symbolic knowledge describing how to behave in that simple environment. When subsequently experimenting in a more complex but related environment, it is guided by this knowledge until it gains direct experience. A test domain with 15 maze environments, arranged in order of difficulty, is described. A range of experiments in this domain are presented, that demonstrate the benefit of Progressive RL relative to a basic RL approach in which each puzzle is solved from scratch. The experiments also analyse the knowledge formed during introspection, illustrate how domain knowledge may be incorporated, and show that Progressive Reinforcement Learning may be used to solve complex puzzles more quickly.",
    "url": "https://www.researchgate.net/profile/Michael_Madden3/publication/220637641_Transfer_of_Experience_Between_Reinforcement_Learning_Environments_with_Progressive_Difficulty/links/00b7d5249d3c37fa43000000.pdf?disableCoverPage=true",
    "lang": "en",
    "authors": [
      2008564336,
      2154169445
    ],
    "fos": [
      52001869,
      41008148,
      188116033,
      154945302,
      2781235140,
      47932503,
      207685749,
      2779178843,
      129671850,
      119857082,
      97541855
    ],
    "journals": [
      122814990
    ],
    "conferences": [],
    "conference_series": [],
    "references": [
      92265916,
      130452346,
      143164768,
      1258105458,
      1510402218,
      1513681384,
      1515851193,
      1572710235,
      1625504505,
      1777239053,
      1853223271,
      1999138184,
      2033072307,
      2041064082,
      2089561656,
      2103626435,
      2107726111,
      2113102527,
      2114451917,
      2121517924,
      2121863487,
      2122410182,
      2122451452,
      2123504579,
      2125055259,
      2155027007,
      2160395727,
      3011120880,
      3020831056,
      3022194887
    ],
    "filter_matches": [
      "tr",
      "tl",
      "trl",
      "rl"
    ],
    "rank": 20242,
    "citation_count": 40,
    "estimated_citation_count": 81,
    "publication_date": "2004-06-01",
    "found_in": 1,
    "transfer_experiment_type": [
      "s_i",
      "s_f",
      "t",
      "levels"
    ],
    "transfer_experiment_subtype": [
      "multi"
    ],
    "transfer_data_type": [
      "rule"
    ],
    "transfer_performance_metrics": [
      "tr",
      "tt"
    ],
    "implementation": [
      "Custom",
      "CS",
      "Figures",
      "Formulas",
      "Tables"
    ],
    "policy_type": [
      "Q",
      "P-RL"
    ],
    "task_mappings": [
      "N/A"
    ],
    "autonomous_transfer": 0,
    "is_deep_rl": 0,
    "tags": [
      "2D",
      "Grid",
      "Navigation",
      "Simulation"
    ],
    "allowed_learner": [
      "TD"
    ],
    "country": [
      "Ireland"
    ],
    "uni": [
      "National University of Ireland"
    ],
    "department": [
      "Information Technology"
    ],
    "source_task_selection": [
      "all"
    ],
    "was_in_survey": [
      "taylor",
      "silva",
      "bone",
      "lazaric",
      "mag"
    ],
    "in_title": [
      "t",
      "r",
      "l"
    ],
    "in_abs": [
      "t",
      "r",
      "l"
    ],
    "in_contet": [
      "t",
      "r",
      "l"
    ],
    "rejected_or_unpublished": null,
    "duplication_status": null,
    "paper_for_thesis": 0,
    "paper_available": 1,
    "behind_paywall": 0,
    "paywall_circumventable": 0,
    "author_names": [
      "Tom Howley",
      "Michael G. Madden"
    ]
  },
  {
    "id": 2742143911,
    "title": "Advantages and Limitations of using Successor Features for Transfer in Reinforcement Learning",
    "abst": "One question central to Reinforcement Learning is how to learn a feature representation that supports algorithm scaling and re-use of learned information from different tasks. Successor Features approach this problem by learning a feature representation that satisfies a temporal constraint. We present an implementation of an approach that decouples the feature representation from the reward function, making it suitable for transferring knowledge between domains. We then assess the advantages and limitations of using Successor Features for transfer.",
    "url": "http://export.arxiv.org/pdf/1708.00102",
    "lang": "en",
    "authors": [
      1075592465,
      2274436407,
      2739590467
    ],
    "fos": [
      119857082,
      75306776,
      41008148,
      154945302,
      97541855
    ],
    "journals": [
      2596500785
    ],
    "conferences": [],
    "conference_series": [],
    "references": [
      2056354534,
      2097381042,
      2104753538,
      2106261932,
      2107726111,
      2145339207,
      2440926996,
      2567020712
    ],
    "filter_matches": [
      "tr",
      "tl",
      "trl",
      "rl"
    ],
    "rank": 20244,
    "citation_count": 17,
    "estimated_citation_count": 17,
    "publication_date": "2017-07-31",
    "found_in": 1,
    "transfer_experiment_type": [
      "s_i",
      "s_f",
      "r"
    ],
    "transfer_experiment_subtype": [
      "multi"
    ],
    "transfer_data_type": [
      "fea",
      "successor"
    ],
    "transfer_performance_metrics": [
      "j",
      "tt",
      "ap"
    ],
    "implementation": [
      "Custom",
      "CS",
      "Pseudo",
      "Figures",
      "Formulas",
      "Tables",
      "TensorFlow"
    ],
    "policy_type": [
      "FQI",
      "FSF"
    ],
    "task_mappings": [
      "N/A"
    ],
    "autonomous_transfer": 0,
    "is_deep_rl": 1,
    "tags": [
      "Simulation",
      "2D",
      "Navigation",
      "Grid",
      "Successor"
    ],
    "allowed_learner": [
      "TD",
      "batch"
    ],
    "country": [
      "USA"
    ],
    "uni": [
      "Brown University"
    ],
    "department": [],
    "source_task_selection": [
      "all"
    ],
    "was_in_survey": [
      "zhu",
      "mag"
    ],
    "in_title": [
      "t",
      "r",
      "l"
    ],
    "in_abs": [
      "t",
      "r",
      "l",
      "k"
    ],
    "in_contet": [
      "t",
      "r",
      "l",
      "k"
    ],
    "rejected_or_unpublished": 2,
    "duplication_status": null,
    "paper_for_thesis": 0,
    "paper_available": 1,
    "behind_paywall": 0,
    "paywall_circumventable": 0,
    "author_names": [
      "Michael L. Littman",
      "Stefanie Tellex",
      "Lucas Lehnert"
    ]
  },
  {
    "id": 2963049105,
    "title": "Online Transfer Learning in Reinforcement Learning Domains.",
    "abst": "This paper proposes an online transfer framework to capture the interaction among agents and shows that current transfer learning in reinforcement learning is a special case of online transfer. Furthermore, this paper re-characterizes existing agents-teaching-agents methods as online transfer and analyze one such teaching method in three ways. First, the convergence of Qlearning and Sarsa with tabular representation with a finite budget is proven. Second, the convergence of Qlearning and Sarsa with linear function approximation is established. Third, the we show the asymptotic performance cannot be hurt through teaching. Additionally, all theoretical results are empirically validated.",
    "url": "http://irll.eecs.wsu.edu/wp-content/papercite-data/pdf/2015sdmia-zhan.pdf",
    "lang": null,
    "authors": [
      2112607260,
      2148762994
    ],
    "fos": [
      150899416,
      88610354,
      106514582,
      2993364581,
      41008148,
      71923881,
      154945302,
      63251961,
      119857082,
      97541855
    ],
    "journals": [],
    "conferences": [
      2787475599
    ],
    "conference_series": [
      1184914352
    ],
    "references": [
      1944672,
      43161092,
      158722652,
      950880443,
      1515851193,
      1529399279,
      1583136812,
      1594563152,
      1925875298,
      1986014385,
      2004030284,
      2020764470,
      2097381042,
      2130005627,
      2132625905,
      2142690706,
      2147750403,
      2150339816,
      2151661095,
      2158091072,
      2165131254,
      2171578145,
      2585643748,
      2903158431,
      3011120880
    ],
    "filter_matches": [
      "tr",
      "tl",
      "trl",
      "rl"
    ],
    "rank": 20264,
    "citation_count": 1,
    "estimated_citation_count": 1,
    "publication_date": "2015-01-01",
    "found_in": 1,
    "transfer_experiment_type": [
      "s"
    ],
    "transfer_experiment_subtype": [
      "same_all"
    ],
    "transfer_data_type": [
      "advisor"
    ],
    "transfer_performance_metrics": [
      "tr"
    ],
    "implementation": [
      "CS",
      "Custom",
      "Theorem",
      "Figures",
      "Tables",
      "Formulas",
      "Lemma"
    ],
    "policy_type": [
      "Q",
      "SARSA"
    ],
    "task_mappings": [
      "N/A"
    ],
    "autonomous_transfer": 0,
    "is_deep_rl": 0,
    "tags": [
      "Simulation",
      "Games",
      "Pacman",
      "Atari"
    ],
    "allowed_learner": [
      "TD"
    ],
    "country": [
      "USA"
    ],
    "uni": [
      "Washington State University"
    ],
    "department": [
      "Electrical Engineering and Computer Science"
    ],
    "source_task_selection": [
      "h"
    ],
    "was_in_survey": [
      "zhu",
      "mag"
    ],
    "in_title": [
      "t",
      "r",
      "l"
    ],
    "in_abs": [
      "t",
      "l"
    ],
    "in_contet": [
      "t",
      "r",
      "l",
      "k"
    ],
    "rejected_or_unpublished": null,
    "duplication_status": null,
    "paper_for_thesis": 0,
    "paper_available": 1,
    "behind_paywall": 0,
    "paywall_circumventable": 0,
    "author_names": [
      "Yusen Zhan",
      "Matthew D. Taylor"
    ]
  },
  {
    "id": 2985293964,
    "title": "Transfer in Deep Reinforcement Learning Using Knowledge Graphs.",
    "abst": null,
    "url": "https://dblp.uni-trier.de/db/journals/corr/corr1908.html#abs-1908-06556",
    "lang": null,
    "authors": [
      2124757846,
      2624397557
    ],
    "fos": [
      2987255567,
      154945302,
      97541855,
      41008148
    ],
    "journals": [],
    "conferences": [],
    "conference_series": [
      2758004393
    ],
    "references": [],
    "filter_matches": [
      "tr",
      "tl",
      "trl",
      "rl"
    ],
    "rank": 20283,
    "citation_count": 1,
    "estimated_citation_count": 1,
    "publication_date": "2019-01-01",
    "found_in": 1,
    "transfer_experiment_type": [
      "s",
      "levels"
    ],
    "transfer_experiment_subtype": [
      "multi",
      "lit"
    ],
    "transfer_data_type": [
      "graph",
      "pi"
    ],
    "transfer_performance_metrics": [
      "tt"
    ],
    "implementation": [
      "CS",
      "Custom",
      "KG-DQN",
      "PyTorch",
      "Jericho",
      "TextWorld",
      "Formulas",
      "Figures",
      "Tables"
    ],
    "policy_type": [
      "DQN",
      "KG-DQN"
    ],
    "task_mappings": [
      "N/A"
    ],
    "autonomous_transfer": 0,
    "is_deep_rl": 1,
    "tags": [
      "Simulation",
      "TextBased",
      "Game"
    ],
    "allowed_learner": [
      "TD"
    ],
    "country": [
      "USA"
    ],
    "uni": [
      "Georgia Institute of Technology"
    ],
    "department": [
      "Interactive Computing"
    ],
    "source_task_selection": [
      "h"
    ],
    "was_in_survey": [
      "mag"
    ],
    "in_title": [
      "t",
      "r",
      "l",
      "k"
    ],
    "in_abs": [
      "t",
      "r",
      "l",
      "k"
    ],
    "in_contet": [
      "t",
      "r",
      "l",
      "k",
      "s"
    ],
    "rejected_or_unpublished": null,
    "duplication_status": null,
    "paper_for_thesis": 0,
    "paper_available": 1,
    "behind_paywall": 0,
    "paywall_circumventable": 0,
    "author_names": [
      "Mark O. Riedl",
      "Prithviraj Ammanabrolu"
    ]
  },
  {
    "id": 2949472713,
    "title": "Transfer and Online Reinforcement Learning in STT-MRAM Based Embedded Systems for Autonomous Drones",
    "abst": "In this paper we present an algorithm-hardware codesign for camera-based autonomous flight in small drones. We show that the large write-latency and write-energy for nonvolatile memory (NVM) based embedded systems makes them unsuitable for real-time reinforcement learning (RL). We address this by performing transfer learning (TL) on metaenvironments and RL on the last few layers of a deep convolutional network. While the NVM stores the meta-model from TL, an on-die SRAM stores the weights of the last few layers. Thus all the real-time updates via RL are carried out on the SRAM arrays. This provides us with a practical platform with comparable performance as end-to-end RL and 83.4% lower energy per image frame",
    "url": "https://arxiv.org/abs/1905.06314",
    "lang": "en",
    "authors": [
      2515360915,
      2698663285,
      2760868830,
      2950376593
    ],
    "fos": [
      59519942,
      149635348,
      150899416,
      46891859,
      177950962,
      2983261157,
      97541855,
      68043766,
      3018229523,
      41008148
    ],
    "journals": [
      2597013281
    ],
    "conferences": [],
    "conference_series": [],
    "references": [
      2075278406,
      2092178912,
      2156728623,
      2165698076,
      2289252105,
      2442974303,
      2487018225,
      2565902248,
      2618530766,
      2787295179,
      2793776854,
      2884585555
    ],
    "filter_matches": [
      "tr",
      "tl",
      "trl",
      "rl"
    ],
    "rank": 20335,
    "citation_count": 0,
    "estimated_citation_count": 0,
    "publication_date": "2019-04-22",
    "found_in": 1,
    "transfer_experiment_type": [
      "s_i",
      "s_f",
      "levels"
    ],
    "transfer_experiment_subtype": [
      "multi"
    ],
    "transfer_data_type": [
      "fea",
      "pi"
    ],
    "transfer_performance_metrics": [
      "ap"
    ],
    "implementation": [
      "Custom",
      "CS",
      "Pseudo",
      "Figures",
      "Formulas",
      "TensorFlow",
      "AirSim",
      "Videos",
      "LinkNotIncluded"
    ],
    "policy_type": [
      "DDQN",
      "DNN"
    ],
    "task_mappings": [
      "N/A"
    ],
    "autonomous_transfer": 0,
    "is_deep_rl": 1,
    "tags": [
      "RealTime",
      "Simulation",
      "Drone",
      "ObjectAvoidance",
      "CNN",
      "STT-MRAM",
      "Robotics",
      "Vehicle",
      "Aerial"
    ],
    "allowed_learner": [
      "TD"
    ],
    "country": [
      "USA"
    ],
    "uni": [
      "Georgia Institute of Technology",
      "Samsung semiconductor"
    ],
    "department": [
      "Advanced Logic Lab"
    ],
    "source_task_selection": [
      "all"
    ],
    "was_in_survey": [
      "mag"
    ],
    "in_title": [
      "t",
      "r",
      "l"
    ],
    "in_abs": [
      "t",
      "r",
      "l"
    ],
    "in_contet": [
      "t",
      "r",
      "l"
    ],
    "rejected_or_unpublished": null,
    "duplication_status": 2,
    "paper_for_thesis": 2966501669,
    "paper_available": 1,
    "behind_paywall": 0,
    "paywall_circumventable": 0,
    "author_names": [
      "Insik Yoon",
      "Arijit Raychowdhury",
      "Titash Rakshit",
      "Aqeel Anwar"
    ]
  },
  {
    "id": 2782656435,
    "title": "Sample-Efficient Reinforcement Learning through Transfer and Architectural Priors.",
    "abst": "Recent work in deep reinforcement learning has allowed algorithms to learn complex tasks such as Atari 2600 games just from the reward provided by the game, but these algorithms presently require millions of training steps in order to learn, making them approximately five orders of magnitude slower than humans. One reason for this is that humans build robust shared representations that are applicable to collections of problems, making it much easier to assimilate new variants. This paper first introduces the idea of automatically-generated game sets to aid in transfer learning research, and then demonstrates the utility of shared representations by showing that models can substantially benefit from the incorporation of relevant architectural priors. This technique affords a remarkable 50x positive transfer on a toy problem-set.",
    "url": "http://export.arxiv.org/pdf/1801.02268",
    "lang": "en",
    "authors": [
      305618809,
      2783350257
    ],
    "fos": [
      33923547,
      154945302,
      97541855,
      177769412,
      2993826840,
      119857082,
      150899416
    ],
    "journals": [
      2597173376
    ],
    "conferences": [],
    "conference_series": [],
    "references": [
      1757796397,
      2084323233,
      2088862785,
      2097381042,
      2107298017,
      2165698076,
      2258731934,
      2583761661,
      2885825670
    ],
    "filter_matches": [
      "tr",
      "tl",
      "trl",
      "rl"
    ],
    "rank": 20376,
    "citation_count": 9,
    "estimated_citation_count": 9,
    "publication_date": "2018-01-07",
    "found_in": 1,
    "transfer_experiment_type": [
      "s_i",
      "s_f",
      "levels"
    ],
    "transfer_experiment_subtype": [
      "multi"
    ],
    "transfer_data_type": [
      "pri",
      "fea"
    ],
    "transfer_performance_metrics": [
      "tr"
    ],
    "implementation": [
      "Custom",
      "CS",
      "TensorFlow",
      "Keras",
      "Gym",
      "Figures",
      "Formulas"
    ],
    "policy_type": [
      "DQN",
      "DDQN"
    ],
    "task_mappings": [
      "N/A"
    ],
    "autonomous_transfer": 0,
    "is_deep_rl": 1,
    "tags": [
      "2D",
      "Simulation",
      "Navigation",
      "Grid",
      "Pickup"
    ],
    "allowed_learner": [
      "TD"
    ],
    "country": [
      "USA"
    ],
    "uni": [
      "Cornell University"
    ],
    "department": [
      "SE(3) Group"
    ],
    "source_task_selection": [
      "h"
    ],
    "was_in_survey": [
      "mag"
    ],
    "in_title": [
      "t",
      "r",
      "l"
    ],
    "in_abs": [
      "t",
      "r",
      "l"
    ],
    "in_contet": [
      "t",
      "r",
      "l",
      "k",
      "s"
    ],
    "rejected_or_unpublished": 2,
    "duplication_status": null,
    "paper_for_thesis": 0,
    "paper_available": 1,
    "behind_paywall": 0,
    "paywall_circumventable": 0,
    "author_names": [
      "Serge Belongie",
      "Benjamin Spector"
    ]
  },
  {
    "id": 2953431737,
    "title": "End-to-end nonprehensile rearrangement with deep reinforcement learning and simulation-to-reality transfer",
    "abst": "Abstract Nonprehensile rearrangement is the problem of controlling a robot to interact with objects through pushing actions in order to reconfigure the objects into a predefined goal pose. In this work, we rearrange one object at a time in an environment with obstacles using an end-to-end policy that maps raw pixels as visual input to control actions without any form of engineered feature extraction. To reduce the amount of training data that needs to be collected using a real robot, we propose a simulation-to-reality transfer approach. In the first step, we model the nonprehensile rearrangement task in simulation and use deep reinforcement learning to learn a suitable rearrangement policy, which requires in the order of hundreds of thousands of example actions for training. Thereafter, we collect a small dataset of only 70 episodes of real-world actions as supervised examples for adapting the learned rearrangement policy to real-world input data. In this process, we make use of newly proposed strategies for improving the reinforcement learning process, such as heuristic exploration and the curation of a balanced set of experiences. We evaluate our method in both simulation and real setting using a Baxter robot to show that the proposed approach can effectively improve the training process in simulation, as well as efficiently adapt the learned policy to the real world application, even when the camera pose is different from simulation. Additionally, we show that the learned system not only can provide adaptive behavior to handle unforeseen events during executions, such as distraction objects, sudden changes in positions of the objects, and obstacles, but also can deal with obstacle shapes that were not present in the training process.",
    "url": "https://dblp.uni-trier.de/db/journals/ras/ras119.html#YuanHKWS19",
    "lang": "en",
    "authors": [
      326313970,
      2096852010,
      2158496488,
      2231443348,
      2793716566
    ],
    "fos": [
      173801870,
      97541855,
      2776378700,
      31972630,
      52622490,
      41008148,
      68784500,
      2776650193,
      154945302,
      90509273,
      74296488
    ],
    "journals": [
      133768115
    ],
    "conferences": [],
    "conference_series": [],
    "references": [
      143499627,
      757786120,
      1515851193,
      1521785144,
      1522301498,
      1522963174,
      1595483645,
      1731081199,
      1964478285,
      1982262386,
      2044995998,
      2047057213,
      2047085460,
      2068127265,
      2080279876,
      2083624955,
      2096698697,
      2104733512,
      2107281367,
      2109447584,
      2121863487,
      2134069397,
      2137766593,
      2145339207,
      2165698076,
      2173248099,
      2201581102,
      2201912979,
      2229258756,
      2405644564,
      2528489519,
      2529658650,
      2534269850,
      2565902248,
      2571420106,
      2575705757,
      2592873849,
      2601066903,
      2617317556,
      2729705546,
      2741122588,
      2758237641,
      2765349170,
      2793704326,
      2952606116,
      2953249127,
      2963094133,
      2968601742
    ],
    "filter_matches": [
      "tr",
      "tl",
      "trl",
      "rl"
    ],
    "rank": 20402,
    "citation_count": 4,
    "estimated_citation_count": 4,
    "publication_date": "2019-07-04",
    "found_in": 1,
    "transfer_experiment_type": [
      "t",
      "s_i",
      "s_f",
      "#"
    ],
    "transfer_experiment_subtype": [
      "sim2real",
      "multi"
    ],
    "transfer_data_type": [
      "pi",
      "fea"
    ],
    "transfer_performance_metrics": [
      "ap"
    ],
    "implementation": [
      "Custom",
      "CS",
      "Pseudo",
      "Figures",
      "Formulas",
      "Tables"
    ],
    "policy_type": [
      "DQN",
      "Q"
    ],
    "task_mappings": [
      "N/A"
    ],
    "autonomous_transfer": 0,
    "is_deep_rl": 1,
    "tags": [
      "Simulation",
      "RealWorld",
      "Robotics",
      "Arm",
      "Objects",
      "Obstacle"
    ],
    "allowed_learner": [
      "TD"
    ],
    "country": [
      "China",
      "USA",
      "Sweden"
    ],
    "uni": [
      "Hong Kong University of Science and Technology",
      "Yale University",
      "KTH Royal Institute of Technology",
      "rebro University"
    ],
    "department": [
      "Robotics Institute",
      "Mechanical Engineering and Material Science",
      "Centre for Autonomous System",
      "Centre for Applied Autonomous Sensor Systems"
    ],
    "source_task_selection": [
      "all"
    ],
    "was_in_survey": [
      "mag"
    ],
    "in_title": [
      "t",
      "r",
      "l"
    ],
    "in_abs": [
      "t",
      "r",
      "l"
    ],
    "in_contet": [
      "t",
      "r",
      "l",
      "k",
      "s"
    ],
    "rejected_or_unpublished": null,
    "duplication_status": null,
    "paper_for_thesis": 0,
    "paper_available": 1,
    "behind_paywall": 1,
    "paywall_circumventable": 1,
    "author_names": [
      "Danica Kragic",
      "Johannes A. Stork",
      "Kaiyu Hang",
      "Michael Yu Wang",
      "Weihao Yuan"
    ]
  },
  {
    "id": 2138497321,
    "title": "Representation Transfer for Reinforcement Learning",
    "abst": "Transfer learning problems are typically framed as leveraging knowledge learned on a source task to improve learning on a related, but different, target task. Current transfer lear ning methods are able to successfully transfer knowledge from a source reinforcement learning task into a target task, redu cing learning time. However, the complimentary task of transferring knowledge between agents with different internal representations has not been well explored The goal in both types of transfer problems is the same: reduce the time needed to learn the target with transfer, relative to learning the tar get without transfer. This work defines representation transfer, contrasts it with task transfer, and introduces two novel al gorithms. Additionally, we show representation transfer algorithms can also be successfully used for task transfer, pr oviding an empirical connection between the two problems. These algorithms are fully implemented in a complex multiagent domain and experiments demonstrate that transferring the learned knowledge between different representations is both possible and beneficial.",
    "url": "http://www.cs.utexas.edu/users/ai-lab/?AAAI07-Symposium-Taylor",
    "lang": "en",
    "authors": [
      2147180669,
      2148762994
    ],
    "fos": [
      47932503,
      150899416,
      154945302,
      41008148,
      77075516,
      28006648,
      60509570,
      119857082,
      97541855
    ],
    "journals": [],
    "conferences": [
      2785613717
    ],
    "conference_series": [
      1184914352
    ],
    "references": [
      29328521,
      36691172,
      65244443,
      1515851193,
      1585603966,
      1984542317,
      2097113539,
      2104641222,
      2111935653,
      2113913482,
      2119053738,
      2122982548,
      2128905965,
      2164998010,
      2169428619,
      2169659168,
      2180809782
    ],
    "filter_matches": [
      "tr",
      "tl",
      "trl",
      "rl"
    ],
    "rank": 20407,
    "citation_count": 17,
    "estimated_citation_count": 17,
    "publication_date": "2007-01-01",
    "found_in": 1,
    "transfer_experiment_type": [
      "a",
      "v"
    ],
    "transfer_experiment_subtype": [
      "diff-it"
    ],
    "transfer_data_type": [
      "Q"
    ],
    "transfer_performance_metrics": [
      "tt"
    ],
    "implementation": [
      "Custom",
      "CS",
      "Pseudo",
      "Formulas",
      "Figures"
    ],
    "policy_type": [
      "SARSA"
    ],
    "task_mappings": [
      "sup"
    ],
    "autonomous_transfer": 0,
    "is_deep_rl": 0,
    "tags": [
      "Simulation",
      "RoboCup",
      "KeepAway",
      "MultiAgent",
      "3v2",
      "4v3"
    ],
    "allowed_learner": [
      "TD",
      "PS"
    ],
    "country": [
      "USA"
    ],
    "uni": [
      "The University of Texas"
    ],
    "department": [
      "Computer Science"
    ],
    "source_task_selection": [
      "h"
    ],
    "was_in_survey": [
      "taylor",
      "mag",
      "lazaric",
      "zhu",
      "bone"
    ],
    "in_title": [
      "t",
      "r",
      "l"
    ],
    "in_abs": [
      "t",
      "r",
      "l",
      "k"
    ],
    "in_contet": [
      "t",
      "r",
      "l",
      "k"
    ],
    "rejected_or_unpublished": null,
    "duplication_status": null,
    "paper_for_thesis": 0,
    "paper_available": 1,
    "behind_paywall": 0,
    "paywall_circumventable": 0,
    "author_names": [
      "Peter Stone",
      "Matthew D. Taylor"
    ]
  },
  {
    "id": 1822705290,
    "title": "Effective control knowledge transfer through learning skill and representation hierarchies",
    "abst": "Learning capabilities of computer systems still lag far behind biological systems. One of the reasons can be seen in the inefficient re-use of control knowledge acquired over the lifetime of the artificial learning system. To address this deficiency, this paper presents a learning architecture which transfers control knowledge in the form of behavioral skills and corresponding representation concepts from one task to subsequent learning tasks. The presented system uses this knowledge to construct a more compact state space representation for learning while assuring bounded optimality of the learned task policy by utilizing a representation hierarchy. Experimental results show that the presented method can significantly outperform learning on a flat state space representation and the MAXQ method for hierarchical reinforcement learning.",
    "url": "https://ijcai.org/papers07/Papers/IJCAI07-331.pdf",
    "lang": null,
    "authors": [
      2132793154,
      2140678636
    ],
    "fos": [
      58973888,
      8038995,
      119857082,
      188888258,
      120822770,
      199190896,
      77967617,
      154945302,
      28006648,
      24138899,
      41008148
    ],
    "journals": [],
    "conferences": [
      2793510348
    ],
    "conference_series": [
      1203999783
    ],
    "references": [
      1553182805,
      1801398035,
      2038694949,
      2061504687,
      2102000945,
      2109910161,
      2111625828,
      2121517924,
      2126565096,
      2130903752
    ],
    "filter_matches": [
      "tl"
    ],
    "rank": 20412,
    "citation_count": 38,
    "estimated_citation_count": 58,
    "publication_date": "2007-01-06",
    "found_in": 1,
    "transfer_experiment_type": [
      "r"
    ],
    "transfer_experiment_subtype": [
      "same_all"
    ],
    "transfer_data_type": [
      "options"
    ],
    "transfer_performance_metrics": [
      "tt"
    ],
    "implementation": [
      "Custom",
      "CS",
      "Formulas",
      "Figures",
      "Web",
      "Dead",
      "UrbanCombatTestbed"
    ],
    "policy_type": [
      "Q"
    ],
    "task_mappings": [
      "N/A"
    ],
    "autonomous_transfer": 0,
    "is_deep_rl": 0,
    "tags": [
      "3D",
      "Navigation",
      "Simulation"
    ],
    "allowed_learner": [
      "H"
    ],
    "country": [
      "USA"
    ],
    "uni": [
      "West Chester University of Pennsylvania",
      "The University of Texas"
    ],
    "department": [
      "Computer Science",
      "Computer Science and Engineering"
    ],
    "source_task_selection": [
      "h"
    ],
    "was_in_survey": [
      "taylor"
    ],
    "in_title": [
      "t",
      "l"
    ],
    "in_abs": [
      "t",
      "r",
      "l"
    ],
    "in_contet": [
      "t",
      "r",
      "l"
    ],
    "rejected_or_unpublished": null,
    "duplication_status": null,
    "paper_for_thesis": 0,
    "paper_available": 1,
    "behind_paywall": 0,
    "paywall_circumventable": 0,
    "author_names": [
      "Mehran Asadi",
      "Manfred Huber"
    ]
  },
  {
    "id": 2963913081,
    "title": "Transfer Learning for Related Reinforcement Learning Tasks via Image-to-Image Translation",
    "abst": null,
    "url": "http://proceedings.mlr.press/v97/gamrian19a.html",
    "lang": "en",
    "authors": [
      2144962531,
      2809396666
    ],
    "fos": [
      150899416,
      41008148,
      154945302,
      119857082,
      2779757391,
      97541855
    ],
    "journals": [],
    "conferences": [
      2890215423
    ],
    "conference_series": [
      1180662882
    ],
    "references": [],
    "filter_matches": [
      "tr",
      "tl",
      "trl",
      "rl"
    ],
    "rank": 20420,
    "citation_count": 4,
    "estimated_citation_count": 4,
    "publication_date": "2019-06-09",
    "found_in": 1,
    "transfer_experiment_type": [
      "s",
      "t",
      "s_i",
      "s_f",
      "levels"
    ],
    "transfer_experiment_subtype": [
      "diff-no"
    ],
    "transfer_data_type": [
      "GAN",
      "imitation"
    ],
    "transfer_performance_metrics": [
      "ap",
      "tt"
    ],
    "implementation": [
      "OSS",
      "Custom",
      "PyTorch",
      "Gym",
      "Pseudo",
      "Formulas",
      "Figures",
      "Tables",
      "Videos"
    ],
    "policy_type": [
      "A2C",
      "A3C"
    ],
    "task_mappings": [
      "N/A"
    ],
    "autonomous_transfer": 0,
    "is_deep_rl": 1,
    "tags": [
      "2D",
      "Games",
      "VideoGames",
      "Atari",
      "Breakout",
      "Roadfighter",
      "Simulation"
    ],
    "allowed_learner": [
      "TD"
    ],
    "country": [
      "Israel"
    ],
    "uni": [
      "Bar-Ilan University",
      "Allen Institute for Artificial Intelligence"
    ],
    "department": [
      "Computer Science",
      "Industry"
    ],
    "source_task_selection": [
      "lib"
    ],
    "was_in_survey": [
      "mag"
    ],
    "in_title": [
      "t",
      "r",
      "l"
    ],
    "in_abs": [
      "t",
      "r",
      "l"
    ],
    "in_contet": [
      "t",
      "r",
      "l",
      "k"
    ],
    "rejected_or_unpublished": null,
    "duplication_status": null,
    "paper_for_thesis": 0,
    "paper_available": 1,
    "behind_paywall": 0,
    "paywall_circumventable": 0,
    "author_names": [
      "Yoav Goldberg",
      "Shani Gamrian"
    ]
  },
  {
    "id": 1598748993,
    "title": "Structure in the Space of Value Functions",
    "abst": "Solving in an efficient manner many different optimal control tasks within the same underlying environment requires decomposing the environment into its computationally elemental fragments. We suggest how to find fragmentations using unsupervised, mixture model, learning methods on data derived from optimal value functions for multiple tasks, and show that these fragmentations are in accord with observable structure in the environments. Further, we present evidence that such fragments can be of use in a practical reinforcement learning context, by facilitating online, actor-critic learning of multiple goals MDPs.",
    "url": "https://jhu.pure.elsevier.com/en/publications/structure-in-the-space-of-value-functions-3",
    "lang": "en",
    "authors": [
      2057359516,
      2141591103
    ],
    "fos": [
      37404715,
      58973888,
      91575142,
      77967617,
      119857082,
      8038995,
      154945302,
      61224824,
      97541855,
      188116033,
      178980831,
      33923547
    ],
    "journals": [
      62148650
    ],
    "conferences": [],
    "conference_series": [],
    "references": [
      16046748,
      125130877,
      354832773,
      1488730473,
      1515851193,
      1533169541,
      1568042657,
      1594216983,
      1600813180,
      1610678877,
      1631187438,
      1748123235,
      1756110333,
      1981814724,
      1992402718,
      2020149918,
      2037210683,
      2049633694,
      2061504687,
      2069317438,
      2091565802,
      2098589862,
      2099111195,
      2101533993,
      2102000945,
      2102409316,
      2103504761,
      2110415190,
      2111874892,
      2114451917,
      2117341272,
      2121863487,
      2125510930,
      2151454335,
      2156067405,
      2158548602,
      2160371091,
      2162837059,
      2259005268,
      2951774643,
      3011120880
    ],
    "filter_matches": [
      "ssvf"
    ],
    "rank": 20450,
    "citation_count": 47,
    "estimated_citation_count": 72,
    "publication_date": "2002-11-01",
    "found_in": 1,
    "transfer_experiment_type": [
      "s_f"
    ],
    "transfer_experiment_subtype": [
      "multi"
    ],
    "transfer_data_type": [
      "sub"
    ],
    "transfer_performance_metrics": [
      "j",
      "tr"
    ],
    "implementation": [
      "Custom",
      "CS",
      "Figures",
      "Formulas"
    ],
    "policy_type": [],
    "task_mappings": [
      "N/A"
    ],
    "autonomous_transfer": 0,
    "is_deep_rl": 0,
    "tags": [
      "2D",
      "Grid",
      "Navigation",
      "Simulation"
    ],
    "allowed_learner": [
      "TD",
      "H"
    ],
    "country": [
      "UK"
    ],
    "uni": [
      "University of Edinburgh"
    ],
    "department": [
      "Computational Neuroscience"
    ],
    "source_task_selection": [
      "all"
    ],
    "was_in_survey": [
      "taylor"
    ],
    "in_title": [
      "r",
      "l"
    ],
    "in_abs": [
      "r",
      "l"
    ],
    "in_contet": [
      "r",
      "l",
      "k"
    ],
    "rejected_or_unpublished": null,
    "duplication_status": null,
    "paper_for_thesis": 0,
    "paper_available": 1,
    "behind_paywall": 0,
    "paywall_circumventable": 0,
    "author_names": [
      "Peter Dayan"
    ]
  },
  {
    "id": 1457482454,
    "title": "Transfer learning in multi-agent reinforcement learning domains",
    "abst": "In the context of reinforcement learning, transfer learning refers to the concept of reusing knowledge acquired in past tasks to speed up the learning procedure in new tasks. Transfer learning methods have been succesfully applied in single-agent reinforcement learning algorithms, but no prior work has focused on applying them in a multi-agent environment. We propose a novel method for transfer learning in multi-agent reinforcement learning domains. We proceed to test the proposed approach in a multi-agent domain under various configurations. The results demonstrate that the method can reduce the learning time and increase the asymptotic performance of the learning algorithm.",
    "url": "https://doi.org/10.1007/978-3-642-29946-9_25",
    "lang": "en",
    "authors": [
      127847919,
      695239088,
      2686784933
    ],
    "fos": [
      8038995,
      97541855,
      119857082,
      188888258,
      28006648,
      154945302,
      58973888,
      24138899,
      41008148,
      199190896,
      77967617
    ],
    "journals": [],
    "conferences": [
      37046440
    ],
    "conference_series": [
      2757237056
    ],
    "references": [
      36691172,
      1487385582,
      1506146479,
      1564534945,
      1854866626,
      2014512216,
      2031727428,
      2079247031,
      2096882643,
      2097381042,
      2104602264,
      2110292307,
      2123354780,
      2133040789,
      2158150115
    ],
    "filter_matches": [
      "tr",
      "tl",
      "trl",
      "rl"
    ],
    "rank": 20464,
    "citation_count": 27,
    "estimated_citation_count": 27,
    "publication_date": "2011-09-09",
    "found_in": 1,
    "transfer_experiment_type": [
      "#",
      "s_i",
      "s_f"
    ],
    "transfer_experiment_subtype": [
      "multi"
    ],
    "transfer_data_type": [
      "Q",
      "pi"
    ],
    "transfer_performance_metrics": [
      "j",
      "ap"
    ],
    "implementation": [
      "Custom",
      "CS",
      "Pseudo",
      "Figures",
      "Formulas",
      "Tables"
    ],
    "policy_type": [
      "Q",
      "BITER"
    ],
    "task_mappings": [
      "Ma",
      "exp"
    ],
    "autonomous_transfer": 0,
    "is_deep_rl": 0,
    "tags": [
      "2D",
      "Navigation",
      "Grid",
      "Simulation",
      "PredatorPrey",
      "MultiAgent"
    ],
    "allowed_learner": [
      "TD"
    ],
    "country": [
      "Greece"
    ],
    "uni": [
      "Aristotle University"
    ],
    "department": [
      "Informatics"
    ],
    "source_task_selection": [
      "h"
    ],
    "was_in_survey": [
      "silva",
      "mag",
      "multi"
    ],
    "in_title": [
      "t",
      "r",
      "l"
    ],
    "in_abs": [
      "t",
      "r",
      "l",
      "k"
    ],
    "in_contet": [
      "t",
      "r",
      "l",
      "k"
    ],
    "rejected_or_unpublished": null,
    "duplication_status": null,
    "paper_for_thesis": 0,
    "paper_available": 1,
    "behind_paywall": 0,
    "paywall_circumventable": 0,
    "author_names": [
      "Ioannis Partalas",
      "Ioannis Vlahavas",
      "Georgios Boutsioukis"
    ]
  },
  {
    "id": 3001528895,
    "title": "Single Episode Policy Transfer in Reinforcement Learning.",
    "abst": null,
    "url": "http://arxiv.org/pdf/1910.07719.pdf",
    "lang": null,
    "authors": [
      2099091510,
      2636290417,
      2725665757,
      2788100770
    ],
    "fos": [
      2776731479,
      154945302,
      97541855,
      119857082,
      2910112148,
      41008148
    ],
    "journals": [],
    "conferences": [
      2924662680
    ],
    "conference_series": [
      2584161585
    ],
    "references": [],
    "filter_matches": [
      "tr",
      "tl",
      "trl",
      "rl"
    ],
    "rank": 20501,
    "citation_count": 0,
    "estimated_citation_count": 0,
    "publication_date": "2019-09-25",
    "found_in": 1,
    "transfer_experiment_type": [
      "t"
    ],
    "transfer_experiment_subtype": [
      "multi"
    ],
    "transfer_data_type": [
      "VAE",
      "latentspace"
    ],
    "transfer_performance_metrics": [
      "tt",
      "tr"
    ],
    "implementation": [
      "OSS",
      "Custom",
      "Pseudo",
      "Formulas",
      "Figures"
    ],
    "policy_type": [
      "DDQN"
    ],
    "task_mappings": [
      "N/A"
    ],
    "autonomous_transfer": 0,
    "is_deep_rl": 1,
    "tags": [
      "Simulation",
      "2D",
      "Navigation",
      "Acrobot",
      "HIV",
      "ClassicControl"
    ],
    "allowed_learner": [
      "TD",
      "MB"
    ],
    "country": [
      "USA"
    ],
    "uni": [
      "Georgia Institute of Technology",
      "Lawrence Livermore National Laboratory"
    ],
    "department": [],
    "source_task_selection": [
      "h"
    ],
    "was_in_survey": [
      "mag"
    ],
    "in_title": [
      "t",
      "r",
      "l"
    ],
    "in_abs": [
      "t",
      "r",
      "l"
    ],
    "in_contet": [
      "t",
      "r",
      "l",
      "k",
      "s"
    ],
    "rejected_or_unpublished": null,
    "duplication_status": null,
    "paper_for_thesis": 0,
    "paper_available": 1,
    "behind_paywall": 0,
    "paywall_circumventable": 0,
    "author_names": [
      "Hongyuan Zha",
      "Jiachen Yang",
      "Brenden K. Petersen",
      "Daniel Faissol"
    ]
  },
  {
    "id": 2887671224,
    "title": "Policy and Value Transfer in Lifelong Reinforcement Learning",
    "abst": null,
    "url": "http://proceedings.mlr.press/v80/abel18b.html",
    "lang": "en",
    "authors": [
      1075592465,
      1965389844,
      2480204961,
      2573981667,
      2804164797
    ],
    "fos": [
      97541855,
      154945302,
      119857082,
      2993118550,
      41008148
    ],
    "journals": [],
    "conferences": [
      2782892079
    ],
    "conference_series": [
      1180662882
    ],
    "references": [],
    "filter_matches": [
      "tr",
      "tl",
      "trl",
      "rl"
    ],
    "rank": 20502,
    "citation_count": 6,
    "estimated_citation_count": 6,
    "publication_date": "2018-07-03",
    "found_in": 1,
    "transfer_experiment_type": [
      "v",
      "s_i",
      "s_f",
      "levels"
    ],
    "transfer_experiment_subtype": [
      "multi"
    ],
    "transfer_data_type": [
      "Q"
    ],
    "transfer_performance_metrics": [
      "j",
      "tr",
      "tt",
      "ap"
    ],
    "implementation": [
      "OSS",
      "Custom",
      "Pseudo",
      "Formulas",
      "Figures",
      "Theorem",
      "Tables"
    ],
    "policy_type": [
      "Q",
      "Delayed-Q"
    ],
    "task_mappings": [
      "N/A"
    ],
    "autonomous_transfer": 0,
    "is_deep_rl": 0,
    "tags": [
      "2D",
      "Navigation",
      "Grid",
      "Simulation"
    ],
    "allowed_learner": [
      "TD",
      "MB"
    ],
    "country": [
      "USA"
    ],
    "uni": [
      "Brown University"
    ],
    "department": [
      "Computer Science"
    ],
    "source_task_selection": [
      "all"
    ],
    "was_in_survey": [
      "mag"
    ],
    "in_title": [
      "t",
      "r",
      "l"
    ],
    "in_abs": [
      "t",
      "l"
    ],
    "in_contet": [
      "t",
      "r",
      "l",
      "k",
      "s"
    ],
    "rejected_or_unpublished": null,
    "duplication_status": null,
    "paper_for_thesis": 0,
    "paper_available": 1,
    "behind_paywall": 0,
    "paywall_circumventable": 0,
    "author_names": [
      "Michael L. Littman",
      "George Konidaris",
      "David Abel",
      "Yuu Jinnai",
      "Sophie Yue Guo"
    ]
  },
  {
    "id": 1490954610,
    "title": "Learning relational options for inductive transfer in relational reinforcement learning",
    "abst": "In reinforcement learning problems, an agent has the task of learning a good or optimal strategy from interaction with his environment. At the start of the learning task, the agent usually has very little information. Therefore, when faced with complex problems that have a large state space, learning a good strategy might be infeasible or too slow to work in practice. One way to overcome this problem, is the use of guidance to supply the agent with traces of &quot;reasonable policies&quot;. However, in a lot of cases it will be hard for the user to supply such a policy. In this paper, we will investigate the use of transfer learning in Relational Reinforcement Learning. The goal of transfer learning is to accelerate learning on a target task after training on a different, but related, source task. More specifically, we introduce an extension of the options framework to the relational setting and show how one can learn skills that can be transferred across similar, but different domains. We present experiments showing the possible benefits of using relational options for transfer learning.",
    "url": "https://rd.springer.com/chapter/10.1007/978-3-540-78469-2_12",
    "lang": "en",
    "authors": [
      1275083776,
      1982051687,
      2016312802
    ],
    "fos": [
      97541855,
      77075516,
      154945302,
      188888258,
      119857082,
      150899416,
      177877439,
      41008148,
      77967617,
      28006648,
      12298181
    ],
    "journals": [],
    "conferences": [],
    "conference_series": [
      1200006883
    ],
    "references": [
      115717799,
      143164768,
      198956113,
      1492014007,
      1506146479,
      1510402218,
      1515851193,
      1592847719,
      2014512216,
      2031727428,
      2033072307,
      2109910161,
      2121863487,
      2133632477,
      2150821861,
      2154441654,
      2396715201,
      2503474909,
      2521075165,
      2612635229,
      3020831056
    ],
    "filter_matches": [
      "tr",
      "tl",
      "trl",
      "rl"
    ],
    "rank": 20511,
    "citation_count": 34,
    "estimated_citation_count": 71,
    "publication_date": "2007-06-19",
    "found_in": 1,
    "transfer_experiment_type": [
      "#"
    ],
    "transfer_experiment_subtype": [
      "diff-no"
    ],
    "transfer_data_type": [
      "options"
    ],
    "transfer_performance_metrics": [
      "ap",
      "j",
      "tr"
    ],
    "implementation": [
      "Custom",
      "CS",
      "Formulas",
      "Figures"
    ],
    "policy_type": [
      "Q",
      "TILDE"
    ],
    "task_mappings": [
      "N/A"
    ],
    "autonomous_transfer": 0,
    "is_deep_rl": 0,
    "tags": [
      "2D",
      "Navigation",
      "TicTacToe",
      "BoardGames",
      "Blocksworld",
      "Grid",
      "Simulation",
      "Games"
    ],
    "allowed_learner": [
      "RRL",
      "H"
    ],
    "country": [
      "Netherlands"
    ],
    "uni": [
      "K.U.Leuven"
    ],
    "department": [
      "Computer Science"
    ],
    "source_task_selection": [
      "h"
    ],
    "was_in_survey": [
      "taylor",
      "mag"
    ],
    "in_title": [
      "t",
      "r",
      "l"
    ],
    "in_abs": [
      "t",
      "r",
      "l"
    ],
    "in_contet": [
      "t",
      "r",
      "l",
      "k"
    ],
    "rejected_or_unpublished": null,
    "duplication_status": null,
    "paper_for_thesis": 0,
    "paper_available": 1,
    "behind_paywall": 1,
    "paywall_circumventable": 1,
    "author_names": [
      "Maurice Bruynooghe",
      "Kurt Driessens",
      "Tom Croonenborghs"
    ]
  },
  {
    "id": 2979341666,
    "title": "Autonomous Navigation via Deep Reinforcement Learning for Resource Constraint Edge Nodes using Transfer Learning",
    "abst": "Smart and agile drones are fast becoming ubiquitous at the edge of the cloud. The usage of these drones are constrained by their limited power and compute capability. In this paper, we present a Transfer Learning (TL) based approach to reduce on-board computation required to train a deep neural network for autonomous navigation via Deep Reinforcement Learning for a target algorithmic performance. A library of 3D realistic meta-environments is manually designed using Unreal Gaming Engine and the network is trained end-to-end. These trained meta-weights are then used as initializers to the network in a test environment and fine-tuned for the last few fully connected layers. Variation in drone dynamics and environmental characteristics is carried out to show robustness of the approach. Using NVIDIA GPU profiler it was shown that the energy consumption and training latency is reduced by 3.7x and 1.8x respectively without significant degradation in the performance in terms of average distance traveled before crash i.e. Mean Safe Flight (MSF). The approach is also tested on a real environment using DJI Tello drone and similar results were reported.",
    "url": "https://arxiv.org/abs/1910.05547",
    "lang": "en",
    "authors": [
      2698663285,
      2950376593
    ],
    "fos": [
      59519942,
      97970142,
      79974875,
      45374587,
      154945302,
      97541855,
      79403827,
      2780165032,
      150899416,
      50644808,
      33923547,
      119857082
    ],
    "journals": [
      2597173376
    ],
    "conferences": [],
    "conference_series": [],
    "references": [
      2097381042,
      2145339207,
      2150886651,
      2155968351,
      2163605009,
      2164114810,
      2165698076,
      2167215970,
      2173564293,
      2201581102,
      2257979135,
      2279098554,
      2395579298,
      2575472443,
      2604319603,
      2615547864,
      2765315405,
      2887280559,
      2945719686,
      2947960291,
      2962957005,
      2963376229,
      2963674932,
      2981230744
    ],
    "filter_matches": [
      "tr",
      "tl",
      "trl",
      "rl"
    ],
    "rank": 20518,
    "citation_count": 0,
    "estimated_citation_count": 0,
    "publication_date": "2019-10-12",
    "found_in": 1,
    "transfer_experiment_type": [
      "t",
      "s_i",
      "s_f",
      "levels"
    ],
    "transfer_experiment_subtype": [
      "sim2real",
      "multi"
    ],
    "transfer_data_type": [
      "pi",
      "offline"
    ],
    "transfer_performance_metrics": [
      "j",
      "tr",
      "ap",
      "tt"
    ],
    "implementation": [
      "OSS",
      "TensorFlow",
      "AirSim",
      "Figures",
      "Tables",
      "Formulas",
      "Pseudo",
      "Videos"
    ],
    "policy_type": [
      "DQN"
    ],
    "task_mappings": [
      "N/A"
    ],
    "autonomous_transfer": 0,
    "is_deep_rl": 1,
    "tags": [
      "3D",
      "Navigation",
      "Simulation",
      "Drone",
      "RealWorld",
      "CNN",
      "Aerial"
    ],
    "allowed_learner": [
      "TD"
    ],
    "country": [
      "USA"
    ],
    "uni": [
      "Georgia Institute of Technology"
    ],
    "department": [
      "Electrical and Computer Engineering"
    ],
    "source_task_selection": [
      "lib"
    ],
    "was_in_survey": [
      "mag"
    ],
    "in_title": [
      "t",
      "r",
      "l"
    ],
    "in_abs": [
      "t",
      "r",
      "l"
    ],
    "in_contet": [
      "t",
      "r",
      "l",
      "k",
      "s"
    ],
    "rejected_or_unpublished": null,
    "duplication_status": null,
    "paper_for_thesis": 0,
    "paper_available": 1,
    "behind_paywall": 0,
    "paywall_circumventable": 0,
    "author_names": [
      "Arijit Raychowdhury",
      "Aqeel Anwar"
    ]
  },
  {
    "id": 2181867278,
    "title": "Automatically Mapped Transfer between Reinforcement Learning Tasks via Three-Way Restricted Boltzmann Machines",
    "abst": "Existing reinforcement learning approaches are often hampered by learning tabula rasa. Transfer for reinforcement learning tackles this problem by enabling the reuse of previously learned results, but may require an inter-task mapping to encode how the previously learned task and the new task are related. This paper presents an autonomous framework for learning inter-task mappings based on an adaptation of restricted Boltzmann machines. Both a full model and a computationally efficient factored model are introduced and shown to be effective in multiple transfer learning scenarios.",
    "url": "https://research.tue.nl/en/publications/automatically-mapped-transfer-between-reinforcement-learning-task-2",
    "lang": "en",
    "authors": [
      1982051687,
      1984519118,
      2027870957,
      2142969897,
      2148762994,
      2215740056
    ],
    "fos": [
      8038995,
      154945302,
      41008148,
      58973888,
      24138899,
      119857082,
      188888258,
      199190896,
      28006648,
      97541855,
      77967617
    ],
    "journals": [],
    "conferences": [],
    "conference_series": [
      2755314191
    ],
    "references": [
      24477102,
      97016928,
      110451278,
      203338875,
      1515851193,
      1569756368,
      1626155273,
      2004030284,
      2012036715,
      2042492924,
      2072128103,
      2079247031,
      2097381042,
      2098723043,
      2116064496,
      2121863487,
      2123995443,
      2133040789,
      2154328025,
      2158150115,
      2312609093,
      2492778706,
      2525562026,
      2529326969,
      2911283634
    ],
    "filter_matches": [
      "tr",
      "tl",
      "trl",
      "rl"
    ],
    "rank": 20540,
    "citation_count": 16,
    "estimated_citation_count": 16,
    "publication_date": "2013-09-23",
    "found_in": 1,
    "transfer_experiment_type": [
      "a",
      "t",
      "r"
    ],
    "transfer_experiment_subtype": [
      "lit"
    ],
    "transfer_data_type": [
      "pi"
    ],
    "transfer_performance_metrics": [
      "j",
      "tt"
    ],
    "implementation": [
      "Custom",
      "CS",
      "Formulas",
      "Pseudo",
      "Figures"
    ],
    "policy_type": [
      "RBM"
    ],
    "task_mappings": [
      "exp"
    ],
    "autonomous_transfer": 1,
    "is_deep_rl": 1,
    "tags": [
      "Simulation",
      "MountainCar",
      "CartPole",
      "RBM",
      "ClassicControl"
    ],
    "allowed_learner": [
      "TD"
    ],
    "country": [
      "Netherlands",
      "USA"
    ],
    "uni": [
      "Maastricht University",
      "Washington State University"
    ],
    "department": [
      "Knowledge Engineering",
      "Electrical Engineering and Computer Science"
    ],
    "source_task_selection": [
      "all"
    ],
    "was_in_survey": [
      "mag"
    ],
    "in_title": [
      "t",
      "r",
      "l"
    ],
    "in_abs": [
      "t",
      "r",
      "l"
    ],
    "in_contet": [
      "t",
      "r",
      "l",
      "k"
    ],
    "rejected_or_unpublished": null,
    "duplication_status": null,
    "paper_for_thesis": 0,
    "paper_available": 1,
    "behind_paywall": 0,
    "paywall_circumventable": 0,
    "author_names": [
      "Kurt Driessens",
      "Decebal Constantin Mocanu",
      "Karl Tuyls",
      "Gerhard Weiss",
      "Matthew D. Taylor",
      "Haitham Bou Ammar"
    ]
  },
  {
    "id": 2313922182,
    "title": "Towards transferring skills to flexible surgical robots with programming by demonstration and reinforcement learning",
    "abst": "Flexible manipulators such as tendon-driven serpentine manipulators perform better than traditional rigid ones in minimally invasive surgical tasks, including navigation in confined space through key-hole like incisions. However, due to the inherent nonlinearities and model uncertainties, motion control of such manipulators becomes extremely challenging. In this work, a hybrid framework combining Programming by Demonstration (PbD) and reinforcement learning is proposed to solve this problem. Gaussian Mixture Models (GMM), Gaussian Mixture Regression (GMR) and linear regression are used to learn the inverse kinematic model of the manipulator from human demonstrations. The learned model is used as nominal model to calculate the output end-effector trajectories of the manipulator. Two surgical tasks are performed to demonstrate the effectiveness of reinforcement learning: tube insertion and circle following. Gaussian noise is introduced to the standard model and the disturbed models are fed to the manipulator to calculate the actuator input with respect to the task specific end-effector trajectories. An expectation maximization (E-M) based reinforcement learning algorithm is used to update the disturbed model with returns from rollouts. Simulation results have verified that the disturbed model can be converged to the standard one and the tracking accuracy is enhanced.",
    "url": "https://ieeexplore.ieee.org/document/7449855/",
    "lang": "en",
    "authors": [
      2109301670,
      2130362075,
      2307274210,
      2423073186
    ],
    "fos": [
      61224824,
      145565327,
      23224414,
      97541855,
      65244806,
      41008148,
      44154836,
      39920418,
      2779038628,
      4199805,
      182081679
    ],
    "journals": [],
    "conferences": [],
    "conference_series": [
      2754972727
    ],
    "references": [
      114517082,
      116902681,
      1601667748,
      1684361744,
      1964357740,
      1967728830,
      1977655452,
      1985690171,
      1995071895,
      2000691728,
      2012204020,
      2012479417,
      2016765487,
      2038061090,
      2047191624,
      2050268494,
      2059515884,
      2095804824,
      2102041666,
      2105594594,
      2110708319,
      2120636621,
      2120772693,
      2124609748,
      2129515556,
      2150205249,
      2167117957,
      2171428093,
      2340085100
    ],
    "filter_matches": [
      "tr",
      "tl",
      "trl",
      "rl"
    ],
    "rank": 20558,
    "citation_count": 12,
    "estimated_citation_count": 12,
    "publication_date": "2016-02-01",
    "found_in": 1,
    "transfer_experiment_type": [
      "t"
    ],
    "transfer_experiment_subtype": [
      "same_all",
      "sim2real"
    ],
    "transfer_data_type": [
      "pi"
    ],
    "transfer_performance_metrics": [],
    "implementation": [
      "Custom",
      "CS",
      "Figures",
      "Formulas"
    ],
    "policy_type": [
      "Q",
      "PoWER"
    ],
    "task_mappings": [
      "N/A"
    ],
    "autonomous_transfer": 0,
    "is_deep_rl": 0,
    "tags": [
      "Simulation",
      "Robotics",
      "Arm",
      "Surgery"
    ],
    "allowed_learner": [
      "TD"
    ],
    "country": [
      "China",
      "Singapore"
    ],
    "uni": [
      "University of Hong Kong",
      "National University of Singapore"
    ],
    "department": [
      "Industrial Manufacturing Systems Engineering",
      "Biomedical Engineering"
    ],
    "source_task_selection": [],
    "was_in_survey": [
      "mag"
    ],
    "in_title": [
      "t",
      "r",
      "l",
      "s"
    ],
    "in_abs": [
      "r",
      "l"
    ],
    "in_contet": [
      "r",
      "l"
    ],
    "rejected_or_unpublished": null,
    "duplication_status": null,
    "paper_for_thesis": 0,
    "paper_available": 1,
    "behind_paywall": 1,
    "paywall_circumventable": 1,
    "author_names": [
      "Hongliang Ren",
      "Henry Y. K. Lau",
      "Wenjun Xu",
      "Jie Chen"
    ]
  },
  {
    "id": 2944895663,
    "title": "TibGM: A Transferable and Information-Based Graphical Model Approach for Reinforcement Learning",
    "abst": "One of the challenges to reinforcement learning (RL) is scalable transferability among complex tasks. Incorporating a graphical model (GM), along with the rich family of related methods, as a basis for RL frameworks provides potential to address issues such as transferability, generalisation and exploration. Here we propose a flexible GM-based RL framework which leverages efficient inference procedures to enhance generalisation and transfer power. In our proposed transferable and information-based graphical model framework &#x2018;TibGM&#x2019;, we show the equivalence between our mutual information-based objective in the GM, and an RL consolidated objective consisting of a standard reward maximisation target and a generalisation/transfer objective. In settings where there is a sparse or deceptive reward signal, our TibGM framework is flexible enough to incorporate exploration bonuses depicting intrinsic rewards. We empirically verify improved performance and exploration power.",
    "url": "http://eprints.gla.ac.uk/212298/",
    "lang": "en",
    "authors": [
      2111050734,
      2600530591
    ],
    "fos": [
      41008148,
      61272859,
      152139883,
      154945302,
      177148314,
      97541855,
      2776214188,
      48044578,
      155846161,
      119857082,
      176935170
    ],
    "journals": [],
    "conferences": [
      2890215423
    ],
    "conference_series": [
      1180662882
    ],
    "references": [],
    "filter_matches": [
      "tr",
      "tl",
      "trl",
      "rl"
    ],
    "rank": 20566,
    "citation_count": 0,
    "estimated_citation_count": 0,
    "publication_date": "2019-06-09",
    "found_in": 1,
    "transfer_experiment_type": [
      "t",
      "a"
    ],
    "transfer_experiment_subtype": [
      "same_all"
    ],
    "transfer_data_type": [
      "model",
      "latent"
    ],
    "transfer_performance_metrics": [
      "ap",
      "tr"
    ],
    "implementation": [
      "Custom",
      "CS",
      "Formulas",
      "Gym",
      "rllab",
      "SAC",
      "Figures",
      "Tables",
      "github/haarnoja/sac"
    ],
    "policy_type": [
      "TibGM",
      "DDPG",
      "LSP",
      "SAC",
      "PPO",
      "ERL",
      "DIAYN",
      "VIREL",
      "GEP-PG",
      "ProMP"
    ],
    "task_mappings": [
      "N/A"
    ],
    "autonomous_transfer": 0,
    "is_deep_rl": 1,
    "tags": [
      "Simulation",
      "Robotics",
      "Mujoco",
      "Swimmer",
      "Hopper",
      "Walker2d",
      "HalfCheetah",
      "Ant",
      "Humanoid",
      "MountainCar",
      "ClassicControl"
    ],
    "allowed_learner": [
      "TD",
      "H"
    ],
    "country": [
      "UK"
    ],
    "uni": [
      "University of Cambridge",
      "Alan Turing Institute"
    ],
    "department": [
      "Engineering"
    ],
    "source_task_selection": [
      "h"
    ],
    "was_in_survey": [
      "mag"
    ],
    "in_title": [
      "t",
      "r",
      "l"
    ],
    "in_abs": [
      "t",
      "r",
      "l"
    ],
    "in_contet": [
      "t",
      "r",
      "l"
    ],
    "rejected_or_unpublished": null,
    "duplication_status": null,
    "paper_for_thesis": 0,
    "paper_available": 1,
    "behind_paywall": 0,
    "paywall_circumventable": 0,
    "author_names": [
      "Adrian Weller",
      "Tameem Adel"
    ]
  },
  {
    "id": 2735260544,
    "title": "Automatic Discovery and Transfer of Task Hierarchies in Reinforcement Learning",
    "abst": "Sequential decision tasks present many opportunities for the study of transfer learning. A principal one among them is the existence of multiple domains that share the same underlying causal structure for actions. We describe an approach that exploits this shared causal structure to discover a hierarchical task structure in a source domain, which in turn speeds up learning of task execution knowledge in a new target domain. Our approach is theoretically justi&#xFB01;ed and compares favorably to manually designed task hierarchies in learning ef&#xFB01;ciency in the target domain. We demonstrate that causally motivated task hierarchies transfer more robustly than other kinds of detailed knowledge that depend on the idiosyncrasies of the source domain and are hence less transferable.",
    "url": "https://dblp.uni-trier.de/db/journals/aim/aim32.html#MehtaRTD11",
    "lang": null,
    "authors": [
      160031478,
      1993564419,
      2150653897,
      2440006205
    ],
    "fos": [
      154945302,
      20162079,
      2776960227,
      31170391,
      28006648,
      163504300,
      97541855,
      41008148,
      119857082,
      124527596,
      150899416
    ],
    "journals": [
      163019073
    ],
    "conferences": [],
    "conference_series": [],
    "references": [
      69737162,
      89818670,
      131709709,
      1511612612,
      1544444076,
      1567001657,
      1592847719,
      1598052524,
      1603524419,
      2020294948,
      2079247031,
      2097381042,
      2104115362,
      2105489771,
      2109727766,
      2121517924,
      2121863487,
      2132057084,
      2147357263,
      2153668164,
      2154393523,
      2156493855,
      2161252410,
      2162227979
    ],
    "filter_matches": [
      "tr",
      "tl",
      "trl",
      "rl"
    ],
    "rank": 20575,
    "citation_count": 8,
    "estimated_citation_count": 8,
    "publication_date": "2011-03-16",
    "found_in": 1,
    "transfer_experiment_type": [
      "v",
      "a"
    ],
    "transfer_experiment_subtype": [
      "diff-no"
    ],
    "transfer_data_type": [
      "model",
      "hierarchy",
      "sub"
    ],
    "transfer_performance_metrics": [
      "j",
      "tt"
    ],
    "implementation": [
      "Custom",
      "CS",
      "Pseudo",
      "Figures"
    ],
    "policy_type": [
      "Q",
      "HI-MAT"
    ],
    "task_mappings": [
      "N/A"
    ],
    "autonomous_transfer": 0,
    "is_deep_rl": 0,
    "tags": [
      "Simulation",
      "2D",
      "Grid",
      "Navigation",
      "Resources",
      "Mining",
      "Wargus",
      "Taxi",
      "MultiAgent"
    ],
    "allowed_learner": [
      "TD",
      "H",
      "B"
    ],
    "country": [
      "USA"
    ],
    "uni": [
      "Oregon State University",
      "Case Western Reserve University"
    ],
    "department": [],
    "source_task_selection": [
      "h"
    ],
    "was_in_survey": [
      "mag"
    ],
    "in_title": [
      "t",
      "r",
      "l"
    ],
    "in_abs": [
      "t",
      "l",
      "k"
    ],
    "in_contet": [
      "t",
      "r",
      "l",
      "k",
      "s"
    ],
    "rejected_or_unpublished": null,
    "duplication_status": null,
    "paper_for_thesis": 0,
    "paper_available": 1,
    "behind_paywall": 0,
    "paywall_circumventable": 0,
    "author_names": [
      "Thomas G. Dietterich",
      "Prasad Tadepalli",
      "Neville Mehta",
      "Soumya Ray"
    ]
  },
  {
    "id": 2517639096,
    "title": "Graph based skill acquisition and transfer Learning for continuous reinforcement learning domains",
    "abst": "Introducing connectivity graph to model agent behavior and environment dynamics.A graph based approach for automatic skill acquisition.A skill based transfer learning method in continuous reinforcement learning domain. Since reinforcement learning algorithms suffer from the curse of dimensionality in continuous domains, generalization is the most challenging issue in this area. Both skill acquisition and transfer learning are successful techniques to overcome such problem that result in big improvements in agent learning performance. In this paper, we propose a novel graph based skill acquisition method, named GSL, and a skill based transfer learning framework, named STL. GSL discovers skills as high-level knowledge using community detection from connectivity graph, a model to capture not only the agents experience but also the environments dynamics. STL incorporates skills previously learned from source task to speed up learning on a new target task. The experimental results indicate the effectiveness of the proposed methods in dealing with continuous reinforcement learning problems. Display Omitted",
    "url": "http://www.sciencedirect.com/science/article/pii/S0167865516302112",
    "lang": "en",
    "authors": [
      222359281,
      2138632143
    ],
    "fos": [
      188888258,
      77967617,
      41008148,
      24138899,
      28006648,
      58973888,
      119857082,
      199190896,
      154945302,
      8038995,
      97541855
    ],
    "journals": [
      151820558
    ],
    "conferences": [],
    "conference_series": [],
    "references": [
      158722652,
      1414356350,
      1500331901,
      1549884255,
      1647196294,
      1822705290,
      1857789879,
      1963873191,
      1974758710,
      1979298416,
      1985756506,
      1989101984,
      2004030284,
      2009543464,
      2012231141,
      2013988526,
      2051464482,
      2067247412,
      2068019092,
      2068855660,
      2097381042,
      2106261932,
      2108535023,
      2109910161,
      2114580749,
      2131681506,
      2142502798,
      2161795906,
      2165698076,
      2168640731,
      2172131460,
      2237216255
    ],
    "filter_matches": [
      "tr",
      "tl",
      "trl",
      "rl"
    ],
    "rank": 20598,
    "citation_count": 11,
    "estimated_citation_count": 11,
    "publication_date": "2017-02-01",
    "found_in": 1,
    "transfer_experiment_type": [
      "s_i",
      "s_f",
      "s",
      "levels"
    ],
    "transfer_experiment_subtype": [
      "same_all"
    ],
    "transfer_data_type": [
      "Q",
      "graph"
    ],
    "transfer_performance_metrics": [
      "tt",
      "ap",
      "tr"
    ],
    "implementation": [
      "Custom",
      "CS",
      "Formulas",
      "Pseudo",
      "Figures",
      "Tables"
    ],
    "policy_type": [
      "GSL",
      "SARSA"
    ],
    "task_mappings": [
      "N/A"
    ],
    "autonomous_transfer": 0,
    "is_deep_rl": 0,
    "tags": [
      "Simulation",
      "2D",
      "Pinball",
      "GraphBased"
    ],
    "allowed_learner": [
      "TD",
      "H"
    ],
    "country": [
      "Iran"
    ],
    "uni": [
      "University of Tehran"
    ],
    "department": [
      "Electrical and Computer Engineering"
    ],
    "source_task_selection": [
      "all"
    ],
    "was_in_survey": [
      "mag"
    ],
    "in_title": [
      "t",
      "r",
      "l",
      "s"
    ],
    "in_abs": [
      "t",
      "r",
      "l",
      "s",
      "k"
    ],
    "in_contet": [
      "t",
      "r",
      "l",
      "s",
      "k"
    ],
    "rejected_or_unpublished": null,
    "duplication_status": null,
    "paper_for_thesis": 0,
    "paper_available": 1,
    "behind_paywall": 1,
    "paywall_circumventable": 1,
    "author_names": [
      "Farzaneh Shoeleh",
      "Masoud Asadpour"
    ]
  },
  {
    "id": 2966501669,
    "title": "Hierarchical Memory System With STT-MRAM and SRAM to Support Transfer and Real-Time Reinforcement Learning in Autonomous Drones",
    "abst": "This article presents a transfer learning (TL) followed by reinforcement learning (RL) algorithm mapped onto a hierarchical embedded memory system to meet the stringent power budgets of autonomous drones. The power reduction is achieved by 1. TL on meta-environments followed by online RL only on the last few layers of a deep convolutional neural network (CNN) instead of end-to-end (E2E) RL and 2. Mapping of the algorithm onto a memory hierarchy where the pre-trained weights of all the conv layers and the first few fully connected (FC) layers are stored in dense, low standby leakage Spin Transfer Torque (STT) RAM eNVM arrays and the weights of the last few FC layers are stored in the on-die SRAM. This memory hierarchy enables real-time RL as the drone explores unknown territories and the system only reads the weights from eNVM (that are slow and power hungry to write otherwise) for inference and uses the on-die SRAM for low latency training through both write and read of the weights of the last few layers. The proposed system is extensively simulated on a virtual environment and dissipates 83.5% lower energy per image frame as well as 79.4% lower latency as compared to E2E RL without any loss of accuracy. The speed of the drone is improved by a factor of $3\times $ due to higher frame rates as well.",
    "url": "https://ui.adsabs.harvard.edu/abs/2019IJEST...9..485Y/abstract",
    "lang": "en",
    "authors": [
      2303751162,
      2515360915,
      2698663285,
      2760868830,
      2884251016
    ],
    "fos": [
      97541855,
      81363708,
      98980195,
      9390403,
      41008148,
      79403827,
      2778100165,
      46891859,
      3261483,
      168834603,
      68043766
    ],
    "journals": [
      142323794
    ],
    "conferences": [],
    "conference_series": [],
    "references": [
      169931978,
      2084653194,
      2091320013,
      2092178912,
      2097381042,
      2100677568,
      2101568739,
      2107726111,
      2108598243,
      2121863487,
      2156728623,
      2164114810,
      2289252105,
      2308963902,
      2337180699,
      2442974303,
      2487018225,
      2500509404,
      2557283755,
      2575472443,
      2584580867,
      2585242491,
      2585407525,
      2594135538,
      2615547864,
      2618530766,
      2623060708,
      2742694477,
      2785635988,
      2787295179,
      2787453651,
      2789805928,
      2793776854,
      2809268437,
      2903841424,
      2912871364,
      2913402275,
      2945719533,
      2946435186,
      2962957005,
      2963491296,
      3011120880
    ],
    "filter_matches": [
      "tr",
      "tl",
      "trl",
      "rl"
    ],
    "rank": 20624,
    "citation_count": 0,
    "estimated_citation_count": 0,
    "publication_date": "2019-07-31",
    "found_in": 1,
    "transfer_experiment_type": [
      "s_i",
      "s_f",
      "levels"
    ],
    "transfer_experiment_subtype": [
      "multi"
    ],
    "transfer_data_type": [
      "fea",
      "pi"
    ],
    "transfer_performance_metrics": [
      "ap"
    ],
    "implementation": [
      "Custom",
      "CS",
      "Pseudo",
      "Figures",
      "TensorFlow",
      "AirSim",
      "Tables"
    ],
    "policy_type": [
      "DDQN",
      "DNN"
    ],
    "task_mappings": [
      "N/A"
    ],
    "autonomous_transfer": 0,
    "is_deep_rl": 1,
    "tags": [
      "RealTime",
      "Simulation",
      "Drone",
      "ObjectAvoidance",
      "CNN",
      "STT-MRAM",
      "Robotics",
      "Vehicle",
      "Aerial",
      "RealWorld"
    ],
    "allowed_learner": [
      "TD"
    ],
    "country": [
      "USA"
    ],
    "uni": [
      "Georgia Institute of Technology",
      "IBM T.J. Watson research center",
      "Samsung Semiconductor"
    ],
    "department": [
      "Industry",
      "Uni",
      "Colab"
    ],
    "source_task_selection": [
      "all"
    ],
    "was_in_survey": [
      "mag"
    ],
    "in_title": [
      "t",
      "r",
      "l"
    ],
    "in_abs": [
      "t",
      "r",
      "l"
    ],
    "in_contet": [
      "t",
      "r",
      "l",
      "k"
    ],
    "rejected_or_unpublished": null,
    "duplication_status": null,
    "paper_for_thesis": 0,
    "paper_available": 1,
    "behind_paywall": 1,
    "paywall_circumventable": 1,
    "author_names": [
      "Rajiv V. Joshi",
      "Insik Yoon",
      "Arijit Raychowdhury",
      "Titash Rakshit",
      "Malik Aqeel Anwar"
    ]
  },
  {
    "id": 2619275098,
    "title": "Personalizing a Dialogue System with Transfer Reinforcement Learning",
    "abst": "It is difficult to train a personalized task-oriented dialogue system because the data collected from each individual is often insufficient. Personalized dialogue systems trained on a small dataset can overfit and make it difficult to adapt to different user needs. One way to solve this problem is to consider a collection of multiple users&#039; data as a source domain and an individual user&#039;s data as a target domain, and to perform a transfer learning from the source to the target domain. By following this idea, we propose &quot;PETAL&quot;(PErsonalized Task-oriented diALogue), a transfer-learning framework based on POMDP to learn a personalized dialogue system. The system first learns common dialogue knowledge from the source domain and then adapts this knowledge to the target user. This framework can avoid the negative transfer problem by considering differences between source and target users. The policy in the personalized POMDP can learn to choose different actions appropriately for different users. Experimental results on a real-world coffee-shopping data and simulation data show that our personalized dialogue system can choose different optimal actions for different users, and thus effectively improve the dialogue quality under the personalized setting.",
    "url": "https://www.arxiv.org/abs/1610.02891",
    "lang": null,
    "authors": [
      2109031554,
      2530027321,
      2626076505,
      2648094648,
      2716735303
    ],
    "fos": [
      150899416,
      17098449,
      119857082,
      2779178101,
      124101348,
      41008148,
      97541855,
      3017568202,
      154945302,
      22019652
    ],
    "journals": [
      2596500785
    ],
    "conferences": [],
    "conference_series": [],
    "references": [
      10957333,
      114517082,
      225503657,
      889023230,
      1975244201,
      2046831886,
      2076337359,
      2097381042,
      2117989772,
      2152883745,
      2250558341,
      2340944142,
      2407709678,
      2410983263,
      2412715517,
      2463983882,
      2467963359,
      2559772828,
      2573814574,
      2951813108,
      2952013107
    ],
    "filter_matches": [
      "tr",
      "tl",
      "trl",
      "rl"
    ],
    "rank": 20633,
    "citation_count": 8,
    "estimated_citation_count": 8,
    "publication_date": "2016-10-10",
    "found_in": 1,
    "transfer_experiment_type": [
      "t",
      "#"
    ],
    "transfer_experiment_subtype": [
      "same_all"
    ],
    "transfer_data_type": [
      "Q"
    ],
    "transfer_performance_metrics": [
      "ap",
      "tr"
    ],
    "implementation": [
      "Custom",
      "CS",
      "Formulas",
      "Pseudo",
      "Figures",
      "Tables"
    ],
    "policy_type": [
      "Q"
    ],
    "task_mappings": [
      "lib"
    ],
    "autonomous_transfer": 0,
    "is_deep_rl": 0,
    "tags": [
      "Simulation",
      "Dialogue",
      "Personalized",
      "PETAL",
      "CoffeeShopping",
      "NLP"
    ],
    "allowed_learner": [
      "TD"
    ],
    "country": [
      "China"
    ],
    "uni": [
      "Hong Kong University"
    ],
    "department": [
      "Science and Technology"
    ],
    "source_task_selection": [],
    "was_in_survey": [
      "mag"
    ],
    "in_title": [
      "t",
      "r",
      "l"
    ],
    "in_abs": [
      "t",
      "r",
      "l",
      "k"
    ],
    "in_contet": [
      "t",
      "r",
      "l",
      "k"
    ],
    "rejected_or_unpublished": 2,
    "duplication_status": null,
    "paper_for_thesis": 0,
    "paper_available": 1,
    "behind_paywall": 0,
    "paywall_circumventable": 0,
    "author_names": [
      "Qiang Yang",
      "Jiajun Li",
      "Shuangyin Li",
      "Yu Zhang",
      "Kaixiang Mo"
    ]
  },
  {
    "id": 193076044,
    "title": "Using cases as heuristics in reinforcement learning: a transfer learning application",
    "abst": "In this paper we propose to combine three AI techniques to speed up a Reinforcement Learning algorithm in a Transfer Learning problem: Case-based Reasoning, Heuristically Accelerated Reinforcement Learning and Neural Networks. To do so, we propose a new algorithm, called L3, which works in 3 stages: in the first stage, it uses Reinforcement Learning to learn how to perform one task, and stores the optimal policy for this problem as a case-base; in the second stage, it uses a Neural Network to map actions from one domain to actions in the other domain and; in the third stage, it uses the case-base learned in the first stage as heuristics to speed up the learning performance in a related, but different, task. The RL algorithm used in the first phase is the Q-learning and in the third phase is the recently proposed Case-based Heuristically Accelerated Q-learning. A set of empirical evaluations were conducted in transferring the learning between two domains, the Acrobot and the Robocup 3D: the policy learned during the solution of the Acrobot Problem is transferred and used to speed up the learning of stability policies for a humanoid robot in the Robocup 3D simulator. The results show that the use of this algorithm can lead to a significant improvement in the performance of the agent.",
    "url": "http://digital.csic.es/bitstream/10261/60868/1/IJCAI%202011%20%281211-1217%29.pdf",
    "lang": "en",
    "authors": [
      189649061,
      2124955968,
      2138885720,
      2512040638
    ],
    "fos": [
      8038995,
      188888258,
      28006648,
      188116033,
      41008148,
      199190896,
      154945302,
      77967617,
      17061570,
      119857082,
      97541855
    ],
    "journals": [],
    "conferences": [
      94944669
    ],
    "conference_series": [
      1203999783
    ],
    "references": [
      10379689,
      36691172,
      90468634,
      1502099479,
      1515851193,
      1559035773,
      1592209052,
      1799762961,
      1817398672,
      1993277309,
      2004030284,
      2031727428,
      2076766268,
      2095182442,
      2097381042,
      2100726628,
      2106953752,
      2110292307,
      2121863487,
      2151340488,
      2153353285,
      2163808368,
      2166798247,
      3011120880
    ],
    "filter_matches": [
      "tr",
      "tl",
      "trl",
      "rl"
    ],
    "rank": 20643,
    "citation_count": 18,
    "estimated_citation_count": 18,
    "publication_date": "2011-07-16",
    "found_in": 1,
    "transfer_experiment_type": [
      "v",
      "t",
      "s",
      "r"
    ],
    "transfer_experiment_subtype": [
      "diff-it"
    ],
    "transfer_data_type": [
      "cases",
      "pi",
      "Q"
    ],
    "transfer_performance_metrics": [
      "ap"
    ],
    "implementation": [
      "Custom",
      "CS",
      "Formulas",
      "Figures",
      "Pseudo",
      "Tables"
    ],
    "policy_type": [
      "Q",
      "HAQL",
      "CB-HAQL",
      "L3"
    ],
    "task_mappings": [
      "sup",
      "exp"
    ],
    "autonomous_transfer": 0,
    "is_deep_rl": 0,
    "tags": [
      "2D",
      "3D",
      "2Dto3D",
      "Robotics",
      "Simulation",
      "Acrobot",
      "RoboCup3D",
      "ClassicControl"
    ],
    "allowed_learner": [
      "CBR",
      "TD"
    ],
    "country": [
      "Brazil",
      "Spain"
    ],
    "uni": [
      "Technological Institute of Aeronautics",
      " Universitat Autonoma de Barcelona"
    ],
    "department": [
      "Electrical Engineering",
      "Artificial Intelligence Research Institute"
    ],
    "source_task_selection": [
      "h"
    ],
    "was_in_survey": [
      "mag"
    ],
    "in_title": [
      "t",
      "r",
      "l"
    ],
    "in_abs": [
      "t",
      "r",
      "l"
    ],
    "in_contet": [
      "t",
      "r",
      "l",
      "k"
    ],
    "rejected_or_unpublished": null,
    "duplication_status": null,
    "paper_for_thesis": 0,
    "paper_available": 1,
    "behind_paywall": 0,
    "paywall_circumventable": 0,
    "author_names": [
      "Ramon L&#x00F3;pez de M&#x00E1;ntaras",
      "Reinaldo A. C. Bianchi",
      "Luiz A. Celiberto",
      "Jackson P. Matsuura"
    ]
  },
  {
    "id": 2925306934,
    "title": "How to pick the domain randomization parameters for sim-to-real transfer of reinforcement learning policies?",
    "abst": "Recently, reinforcement learning (RL) algorithms have demonstrated remarkable success in learning complicated behaviors from minimally processed input. However, most of this success is limited to simulation. While there are promising successes in applying RL algorithms directly on real systems, their performance on more complex systems remains bottle-necked by the relative data inefficiency of RL algorithms. Domain randomization is a promising direction of research that has demonstrated impressive results using RL algorithms to control real robots. At a high level, domain randomization works by training a policy on a distribution of environmental conditions in simulation. If the environments are diverse enough, then the policy trained on this distribution will plausibly generalize to the real world. A human-specified design choice in domain randomization is the form and parameters of the distribution of simulated environments. It is unclear how to the best pick the form and parameters of this distribution and prior work uses hand-tuned distributions. This extended abstract demonstrates that the choice of the distribution plays a major role in the performance of the trained policies in the real world and that the parameter of this distribution can be optimized to maximize the performance of the trained policies in the real world",
    "url": "http://ui.adsabs.harvard.edu/abs/2019arXiv190311774V/abstract",
    "lang": "en",
    "authors": [
      2204490279,
      2923114448,
      2924853859,
      2924882592,
      2925152633
    ],
    "fos": [
      2778869765,
      119857082,
      90509273,
      2989214694,
      2781134609,
      47822265,
      33923547,
      154945302,
      97541855
    ],
    "journals": [
      2597173376
    ],
    "conferences": [],
    "conference_series": [],
    "references": [
      2132083787,
      2344075909,
      2529573675,
      2587284713,
      2605102758,
      2736601468,
      2767050701,
      2787938642,
      2789525339,
      2793955514,
      2807400555,
      2885163910,
      2890803796,
      2894771014,
      2895958971,
      2896213312,
      2897345632,
      2899496413,
      2911087563,
      2912063360,
      2920096535,
      2921605821,
      2949608212,
      2951004968,
      2951553872,
      2962743868,
      2962749646,
      2962897886,
      2962902376,
      2962917939,
      2963047245,
      2963184939,
      2963630234,
      2963859851,
      2964340928
    ],
    "filter_matches": [
      "tr",
      "tl",
      "trl",
      "rl"
    ],
    "rank": 20649,
    "citation_count": 4,
    "estimated_citation_count": 4,
    "publication_date": "2019-03-28",
    "found_in": 1,
    "transfer_experiment_type": [
      "t"
    ],
    "transfer_experiment_subtype": [
      "same_all"
    ],
    "transfer_data_type": [
      "pi"
    ],
    "transfer_performance_metrics": [],
    "implementation": [
      "Formulas",
      "OSS",
      "Docker",
      "Gym",
      "Mujoco",
      "PPO",
      "Custom",
      "PyTorch"
    ],
    "policy_type": [
      "PPO"
    ],
    "task_mappings": [
      "N/A"
    ],
    "autonomous_transfer": 0,
    "is_deep_rl": 1,
    "tags": [
      "2D",
      "Robotics",
      "Simulation",
      "Dart",
      "Mujoco",
      "Hopper",
      "Walker"
    ],
    "allowed_learner": [
      "TD"
    ],
    "country": [
      "USA"
    ],
    "uni": [
      "University of California"
    ],
    "department": [
      "Computer Science and Engineering"
    ],
    "source_task_selection": [],
    "was_in_survey": [
      "mag"
    ],
    "in_title": [
      "t",
      "r",
      "l"
    ],
    "in_abs": [
      "r",
      "l"
    ],
    "in_contet": [
      "t",
      "r",
      "l",
      "k"
    ],
    "rejected_or_unpublished": 2,
    "duplication_status": null,
    "paper_for_thesis": 0,
    "paper_available": 1,
    "behind_paywall": 0,
    "paywall_circumventable": 0,
    "author_names": [
      "Sharad Vikram",
      "Quan Vuong",
      "Sicun Gao",
      "Hao Su",
      "Henrik Iskov Christensen"
    ]
  },
  {
    "id": 2897005774,
    "title": "Deep Transfer Reinforcement Learning for Text Summarization",
    "abst": "Deep neural networks are data hungry models and thus face difficulties when attempting to train on small text datasets. Transfer learning is a potential solution but their effectiveness in the text domain is not as explored as in areas such as image analysis. In this paper, we study the problem of transfer learning for text summarization and discuss why existing state-of-the-art models fail to generalize well on other (unseen) datasets. We propose a reinforcement learning framework based on a self-critic policy gradient approach which achieves good generalization and state-of-the-art results on a variety of datasets. Through an extensive set of experiments, we also show the ability of our proposed framework to fine-tune the text summarization model using only a few training samples. To the best of our knowledge, this is the first work that studies transfer learning in text summarization and provides a generic solution that works well on unseen data.",
    "url": "https://arxiv.org/abs/1810.06667",
    "lang": "en",
    "authors": [
      2100435683,
      2199255697,
      2222978247
    ],
    "fos": [
      170858558,
      154945302,
      119857082,
      33923547,
      97541855,
      2984842247,
      150899416
    ],
    "journals": [
      2597173376
    ],
    "conferences": [],
    "conference_series": [],
    "references": [
      648786980,
      1544827683,
      1731081199,
      2101105183,
      2134797427,
      2149933564,
      2154652894,
      2155541015,
      2250539671,
      2574535369,
      2593768305,
      2601450892,
      2606974598,
      2612675303,
      2753160622,
      2766462485,
      2769084352,
      2793348910,
      2803933017,
      2809324505,
      2887593177,
      2888757417,
      2890249124,
      2890494294,
      2896807716,
      2902744721,
      2951603207,
      2962678461,
      2962810352,
      2962849707,
      2962965405,
      2962972512,
      2962985882,
      2963094133,
      2963418779,
      2963929190
    ],
    "filter_matches": [
      "tr",
      "tl",
      "trl",
      "rl"
    ],
    "rank": 20683,
    "citation_count": 1,
    "estimated_citation_count": 1,
    "publication_date": "2018-10-15",
    "found_in": 1,
    "transfer_experiment_type": [
      "s",
      "r",
      "datasets"
    ],
    "transfer_experiment_subtype": [
      "multi"
    ],
    "transfer_data_type": [
      "pi",
      "pi_dyn"
    ],
    "transfer_performance_metrics": [
      "ap"
    ],
    "implementation": [
      "Custom",
      "CS",
      "Figures",
      "Formulas",
      "ROGUE",
      "Tables"
    ],
    "policy_type": [
      "PG",
      "SelfCriticPolicyGradient"
    ],
    "task_mappings": [
      "N/A"
    ],
    "autonomous_transfer": 0,
    "is_deep_rl": 1,
    "tags": [
      "Text",
      "Summarization",
      "ROGUE",
      "NLP",
      "Simulation",
      "CNN"
    ],
    "allowed_learner": [
      "TD"
    ],
    "country": [
      "USA"
    ],
    "uni": [
      "Virginia Tech"
    ],
    "department": [
      "Discovery Analytics Center"
    ],
    "source_task_selection": [
      "all"
    ],
    "was_in_survey": [
      "mag"
    ],
    "in_title": [
      "t",
      "r",
      "l"
    ],
    "in_abs": [
      "t",
      "r",
      "l",
      "k"
    ],
    "in_contet": [
      "t",
      "r",
      "l",
      "k"
    ],
    "rejected_or_unpublished": null,
    "duplication_status": null,
    "paper_for_thesis": 0,
    "paper_available": 1,
    "behind_paywall": 0,
    "paywall_circumventable": 0,
    "author_names": [
      "Chandan K. Reddy",
      "Naren Ramakrishnan",
      "Yaser Keneshloo"
    ]
  },
  {
    "id": 2605369401,
    "title": "Transfer Reinforcement Learning with Shared Dynamics",
    "abst": "This article addresses a particular Transfer Reinforcement Learning (RL) problem: when dynamics do not change from one task to another, and only the reward function does. Our method relies on two ideas, the first one is that transition samples obtained from a task can be reused to learn on any other task: an immediate reward estimator is learnt in a supervised fashion and for each sample, the reward entry is changed by its reward estimate. The second idea consists in adopting the optimism in the face of uncertainty principle and to use upper bound reward estimates. Our method is tested on a navigation task, under four Transfer RL experimental settings: with a known reward function, with strong and weak expert knowledge on the reward function, and with a completely unknown reward function. It is also evaluated in a Multi-Task RL experiment and compared with the state-of-the-art algorithms. Results reveal that this method constitutes a major improvement for transfer/multi-task problems that share dynamics.",
    "url": "https://hal.archives-ouvertes.fr/hal-01548649",
    "lang": "en",
    "authors": [
      2091572218,
      2251091607
    ],
    "fos": [
      185429906,
      41008148,
      181543814,
      119857082,
      97541855,
      154945302,
      77553402,
      9515004
    ],
    "journals": [],
    "conferences": [
      2786197818
    ],
    "conference_series": [
      1184914352
    ],
    "references": [
      16011919,
      30493077,
      158722652,
      1505937442,
      1701974503,
      1757796397,
      2048226872,
      2061562262,
      2097381042,
      2097931172,
      2117629901,
      2120346334,
      2133040789,
      2133419240,
      2145339207,
      2160589914,
      2257979135,
      2294805292,
      2407709678,
      2476025067,
      2489939061,
      2962767126
    ],
    "filter_matches": [
      "tr",
      "tl",
      "trl",
      "rl"
    ],
    "rank": 20694,
    "citation_count": 16,
    "estimated_citation_count": 16,
    "publication_date": "2017-02-04",
    "found_in": 1,
    "transfer_experiment_type": [
      "t",
      "s_i",
      "s_f",
      "levels"
    ],
    "transfer_experiment_subtype": [
      "multi"
    ],
    "transfer_data_type": [
      "r"
    ],
    "transfer_performance_metrics": [
      "j",
      "tt",
      "ap",
      "tr"
    ],
    "implementation": [
      "Custom",
      "CS",
      "Formulas",
      "Pseudo",
      "Figures"
    ],
    "policy_type": [
      "Q"
    ],
    "task_mappings": [
      "N/A"
    ],
    "autonomous_transfer": 0,
    "is_deep_rl": 0,
    "tags": [
      "2D",
      "Navigation",
      "Grid",
      "Simulation"
    ],
    "allowed_learner": [
      "TD"
    ],
    "country": [
      "France",
      "Canada"
    ],
    "uni": [
      "Orange Labs at Chtillon",
      "Maluuba at Montral"
    ],
    "department": [
      "Industry"
    ],
    "source_task_selection": [
      "lib"
    ],
    "was_in_survey": [
      "mag"
    ],
    "in_title": [
      "t",
      "r",
      "l"
    ],
    "in_abs": [
      "t",
      "r",
      "l",
      "k"
    ],
    "in_contet": [
      "t",
      "r",
      "l",
      "k"
    ],
    "rejected_or_unpublished": null,
    "duplication_status": null,
    "paper_for_thesis": 0,
    "paper_available": 1,
    "behind_paywall": 0,
    "paywall_circumventable": 0,
    "author_names": [
      "Romain Laroche",
      "Merwan Barlier"
    ]
  },
  {
    "id": 3010212570,
    "title": "Exploiting Multi-Task Learning to Achieve Effective Transfer Deep Reinforcement Learning in Elastic Optical Networks",
    "abst": "We propose a multi-task-learning-aided knowledge transferring approach for effective and scalable deep reinforcement learning in EONs. Case studies with RMSA show that this approach can achieve &#x223C; 4&#x00D7; learning time reduction and &#x223C; 17.7% lower blocking probability.",
    "url": "https://www.osapublishing.org/abstract.cfm?uri=OFC-2020-M1B.3",
    "lang": "en",
    "authors": [
      2019333507,
      2098788672,
      2277500096,
      2280885740,
      2989709653
    ],
    "fos": [
      24326235,
      127413603,
      500300565,
      97541855,
      48044578,
      50644808,
      54956558,
      28006648
    ],
    "journals": [],
    "conferences": [],
    "conference_series": [
      2621451516
    ],
    "references": [
      2100828185,
      2772865543,
      2885792477,
      2890645375,
      2919334842,
      2951309479
    ],
    "filter_matches": [
      "tr",
      "tl",
      "trl",
      "rl"
    ],
    "rank": 20701,
    "citation_count": 1,
    "estimated_citation_count": 1,
    "publication_date": "2020-03-08",
    "found_in": 1,
    "transfer_experiment_type": [
      "#"
    ],
    "transfer_experiment_subtype": [
      "multi"
    ],
    "transfer_data_type": [
      "pi",
      "fea"
    ],
    "transfer_performance_metrics": [
      "j",
      "tt",
      "tr",
      "ap"
    ],
    "implementation": [
      "Custom",
      "CS",
      "Formulas",
      "Figures",
      "Tables"
    ],
    "policy_type": [
      "A3C"
    ],
    "task_mappings": [
      "N/A"
    ],
    "autonomous_transfer": 0,
    "is_deep_rl": 1,
    "tags": [
      "Elastic",
      "Optical",
      "Network",
      "Simulation"
    ],
    "allowed_learner": [
      "TD"
    ],
    "country": [
      "USA",
      "China"
    ],
    "uni": [
      "University of California",
      "University of Science and Technology of China"
    ],
    "department": [],
    "source_task_selection": [
      "all"
    ],
    "was_in_survey": [
      "mag"
    ],
    "in_title": [
      "t",
      "r",
      "l"
    ],
    "in_abs": [
      "t",
      "r",
      "l",
      "k"
    ],
    "in_contet": [
      "t",
      "r",
      "l",
      "k"
    ],
    "rejected_or_unpublished": null,
    "duplication_status": null,
    "paper_for_thesis": 0,
    "paper_available": 1,
    "behind_paywall": 0,
    "paywall_circumventable": 0,
    "author_names": [
      "S. J. Ben Yoo",
      "Xiaoliang Chen",
      "Zuqing Zhu",
      "Roberto Proietti",
      "Che-Yu Liu"
    ]
  },
  {
    "id": 2403765497,
    "title": "Directed Exploration in Reinforcement Learning with Transferred Knowledge",
    "abst": "Experimental results suggest that transferred knowledge can reduce the number of exploratory actions needed by reinforcement learning (RL) algorithms to nd acceptable solutions in Markov decision processes compared to learning from scratch. However, most existing transfer learning algorithms for RL are heuristic and transferred knowledge may unexpectedly result in worse performance than learning from scratch (i.e., negative transfer). We introduce a transfer learning algorithm that employs directed exploration, which allows us to motivate our algorithm by analyzing its sample complexity of exploration in the target task. We dene positive and negative transfer from a sample complexity perspective and provide conditions when our algorithm will avoid negative transfer as well as conditions where our algorithm guarantees positive transfer, with high probability. Finally, we demonstrate the advantages of our algorithm experimentally.",
    "url": "http://proceedings.mlr.press/v24/mann12a.html",
    "lang": "en",
    "authors": [
      2100368226,
      2146971270
    ],
    "fos": [
      150899416,
      173801870,
      106189395,
      154945302,
      77075516,
      119857082,
      2781235140,
      41008148,
      3020234346,
      2779178101,
      97541855
    ],
    "journals": [],
    "conferences": [
      585632621
    ],
    "conference_series": [
      2757237056
    ],
    "references": [
      24477102,
      107583932,
      1488730473,
      1504212531,
      1505937442,
      1526654727,
      2056584142,
      2097381042,
      2114580749,
      2123447947,
      2129670787,
      2133040789,
      2142502798,
      2158150115,
      2166265228,
      2312609093,
      2489939061
    ],
    "filter_matches": [
      "tr",
      "tl",
      "trl",
      "rl"
    ],
    "rank": 20746,
    "citation_count": 14,
    "estimated_citation_count": 14,
    "publication_date": "2012-01-01",
    "found_in": 1,
    "transfer_experiment_type": [
      "s",
      "s_i",
      "s_f",
      "a"
    ],
    "transfer_experiment_subtype": [
      "multi"
    ],
    "transfer_data_type": [
      "Q"
    ],
    "transfer_performance_metrics": [
      "j",
      "tt"
    ],
    "implementation": [
      "Custom",
      "CS",
      "Theorem",
      "Formulas",
      "Lemma",
      "Figures"
    ],
    "policy_type": [
      "DelayedQ"
    ],
    "task_mappings": [
      "Ma"
    ],
    "autonomous_transfer": 0,
    "is_deep_rl": 0,
    "tags": [
      "Robotics",
      "Kinematic",
      "Arm",
      "Simulation"
    ],
    "allowed_learner": [
      "TD"
    ],
    "country": [
      "USA"
    ],
    "uni": [
      "Texas A&M University"
    ],
    "department": [
      "Computer Science and Engineering"
    ],
    "source_task_selection": [],
    "was_in_survey": [
      "mag"
    ],
    "in_title": [
      "t",
      "r",
      "l",
      "k"
    ],
    "in_abs": [
      "t",
      "r",
      "l"
    ],
    "in_contet": [
      "t",
      "r",
      "l",
      "k"
    ],
    "rejected_or_unpublished": null,
    "duplication_status": null,
    "paper_for_thesis": 0,
    "paper_available": 1,
    "behind_paywall": 0,
    "paywall_circumventable": 0,
    "author_names": [
      "Yoonsuck Choe",
      "Timothy A. Mann"
    ]
  },
  {
    "id": 2049423956,
    "title": "Transfer learning with Partially Constrained Models: Application to reinforcement learning of linked multicomponent robot system control",
    "abst": "Abstract Transfer learning is a hierarchical approach to reinforcement learning of complex tasks modeled as Markov Decision Processes. The learning results on the source task are used as the starting point for the learning on the target task. In this paper we deal with a hierarchy of constrained systems, where the source task is an under-constrained system, hence called the Partially Constrained Model (PCM). Constraints in the framework of reinforcement learning are dealt with by state-action veto policies. We propose a theoretical background for the hierarchy of training refinements, showing that the effective action repertoires learnt on the PCM are maximal, and that the PCM-optimal policy gives maximal state value functions. We apply the approach to learn the control of Linked Multicomponent Robotic Systems using Reinforcement Learning. The paradigmatic example is the transportation of a hose. The system has strong physical constraints and a large state space. Learning experiments in the target task are realized over an accurate but computationally expensive simulation of the hose dynamics. The PCM is obtained simplifying the hose model. Learning results of the PCM Transfer Learning show an spectacular improvement over conventional Q-learning on the target task.",
    "url": "https://dblp.uni-trier.de/db/journals/ras/ras61.html#Fernandez-GaunaLG13",
    "lang": "en",
    "authors": [
      1749970258,
      1964126387,
      2171936949
    ],
    "fos": [
      28006648,
      41008148,
      112972136,
      188888258,
      97541855,
      24138899,
      154945302,
      77075516,
      44154836,
      77967617,
      8038995,
      119857082
    ],
    "journals": [
      133768115
    ],
    "conferences": [],
    "conference_series": [],
    "references": [
      15411808,
      24477102,
      32403112,
      134786152,
      564286975,
      1490954610,
      1506085041,
      1507591516,
      1508861003,
      1515436326,
      1517733364,
      1533597678,
      1545006598,
      1557517019,
      1557760016,
      1584313244,
      1586162706,
      1607318605,
      1870548046,
      1996625075,
      2001494341,
      2004030284,
      2041872606,
      2054756905,
      2089561656,
      2097381042,
      2101075098,
      2109805926,
      2109910161,
      2118022839,
      2121863487,
      2134153324,
      2138497321,
      2153204852
    ],
    "filter_matches": [
      "tr",
      "tl",
      "trl",
      "rl"
    ],
    "rank": 20758,
    "citation_count": 16,
    "estimated_citation_count": 16,
    "publication_date": "2013-07-01",
    "found_in": 1,
    "transfer_experiment_type": [
      "v",
      "t",
      "r"
    ],
    "transfer_experiment_subtype": [
      "multi"
    ],
    "transfer_data_type": [
      "Q"
    ],
    "transfer_performance_metrics": [
      "tt",
      "ap"
    ],
    "implementation": [
      "Custom",
      "CS",
      "Formulas",
      "Figures",
      "Videos"
    ],
    "policy_type": [
      "Q"
    ],
    "task_mappings": [
      "Ma"
    ],
    "autonomous_transfer": 0,
    "is_deep_rl": 0,
    "tags": [
      "Robotics",
      "Simulation",
      "Hose",
      "PCM",
      "2D",
      "Navigation"
    ],
    "allowed_learner": [
      "TD"
    ],
    "country": [
      "Spain"
    ],
    "uni": [
      "University of the Basque Country"
    ],
    "department": [
      "Computational Intelligence Group"
    ],
    "source_task_selection": [
      "h"
    ],
    "was_in_survey": [
      "mag"
    ],
    "in_title": [
      "t",
      "r",
      "l"
    ],
    "in_abs": [
      "t",
      "r",
      "l"
    ],
    "in_contet": [
      "t",
      "r",
      "l",
      "k"
    ],
    "rejected_or_unpublished": null,
    "duplication_status": null,
    "paper_for_thesis": 0,
    "paper_available": 1,
    "behind_paywall": 1,
    "paywall_circumventable": 1,
    "author_names": [
      "Borja Fernandez-Gauna",
      "Jose Manuel Lopez-Guede",
      "Manuel Gra&#x00F1;a"
    ]
  },
  {
    "id": 2533806771,
    "title": "A Reinforcement Learning Architecture That Transfers Knowledge Between Skills When Solving Multiple Tasks",
    "abst": "When humans learn several skills to solve multiple tasks, they exhibit an extraordinary capacity to transfer knowledge between them. We present here the last enhanced version of a bio-inspired reinforcement-learning (RL) modular architecture able to perform skill-to-skill knowledge transfer and called transfer expert RL (TERL) model. TERL architecture is based on a RL actor&#x2013;critic model where both actor and critic have a hierarchical structure, inspired by the mixture-of-experts model, formed by a gating network that selects experts specializing in learning the policies or value functions of different tasks. A key feature of TERL is the capacity of its gating networks to accumulate, in parallel, evidence on the capacity of experts to solve the new tasks so as to increase the responsibility for action of the best ones. A second key feature is the use of two different responsibility signals for the experts&#x2019; functioning and learning: this allows the training of multiple experts for each task so that some of them can be later recruited to solve new tasks and avoid catastrophic interference. The utility of TERL mechanisms is shown with tests involving two simulated dynamic robot arms engaged in solving reaching tasks, in particular a planar 2-DoF arm, and a 3-D 4-DoF arm.",
    "url": "http://ieeexplore.ieee.org/document/7592409/",
    "lang": "en",
    "authors": [
      1985823890,
      2101452168,
      2304962733,
      2399943886
    ],
    "fos": [
      2776960227,
      154945302,
      28006648,
      123657996,
      90509273,
      188888258,
      41008148,
      119857082,
      157590458,
      97541855,
      2987169351
    ],
    "journals": [
      2488537894
    ],
    "conferences": [],
    "conference_series": [],
    "references": [
      22241732,
      144463461,
      158722652,
      169931978,
      170861134,
      656732519,
      755046805,
      1486707268,
      1490449635,
      1492014007,
      1492713280,
      1530490888,
      1541981780,
      1562462135,
      1590744975,
      1592847719,
      1682403713,
      1922715780,
      1979464096,
      1979500821,
      1981410850,
      1989121059,
      1991564165,
      1998152406,
      1998994802,
      1999181826,
      2004030284,
      2005269174,
      2005663711,
      2007813200,
      2008707229,
      2012036715,
      2019435771,
      2020925019,
      2021947606,
      2022163520,
      2026018190,
      2027243898,
      2031727428,
      2035848490,
      2038269764,
      2050172486,
      2060052647,
      2060277733,
      2060425914,
      2064527819,
      2068077601,
      2076214212,
      2078672925,
      2081710423,
      2091565802,
      2097381042,
      2097861969,
      2099652807,
      2099860458,
      2101524054,
      2103768739,
      2108535023,
      2109910161,
      2110278466,
      2113122939,
      2113306468,
      2113501460,
      2117726420,
      2121863487,
      2124175081,
      2128562176,
      2132057084,
      2132622533,
      2133853511,
      2143435603,
      2148201341,
      2150884987,
      2151137320,
      2153562927,
      2155027007,
      2157904933,
      2159425529,
      2162892219,
      2167518172,
      2168342951,
      2169743339,
      2170418363,
      2185015080,
      2321749259,
      2323447981,
      2334036380,
      2506243885,
      2535997952,
      2552623879,
      2588399964,
      2737153679,
      2952823459,
      2965916140,
      2986886066
    ],
    "filter_matches": [
      "tr",
      "tl",
      "trl",
      "rl"
    ],
    "rank": 20785,
    "citation_count": 4,
    "estimated_citation_count": 4,
    "publication_date": "2019-06-01",
    "found_in": 1,
    "transfer_experiment_type": [
      "t",
      "s",
      "s_i",
      "s_f"
    ],
    "transfer_experiment_subtype": [
      "multi"
    ],
    "transfer_data_type": [
      "advisor"
    ],
    "transfer_performance_metrics": [
      "tt"
    ],
    "implementation": [
      "iCub",
      "Custom",
      "CS",
      "Formulas",
      "Figures",
      "Tables"
    ],
    "policy_type": [
      "TERL"
    ],
    "task_mappings": [
      "exp"
    ],
    "autonomous_transfer": 1,
    "is_deep_rl": 1,
    "tags": [
      "Simulation",
      "2D",
      "Robotics",
      "Arm",
      "3D",
      "iCub"
    ],
    "allowed_learner": [
      "TD",
      "H"
    ],
    "country": [
      "Singapore",
      "Italy"
    ],
    "uni": [
      "Nanyang Technological University",
      "Laboratory of Computational Embodied Neuroscience"
    ],
    "department": [
      "Robotics Research Centre"
    ],
    "source_task_selection": [
      "all"
    ],
    "was_in_survey": [
      "mag"
    ],
    "in_title": [
      "t",
      "r",
      "l",
      "k",
      "s"
    ],
    "in_abs": [
      "t",
      "r",
      "l",
      "k",
      "s"
    ],
    "in_contet": [
      "t",
      "r",
      "l",
      "k",
      "s"
    ],
    "rejected_or_unpublished": null,
    "duplication_status": null,
    "paper_for_thesis": 0,
    "paper_available": 1,
    "behind_paywall": 0,
    "paywall_circumventable": 0,
    "author_names": [
      "Paolo Tommasino",
      "Marco Mirolli",
      "Daniele Caligiore",
      "Gianluca Baldassarre"
    ]
  },
  {
    "id": 2951871955,
    "title": "Transfer in Deep Reinforcement Learning Using Successor Features and Generalised Policy Improvement",
    "abst": "The ability to transfer skills across tasks has the potential to scale up reinforcement learning (RL) agents to environments currently out of reach. Recently, a framework based on two ideas, successor features (SFs) and generalised policy improvement (GPI), has been introduced as a principled way of transferring skills. In this paper we extend the SFs &amp; GPI framework in two ways. One of the basic assumptions underlying the original formulation of SFs &amp; GPI is that rewards for all tasks of interest can be computed as linear combinations of a fixed set of features. We relax this constraint and show that the theoretical guarantees supporting the framework can be extended to any set of tasks that only differ in the reward function. Our second contribution is to show that one can use the reward functions themselves as features for future tasks, without any loss of expressiveness, thus removing the need to specify a set of features beforehand. This makes it possible to combine SFs &amp; GPI with deep learning in a more stable way. We empirically verify this claim on a complex 3D environment where observations are images from a first-person perspective. We show that the transfer promoted by SFs &amp; GPI leads to very good policies on unseen tasks almost instantaneously. We also describe how to learn policies specialised to the new tasks in a way that allows them to be added to the agent&#039;s set of skills, and thus be reused in the future.",
    "url": "http://export.arxiv.org/pdf/1901.10964",
    "lang": "en",
    "authors": [
      55380488,
      254054780,
      1252112997,
      2227815835,
      2564593992,
      2593774290,
      2631986562,
      2686305946,
      2949153525
    ],
    "fos": [
      108583219,
      97541855,
      9260844,
      119857082,
      154945302,
      75306776,
      92811239,
      33923547
    ],
    "journals": [
      2597173376
    ],
    "conferences": [],
    "conference_series": [],
    "references": [
      158722652,
      1576452626,
      2006791053,
      2056354534,
      2097381042,
      2112796928,
      2119567691,
      2121863487,
      2145339207,
      2162888803,
      2257979135,
      2417089653,
      2426267443,
      2526379199,
      2534060593,
      2604636228,
      2726187156,
      2735995851,
      2765602917,
      2774742309,
      2786036274,
      2804673281,
      2913340405,
      2949267040,
      2949819354,
      2953326374,
      2962717849,
      3011120880
    ],
    "filter_matches": [
      "tr",
      "tl",
      "trl",
      "rl"
    ],
    "rank": 20786,
    "citation_count": 5,
    "estimated_citation_count": 5,
    "publication_date": "2019-01-30",
    "found_in": 1,
    "transfer_experiment_type": [
      "s"
    ],
    "transfer_experiment_subtype": [
      "multi"
    ],
    "transfer_data_type": [
      "fea"
    ],
    "transfer_performance_metrics": [
      "j",
      "tt",
      "tr",
      "ap"
    ],
    "implementation": [
      "Custom",
      "CS",
      "Formulas",
      "Figures",
      "Pseudo",
      "Tables",
      "Videos"
    ],
    "policy_type": [
      "Q",
      "SF+GPI-Q",
      "DQ"
    ],
    "task_mappings": [
      "N/A"
    ],
    "autonomous_transfer": 0,
    "is_deep_rl": 1,
    "tags": [
      "3D",
      "Navigation",
      "Simulation",
      "CNN",
      "Successor",
      "Collect"
    ],
    "allowed_learner": [
      "TD"
    ],
    "country": [
      "USA"
    ],
    "uni": [
      "DeepMind"
    ],
    "department": [
      "Industry"
    ],
    "source_task_selection": [
      "lib"
    ],
    "was_in_survey": [
      "mag",
      "zhu"
    ],
    "in_title": [
      "t",
      "r",
      "l"
    ],
    "in_abs": [
      "t",
      "r",
      "l",
      "s"
    ],
    "in_contet": [
      "t",
      "r",
      "l",
      "s",
      "k"
    ],
    "rejected_or_unpublished": null,
    "duplication_status": null,
    "paper_for_thesis": 0,
    "paper_available": 1,
    "behind_paywall": 0,
    "paywall_circumventable": 0,
    "author_names": [
      "Tom Schaul",
      "R&#x00E9;mi Munos",
      "Daniel J. Mankowitz",
      "Matteo Hessel",
      "John Quan",
      "David Silver",
      "Diana Borsa",
      "Andre Barreto",
      "Augustin &#x017D;&#x00ED;dek"
    ]
  },
  {
    "id": 1582256513,
    "title": "Task similarity measures for transfer in reinforcement learning task libraries",
    "abst": "Recent research in task transfer and task clustering has necessitated the need for task similarity measures in reinforcement learning. Determining task similarity is necessary for selective transfer where only information from relevant tasks and portions of a task are transferred. Which task similarity measure to use is not immediately obvious. It can be shown that no single task similarity measure is uniformly superior. The optimal task similarity measure is dependent upon the task transfer method being employed. We define similarity in terms of tasks, and propose several possible task similarity measures, d/sub T/, d/sub P/, d/sub Q/, and d/sub R/ which are based on the transfer time, policy overlap, Q-values, and reward structure respectively. We evaluate their performance in three separate experimental situations.",
    "url": "http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=1555955",
    "lang": "en",
    "authors": [
      459388977,
      2169019634
    ],
    "fos": [
      2776517306,
      119857082,
      73555534,
      175154964,
      3020089516,
      41008148,
      2993534362,
      154945302,
      97541855
    ],
    "journals": [],
    "conferences": [],
    "conference_series": [
      1140449422
    ],
    "references": [
      121337034,
      1499408472,
      1560550898,
      1566538838,
      2114451917,
      2122451452,
      2172405120
    ],
    "filter_matches": [
      "tr",
      "tl",
      "trl",
      "rl"
    ],
    "rank": 20830,
    "citation_count": 22,
    "estimated_citation_count": 22,
    "publication_date": "2005-12-27",
    "found_in": 1,
    "transfer_experiment_type": [
      "s_i",
      "s_f",
      "levels"
    ],
    "transfer_experiment_subtype": [
      "multi"
    ],
    "transfer_data_type": [
      "fea"
    ],
    "transfer_performance_metrics": [
      "ap"
    ],
    "implementation": [
      "Custom",
      "CS",
      "Figures",
      "Formulas",
      "Tables"
    ],
    "policy_type": [
      "Q"
    ],
    "task_mappings": [
      "N/A"
    ],
    "autonomous_transfer": 0,
    "is_deep_rl": 0,
    "tags": [
      "Simulation",
      "2D",
      "Navigation",
      "Grid",
      "MovingGoal"
    ],
    "allowed_learner": [
      "TD"
    ],
    "country": [
      "USA"
    ],
    "uni": [
      "Brigham Young University"
    ],
    "department": [
      "Computer Science"
    ],
    "source_task_selection": [
      "all"
    ],
    "was_in_survey": [
      "taylor",
      "mag"
    ],
    "in_title": [
      "t",
      "r",
      "l"
    ],
    "in_abs": [
      "t",
      "r",
      "l"
    ],
    "in_contet": [
      "t",
      "r",
      "l"
    ],
    "rejected_or_unpublished": null,
    "duplication_status": null,
    "paper_for_thesis": 0,
    "paper_available": 1,
    "behind_paywall": 0,
    "paywall_circumventable": 0,
    "author_names": [
      "Kevin D. Seppi",
      "James L. Carroll"
    ]
  },
  {
    "id": 2986946894,
    "title": "An Online Search Method for Representative Risky Fault Chains Based on Reinforcement Learning and Knowledge Transfer",
    "abst": "In the analysis of cascading outages and blackouts in power systems, risky cascading fault chains should be accurately identified in order to do further block or alleviate blackouts. However, the huge computational burden makes online analysis difficult. In this paper, an online search method for representative risky fault chains based on reinforcement learning and knowledge transfer is proposed. This method aims at promoting efficiency by exploiting similarities of adjacent power flow snapshots in operations. After the &#x201C;representative risky fault chain&#x201D; is defined, a framework of tree search based on Markov Decision Process and Q-learning is constructed. The knowledge in past runs is accumulated offline and then applied online, with a mechanism of knowledge transition and extension. The proposed learning based approach is verified on an illustrative 39-bus system with different loading levels, and simulations are carried out on a real-world 1000-bus power grid in China to show the effectiveness and efficiency of the proposed approach.",
    "url": "https://ieeexplore.ieee.org/document/8890733",
    "lang": "en",
    "authors": [
      2135109947,
      2136332006,
      2563957365,
      2607775157,
      2609058329,
      2674915252
    ],
    "fos": [
      171089853,
      2776960227,
      124101348,
      55282118,
      97541855,
      133731056,
      127413603,
      89227174,
      106189395,
      2987516962,
      38361682
    ],
    "journals": [
      201921491
    ],
    "conferences": [],
    "conference_series": [],
    "references": [
      768744778,
      1999071210,
      2120483528,
      2120747200,
      2121863487,
      2127208615,
      2142141971,
      2165698076,
      2175357651,
      2301816837,
      2343662311,
      2513580371,
      2519739684,
      2557921982,
      2559953650,
      2617631619,
      2752766402,
      2766447205,
      2770372302,
      2885838543
    ],
    "filter_matches": [
      "tr",
      "tl",
      "trl",
      "rl"
    ],
    "rank": 20833,
    "citation_count": 1,
    "estimated_citation_count": 1,
    "publication_date": "2020-05-01",
    "found_in": 1,
    "transfer_experiment_type": [
      "t"
    ],
    "transfer_experiment_subtype": [
      "multi"
    ],
    "transfer_data_type": [
      "Q"
    ],
    "transfer_performance_metrics": [
      "j",
      "tt",
      "tr",
      "ap"
    ],
    "implementation": [
      "Custom",
      "CS",
      "Formulas",
      "Pseudo",
      "Figures",
      "Tables"
    ],
    "policy_type": [
      "Q"
    ],
    "task_mappings": [
      "N/A"
    ],
    "autonomous_transfer": 0,
    "is_deep_rl": 0,
    "tags": [
      "Simulation",
      "RealWorld",
      "Power",
      "SmartEnergy",
      "Outage",
      "Prevention"
    ],
    "allowed_learner": [
      "TD"
    ],
    "country": [
      "China",
      "USA"
    ],
    "uni": [
      "Tsinghua University",
      "Argonne National Laboratory",
      "University of Tennessee"
    ],
    "department": [
      "Electrical Engineering",
      "Electrical Engineering and Computer Sciences"
    ],
    "source_task_selection": [
      "h"
    ],
    "was_in_survey": [
      "mag"
    ],
    "in_title": [
      "t",
      "r",
      "l",
      "k"
    ],
    "in_abs": [
      "t",
      "r",
      "l",
      "k"
    ],
    "in_contet": [
      "t",
      "r",
      "l",
      "k"
    ],
    "rejected_or_unpublished": null,
    "duplication_status": null,
    "paper_for_thesis": 0,
    "paper_available": 1,
    "behind_paywall": 1,
    "paywall_circumventable": 1,
    "author_names": [
      "Shaowei Huang",
      "Shengwei Mei",
      "Zhimei Zhang",
      "Ying Chen",
      "Kai Sun",
      "Rui Yao"
    ]
  },
  {
    "id": 2989824507,
    "title": "Deep Reinforcement Learning for Transfer of Control Policies",
    "abst": null,
    "url": "https://asmedigitalcollection.asme.org/IDETC-CIE/proceedings/IDETC-CIE2019/59186/V02AT03A003/1069764",
    "lang": "en",
    "authors": [
      2099240893,
      2121561944,
      2145891848,
      2305525080,
      2760933546
    ],
    "fos": [
      24326235,
      97541855,
      107457646,
      41008148,
      31531917
    ],
    "journals": [],
    "conferences": [
      2993381038
    ],
    "conference_series": [
      1187904452
    ],
    "references": [],
    "filter_matches": [
      "tr",
      "tl",
      "trl",
      "rl"
    ],
    "rank": 20839,
    "citation_count": 0,
    "estimated_citation_count": 0,
    "publication_date": "2019-11-25",
    "found_in": 1,
    "transfer_experiment_type": [],
    "transfer_experiment_subtype": [],
    "transfer_data_type": [],
    "transfer_performance_metrics": [],
    "implementation": [
      "Custom",
      "CS"
    ],
    "policy_type": [],
    "task_mappings": [],
    "autonomous_transfer": 2,
    "is_deep_rl": 2,
    "tags": [
      "Rotor",
      "Control",
      "Aircraft",
      "Simulation",
      "Aerial"
    ],
    "allowed_learner": [],
    "country": [
      "USA"
    ],
    "uni": [
      "Penn State University"
    ],
    "department": [],
    "source_task_selection": [],
    "was_in_survey": [
      "mag"
    ],
    "in_title": [
      "t",
      "r",
      "l"
    ],
    "in_abs": [
      "t",
      "r",
      "l",
      "k"
    ],
    "in_contet": [],
    "rejected_or_unpublished": null,
    "duplication_status": null,
    "paper_for_thesis": 0,
    "paper_available": 0,
    "behind_paywall": 1,
    "paywall_circumventable": 0,
    "author_names": [
      "Conrad S. Tucker",
      "Timothy W. Simpson",
      "Simon W. Miller",
      "Michael A. Yukish",
      "James Cunningham"
    ]
  },
  {
    "id": 2626333357,
    "title": "Energy saving in heterogeneous cellular network via transfer reinforcement learning based policy",
    "abst": "Energy efficient operation of heterogeneous networks (HetNets) has become extremely crucial owing to their fast increasing deployment. This work presents a novel approach in which an actor-critic (AC) reinforcement learning (RL) framework is used to enable traffic based ON/OFF switching of base stations (BSs) in a HetNet leading to a reduction in overall energy consumption. Further, previously estimated traffic statistics is exploited in future scenarios which speeds up the learning process and provide additional improvement in energy saving. The presented scheme leads to up to 82% drop in energy consumption which is a quite significant amount. Furthermore, the analysis of system delay and energy saving trade-off is done.",
    "url": "https://ieeexplore.ieee.org/document/7945411/",
    "lang": null,
    "authors": [
      2292648433,
      2303951183,
      2309477498
    ],
    "fos": [
      105339364,
      2983317576,
      2742236,
      158207573,
      2780165032,
      68649174,
      31258907,
      153646914,
      79403827,
      97541855,
      41008148
    ],
    "journals": [],
    "conferences": [
      2606595405
    ],
    "conference_series": [
      1164390529
    ],
    "references": [
      2084032658,
      2087179032,
      2163553782,
      2279554417
    ],
    "filter_matches": [
      "tr",
      "tl",
      "trl",
      "rl"
    ],
    "rank": 20909,
    "citation_count": 5,
    "estimated_citation_count": 5,
    "publication_date": "2017-01-01",
    "found_in": 1,
    "transfer_experiment_type": [
      "s_i"
    ],
    "transfer_experiment_subtype": [
      "same_all"
    ],
    "transfer_data_type": [
      "pi"
    ],
    "transfer_performance_metrics": [
      "ap",
      "tr"
    ],
    "implementation": [
      "Custom",
      "CS",
      "Formulas",
      "Figures"
    ],
    "policy_type": [
      "Actor-Critic"
    ],
    "task_mappings": [
      "N/A"
    ],
    "autonomous_transfer": 0,
    "is_deep_rl": 0,
    "tags": [
      "Energy Saving",
      "Network",
      "WiFi",
      "RealWorld",
      "Simulation"
    ],
    "allowed_learner": [
      "TD"
    ],
    "country": [
      "India"
    ],
    "uni": [
      "Indraprastha Institute of Information Technology"
    ],
    "department": [
      "Electronics and Communication Engineering"
    ],
    "source_task_selection": [
      "h"
    ],
    "was_in_survey": [
      "mag"
    ],
    "in_title": [
      "t",
      "r",
      "l"
    ],
    "in_abs": [
      "r",
      "l"
    ],
    "in_contet": [
      "t",
      "r",
      "l"
    ],
    "rejected_or_unpublished": null,
    "duplication_status": 1,
    "paper_for_thesis": 2604038904,
    "paper_available": 1,
    "behind_paywall": 1,
    "paywall_circumventable": 1,
    "author_names": [
      "Shreyata Sharma",
      "Anand Srivastava",
      "Sumit J. Darak"
    ]
  },
  {
    "id": 2607014226,
    "title": "Learning to Predict Consequences as a Method of Knowledge Transfer in Reinforcement Learning",
    "abst": "The reinforcement learning (RL) paradigm allows agents to solve tasks through trial-and-error learning. To be capable of efficient, long-term learning, RL agents should be able to apply knowledge gained in the past to new tasks they may encounter in the future. The ability to predict actions&#x2019; consequences may facilitate such knowledge transfer. We consider here domains where an RL agent has access to two kinds of information: agent-centric information with constant semantics across tasks, and environment-centric information, which is necessary to solve the task, but with semantics that differ between tasks. For example, in robot navigation, environment-centric information may include the robot&#x2019;s geographic location, while agent-centric information may include sensor readings of various nearby obstacles. We propose that these situations provide an opportunity for a very natural style of knowledge transfer, in which the agent learns to predict actions&#x2019; environmental consequences using agent-centric information. These predictions contain important information about the affordances and dangers present in a novel environment, and can effectively transfer knowledge from agent-centric to environment-centric learning systems. Using several example problems including spatial navigation and network routing, we show that our knowledge transfer approach can allow faster and lower cost learning than existing alternatives.",
    "url": "https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&amp;arnumber=7902152",
    "lang": "en",
    "authors": [
      1974739547,
      2064081269,
      2111718645,
      2569963025,
      2570238812
    ],
    "fos": [
      154945302,
      41008148,
      194995250,
      97541855,
      43729271,
      119857082,
      124246873,
      90509273,
      2776960227,
      2983435990,
      3019319123
    ],
    "journals": [
      42080949
    ],
    "conferences": [],
    "conference_series": [],
    "references": [
      99485931,
      309749295,
      1488730473,
      1491843047,
      1492014007,
      1503421559,
      1505937442,
      1554015367,
      1563109146,
      1977124459,
      1977655452,
      1981110271,
      1985161922,
      1986014385,
      1990654808,
      1993277309,
      2000784441,
      2007584825,
      2012204020,
      2015858749,
      2018480107,
      2020573190,
      2024637532,
      2026724928,
      2048226872,
      2050688638,
      2051850587,
      2055921164,
      2070678636,
      2082689007,
      2088862785,
      2091118421,
      2097381042,
      2097498341,
      2103263764,
      2105546430,
      2106953752,
      2116064496,
      2118656144,
      2120079537,
      2121863487,
      2130750514,
      2132622533,
      2134807560,
      2137004246,
      2139465937,
      2146881125,
      2159752377,
      2162206751,
      2170164558,
      2172158418,
      2179749721,
      2488247662,
      2489939061,
      2515676083,
      2517639096,
      2519881245,
      2554885577,
      2561161270,
      2573488511,
      2578066652,
      2903489506
    ],
    "filter_matches": [
      "tr",
      "tl",
      "trl",
      "rl"
    ],
    "rank": 20919,
    "citation_count": 7,
    "estimated_citation_count": 7,
    "publication_date": "2018-06-01",
    "found_in": 1,
    "transfer_experiment_type": [
      "s_i",
      "s_f",
      "levels"
    ],
    "transfer_experiment_subtype": [
      "multi"
    ],
    "transfer_data_type": [
      "fea",
      "Q",
      "predictions"
    ],
    "transfer_performance_metrics": [
      "ap",
      "tr",
      "tt"
    ],
    "implementation": [
      "CreateSim",
      "Custom",
      "CS",
      "Pseudo",
      "Formulas",
      "Figures"
    ],
    "policy_type": [
      "Q"
    ],
    "task_mappings": [
      "N/A"
    ],
    "autonomous_transfer": 0,
    "is_deep_rl": 0,
    "tags": [
      "2D",
      "Simulation",
      "Navigation",
      "Maze",
      "Grid"
    ],
    "allowed_learner": [
      "TD",
      "MB"
    ],
    "country": [
      "Canada"
    ],
    "uni": [
      "University of Lethbridge"
    ],
    "department": [
      "Department of Neuroscience"
    ],
    "source_task_selection": [
      "h",
      "lib"
    ],
    "was_in_survey": [
      "mag",
      "silva"
    ],
    "in_title": [
      "t",
      "r",
      "l",
      "k"
    ],
    "in_abs": [
      "t",
      "r",
      "l",
      "k"
    ],
    "in_contet": [
      "t",
      "r",
      "l",
      "k",
      "s"
    ],
    "rejected_or_unpublished": null,
    "duplication_status": null,
    "paper_for_thesis": 0,
    "paper_available": 1,
    "behind_paywall": 0,
    "paywall_circumventable": 0,
    "author_names": [
      "Artur Luczak",
      "Aaron J. Gruber",
      "Edgar Bermudez Contreras",
      "Brandon Robertson",
      "Eric Chalmers"
    ]
  },
  {
    "id": 2979766322,
    "title": "Federated Transfer Reinforcement Learning for Autonomous Driving.",
    "abst": "Reinforcement learning (RL) is widely used in autonomous driving tasks and training RL models typically involves in a multi-step process: pre-training RL models on simulators, uploading the pre-trained model to real-life robots, and fine-tuning the weight parameters on robot vehicles. This sequential process is extremely time-consuming and more importantly, knowledge from the fine-tuned model stays local and can not be re-used or leveraged collaboratively. To tackle this problem, we present an online federated RL transfer process for real-time knowledge extraction where all the participant agents make corresponding actions with the knowledge learned by others, even when they are acting in very different environments. To validate the effectiveness of the proposed approach, we constructed a real-life collision avoidance system with Microsoft Airsim simulator and NVIDIA JetsonTX2 car agents, which cooperatively learn from scratch to avoid collisions in indoor environment with obstacle objects. We demonstrate that with the proposed framework, the simulator car agents can transfer knowledge to the RC cars in real-time, with 27% increase in the average distance with obstacles and 42% decrease in the collision counts.",
    "url": "http://arxiv.org/pdf/1910.06001.pdf",
    "lang": "en",
    "authors": [
      2109031554,
      2720039956,
      2904733943,
      2979480297,
      2979869100
    ],
    "fos": [
      2776650193,
      121704057,
      90509273,
      154945302,
      97541855,
      2777016798,
      33923547,
      71901391,
      2781235140,
      119857082,
      120567893
    ],
    "journals": [
      2597173376
    ],
    "conferences": [],
    "conference_series": [],
    "references": [
      1195077797,
      2123514142,
      2165150801,
      2165698076,
      2167340365,
      2173248099,
      2174786457,
      2283463896,
      2418627435,
      2479680810,
      2541884796,
      2606508169,
      2797734773,
      2911450448,
      2912213068,
      2953431737,
      2962717849,
      2963821308,
      2968937098
    ],
    "filter_matches": [
      "tr",
      "tl",
      "trl",
      "rl"
    ],
    "rank": 20942,
    "citation_count": 1,
    "estimated_citation_count": 1,
    "publication_date": "2019-10-14",
    "found_in": 1,
    "transfer_experiment_type": [
      "t"
    ],
    "transfer_experiment_subtype": [
      "sim2real"
    ],
    "transfer_data_type": [
      "pi",
      "federated"
    ],
    "transfer_performance_metrics": [
      "ap",
      "tr"
    ],
    "implementation": [
      "Custom",
      "CS",
      "fakeOSS",
      "Stage",
      "AirSim",
      "JetsonTX2",
      "Figures",
      "Formulas",
      "Tables"
    ],
    "policy_type": [
      "DDPG"
    ],
    "task_mappings": [
      "N/A"
    ],
    "autonomous_transfer": 0,
    "is_deep_rl": 1,
    "tags": [
      "Simulation",
      "RealWorld",
      "RCCar",
      "Driving",
      "Autonomous",
      "Indoor",
      "Collision",
      "Avoidance",
      "MultiAgent"
    ],
    "allowed_learner": [
      "TD"
    ],
    "country": [
      "China"
    ],
    "uni": [
      "Hong Kong University of Science and Technology",
      "WeBank"
    ],
    "department": [
      "Industry",
      "Uni",
      "Colab",
      "Robotics and MultiPerception Laborotary",
      "Department of Artificial Intelligence"
    ],
    "source_task_selection": [
      "h"
    ],
    "was_in_survey": [
      "mag"
    ],
    "in_title": [
      "t",
      "r",
      "l"
    ],
    "in_abs": [
      "t",
      "r",
      "l",
      "k"
    ],
    "in_contet": [
      "t",
      "r",
      "l",
      "k"
    ],
    "rejected_or_unpublished": 2,
    "duplication_status": null,
    "paper_for_thesis": 0,
    "paper_available": 1,
    "behind_paywall": 0,
    "paywall_circumventable": 0,
    "author_names": [
      "Qiang Yang",
      "Yang Liu",
      "Tianjian Chen",
      "Xinle Liang",
      "Ming Liu"
    ]
  },
  {
    "id": 1510402218,
    "title": "Using Options for Knowledge Transfer in Reinforcement Learning",
    "abst": "One of the original motivations for the use of temporally extended actions, or options, in reinforcement learning was to enable t he transfer of learned value functions or policies to new problems. Many experimenters have used options to speed learning on single problems, but options have not been studied in depth as a tool for transfer. In this paper we introduce a formal model of a learning problem as a distribution of Markov Decision Problems (MDPs). Each MDP represents a task the agent will have to solve. Our model can also be viewed as a partially observable Markov decision problem (POMDP), with a special structure that we describe. We study two learning algorithms, one which keeps a single value function that generalizes across tasks, and an increm ental POMDPinspired method maintaining separate value functions for each task. We evaluate the learning algorithms on an extension of the Mountain Car domain, in terms of both learning speed and asymptotic performance. Empirically, we find that temporally extended options can fa cilitate transfer for both algorithms. In our domain, the single value func tion algorithm has much better learning speed because it generalizes its ex perience more broadly across tasks. We also observe that different sets of options can achieve tradeoffs of learning speed versus asymptotic perf ormance.",
    "url": "https://www.researchgate.net/profile/Doina_Precup/publication/2609073_Using_Options_for_Knowledge_Transfer_in_Reinforcement_Learning/links/0912f51191d83cea16000000.pdf",
    "lang": null,
    "authors": [
      2049838,
      2789685315
    ],
    "fos": [
      188116033,
      154945302,
      8038995,
      77075516,
      58973888,
      24138899,
      28006648,
      41008148,
      199190896,
      97541855
    ],
    "journals": [],
    "conferences": [],
    "conference_series": [],
    "references": [
      16046748,
      1488730473,
      1507087299,
      1568042657,
      1594201624,
      1595483645,
      1600813180,
      1631187438,
      1748123235,
      2102000945,
      2114451917,
      2153947321,
      2156067405,
      2158548602,
      2160371091,
      2165131254,
      2343637401
    ],
    "filter_matches": [
      "tr",
      "tl",
      "trl",
      "rl"
    ],
    "rank": 20975,
    "citation_count": 35,
    "estimated_citation_count": 64,
    "publication_date": "1999-01-01",
    "found_in": 1,
    "transfer_experiment_type": [
      "t"
    ],
    "transfer_experiment_subtype": [
      "multi"
    ],
    "transfer_data_type": [
      "options"
    ],
    "transfer_performance_metrics": [
      "tt"
    ],
    "implementation": [
      "Custom",
      "CS",
      "Formulas",
      "Figures"
    ],
    "policy_type": [
      "Q"
    ],
    "task_mappings": [
      "N/A"
    ],
    "autonomous_transfer": 0,
    "is_deep_rl": 0,
    "tags": [
      "MountainCar",
      "Simulation",
      "ClassicControl"
    ],
    "allowed_learner": [
      "TD",
      "H"
    ],
    "country": [
      "USA"
    ],
    "uni": [
      "University of Massachusetts"
    ],
    "department": [],
    "source_task_selection": [
      "all"
    ],
    "was_in_survey": [
      "taylor",
      "lazaric",
      "mag"
    ],
    "in_title": [
      "t",
      "r",
      "l"
    ],
    "in_abs": [
      "t",
      "r",
      "l"
    ],
    "in_contet": [
      "t",
      "r",
      "l"
    ],
    "rejected_or_unpublished": null,
    "duplication_status": null,
    "paper_for_thesis": 0,
    "paper_available": 1,
    "behind_paywall": 0,
    "paywall_circumventable": 0,
    "author_names": [
      "Doina Precup",
      "Theodore J. Perkins"
    ]
  },
  {
    "id": 2899403804,
    "title": "Design of Transfer Reinforcement Learning Under Low Task Similarity",
    "abst": null,
    "url": "http://proceedings.asmedigitalcollection.asme.org/proceeding.aspx?articleid=2713207",
    "lang": "en",
    "authors": [
      2225372412,
      2899151158
    ],
    "fos": [
      41008148,
      24326235,
      97541855,
      107457646
    ],
    "journals": [],
    "conferences": [
      2626656473
    ],
    "conference_series": [
      1187904452
    ],
    "references": [],
    "filter_matches": [
      "tr",
      "tl",
      "trl",
      "rl"
    ],
    "rank": 20988,
    "citation_count": 2,
    "estimated_citation_count": 2,
    "publication_date": "2018-08-26",
    "found_in": 1,
    "transfer_experiment_type": [
      "s",
      "levels"
    ],
    "transfer_experiment_subtype": [
      "multi"
    ],
    "transfer_data_type": [
      "advisor"
    ],
    "transfer_performance_metrics": [
      "tt"
    ],
    "implementation": [
      "Custom",
      "CS",
      "Formulas",
      "Figures",
      "TensorFlow",
      "Tables"
    ],
    "policy_type": [
      "DQN"
    ],
    "task_mappings": [
      "N/A"
    ],
    "autonomous_transfer": 0,
    "is_deep_rl": 1,
    "tags": [
      "2D",
      "Simulation",
      "Navigation",
      "Collision",
      "Avoidance"
    ],
    "allowed_learner": [
      "TD"
    ],
    "country": [
      "USA"
    ],
    "uni": [
      "University of Southern California"
    ],
    "department": [
      "Aerospace & Mechanical Engineering"
    ],
    "source_task_selection": [
      "h"
    ],
    "was_in_survey": [
      "mag"
    ],
    "in_title": [
      "t",
      "r",
      "l"
    ],
    "in_abs": [
      "t",
      "r",
      "l"
    ],
    "in_contet": [
      "t",
      "r",
      "l",
      "s",
      "k"
    ],
    "rejected_or_unpublished": null,
    "duplication_status": 5,
    "paper_for_thesis": 3010746488,
    "paper_available": 1,
    "behind_paywall": 0,
    "paywall_circumventable": 0,
    "author_names": [
      "Yan Jin",
      "Xiongqing Liu"
    ]
  },
  {
    "id": 1523133203,
    "title": "Enhancing Transfer in Reinforcement Learning by Building Stochastic Models of Robot Actions",
    "abst": "Abstract Recent work has shown that reinforcement learning is a viable method of automatically programming behavior-based robots. However, one weakness with this approach is that the learning typically does not transfer across tasks. Furthermore, there is unnecessary duplication of learning effort because information is not shared among the various behaviors. This paper describes an alternative technique based on action models that attempts to maximize transfer within and across tasks. Action models are inferred using a statistical clustering technique from instances generated by a robot exploring its task environment. Task-specific knowledge is encoded using a reward function for each subtask. A multi-step lookahead strategy using the reward functions as static evaluators is employed to select the most appropriate action. Experiments on simulated and real robots show that useful action models can be learned from a 12 by 12 scrolling certainty grid sensor. Furthermore, on the simulator these models are sufficiently rich to enable significant transfer within and across two tasks, box pushing and wall following.",
    "url": "https://dl.acm.org/citation.cfm?id=142042",
    "lang": "en",
    "authors": [
      2133778237
    ],
    "fos": [
      119857082,
      154945302,
      47932503,
      28006648,
      41008148,
      90509273,
      73555534,
      187691185,
      97541855,
      127491075,
      59046462
    ],
    "journals": [],
    "conferences": [
      2784746532
    ],
    "conference_series": [
      1180662882
    ],
    "references": [
      145683767,
      774668953,
      1253821906,
      1491843047,
      1500304516,
      1522632257,
      1529303739,
      1538393421,
      1567903305,
      1592402337,
      1611476298,
      1931792391,
      1971497957,
      1979071892,
      2020149918,
      2073308541,
      2104263168,
      2154418813,
      2161608691,
      2170641298,
      2180885055,
      3011120880
    ],
    "filter_matches": [
      "tr",
      "tl",
      "trl",
      "rl"
    ],
    "rank": 20992,
    "citation_count": 32,
    "estimated_citation_count": 32,
    "publication_date": "1992-07-07",
    "found_in": 1,
    "transfer_experiment_type": [
      "r"
    ],
    "transfer_experiment_subtype": [
      "multi"
    ],
    "transfer_data_type": [],
    "transfer_performance_metrics": [
      "j"
    ],
    "implementation": [
      "Custom",
      "CS"
    ],
    "policy_type": [
      "Q"
    ],
    "task_mappings": [],
    "autonomous_transfer": 2,
    "is_deep_rl": 2,
    "tags": [
      "2D",
      "Grid",
      "Navigation",
      "Robotics",
      "Simulation"
    ],
    "allowed_learner": [
      "TD"
    ],
    "country": [
      "USA"
    ],
    "uni": [
      "IBM"
    ],
    "department": [
      "Industry"
    ],
    "source_task_selection": [],
    "was_in_survey": [
      "mag"
    ],
    "in_title": [
      "t",
      "r",
      "l"
    ],
    "in_abs": [],
    "in_contet": [],
    "rejected_or_unpublished": null,
    "duplication_status": null,
    "paper_for_thesis": 0,
    "paper_available": 0,
    "behind_paywall": 1,
    "paywall_circumventable": 0,
    "author_names": [
      "Sridhar Mahadevan"
    ]
  },
  {
    "id": 2110064866,
    "title": "Transferring Expectations in Model-based Reinforcement Learning",
    "abst": "We study how to automatically select and adapt multiple abstractions or representations of the world to support model-based reinforcement learning. We address the challenges of transfer learning in heterogeneous environments with varying tasks. We present an efficient, online framework that, through a sequence of tasks, learns a set of relevant representations to be used in future tasks. Without predefined mapping strategies, we introduce a general approach to support transfer learning across different state spaces. We demonstrate the potential impact of our system through improved jumpstart and faster convergence to near optimum policy in two benchmark domains.",
    "url": "https://www.researchgate.net/profile/Tze-Yun_Leong/publication/264503330_Transferring_Expectations_in_Model-based_Reinforcement_Learning/links/53e1f85d0cf2d79877a9f9b4.pdf?origin=publication_detail",
    "lang": "en",
    "authors": [
      2083597622,
      2892582062,
      2974191546
    ],
    "fos": [
      47932503,
      119857082,
      3018835662,
      71923881,
      124304363,
      41008148,
      199190896,
      97541855,
      28006648,
      150899416,
      154945302
    ],
    "journals": [],
    "conferences": [
      105510575
    ],
    "conference_series": [
      1127325140
    ],
    "references": [
      36691172,
      64187236,
      179182636,
      183249136,
      193076044,
      1505937442,
      1515851193,
      1537180453,
      1537435730,
      1678414046,
      1854866626,
      1997477668,
      1998274472,
      2056584142,
      2096199223,
      2097381042,
      2110292307,
      2115524942,
      2117629901,
      2121863487,
      2123043338,
      2138019504,
      2165792602,
      2166265228,
      2166798247,
      2168342951,
      2169743339,
      2205628031,
      2796293253
    ],
    "filter_matches": [
      "tr",
      "tl",
      "trl",
      "rl"
    ],
    "rank": 21012,
    "citation_count": 8,
    "estimated_citation_count": 8,
    "publication_date": "2012-12-03",
    "found_in": 1,
    "transfer_experiment_type": [
      "s_f",
      "t"
    ],
    "transfer_experiment_subtype": [
      "multi"
    ],
    "transfer_data_type": [
      "model"
    ],
    "transfer_performance_metrics": [
      "tr",
      "tt"
    ],
    "implementation": [
      "Custom",
      "CS",
      "Formulas",
      "Pseudo",
      "Figures",
      "Tables"
    ],
    "policy_type": [
      "DBN"
    ],
    "task_mappings": [
      "N/A"
    ],
    "autonomous_transfer": 0,
    "is_deep_rl": 1,
    "tags": [
      "Simulation",
      "Expectations",
      "Views",
      "2D",
      "Grid",
      "Navigation",
      "DynamicBayesianNetwork"
    ],
    "allowed_learner": [
      "TD",
      "H",
      "B",
      "MB"
    ],
    "country": [
      "Singapore"
    ],
    "uni": [
      "National University of Singapore"
    ],
    "department": [
      "School of Computing"
    ],
    "source_task_selection": [
      "lib"
    ],
    "was_in_survey": [
      "mag"
    ],
    "in_title": [
      "t",
      "r",
      "l"
    ],
    "in_abs": [
      "t",
      "r",
      "l"
    ],
    "in_contet": [
      "t",
      "r",
      "l",
      "s",
      "k"
    ],
    "rejected_or_unpublished": null,
    "duplication_status": null,
    "paper_for_thesis": 0,
    "paper_available": 1,
    "behind_paywall": 0,
    "paywall_circumventable": 0,
    "author_names": [
      "Tomi Silander",
      "Trung Thanh Nguyen",
      "Tze Y. Leong"
    ]
  },
  {
    "id": 1563109146,
    "title": "Shaping in reinforcement learning via knowledge transferred from human-demonstrations",
    "abst": "Transfer has been widely used to ameliorate the slow convergence speed of reinforcement learning (RL) by reusing the previous obtained knowledge from other related but distinct tasks. In this paper, we propose a framework to transfer knowledge learned directly from human-demonstration trajectories of source tasks to shape the RL algorithm in target task, so as to avoid the time-consuming training process of RL in source tasks and thus we expand the learning paradigm of transfer in RL domains. In our framework, rather than transferring the most common value function or policy, we adopt the visit frequencies of states in successful demonstration trajectories as the acquired knowledge, and then perform transfer via shared agent space. Simulation experiments in obstacle avoidance problems suggest that the transferred knowledge could accelerate the learning process in target task obviously. And as a case study, the experiments show the potential of our framework in knowledge transfer in RL tasks.",
    "url": "https://ieeexplore.ieee.org/document/7260106/",
    "lang": "en",
    "authors": [
      2436685963,
      2443899449,
      2577016052,
      3019767151
    ],
    "fos": [
      199190896,
      2776960227,
      206588197,
      97541855,
      77075516,
      163239763,
      41008148,
      154945302,
      6683253,
      28006648
    ],
    "journals": [],
    "conferences": [
      2625009782
    ],
    "conference_series": [
      2623792093
    ],
    "references": [
      142858861,
      1515851193,
      1540462738,
      1569756368,
      1626155273,
      1777239053,
      1986014385,
      2004030284,
      2031727428,
      2046376809,
      2061562262,
      2079247031,
      2094387729,
      2097381042,
      2097498341,
      2114580749,
      2129427976,
      2134491302,
      2143435603,
      2165698076,
      2169209873,
      2262174858,
      2524747780
    ],
    "filter_matches": [
      "tr",
      "tl",
      "trl",
      "rl"
    ],
    "rank": 21015,
    "citation_count": 7,
    "estimated_citation_count": 7,
    "publication_date": "2015-07-28",
    "found_in": 1,
    "transfer_experiment_type": [
      "t",
      "v",
      "a"
    ],
    "transfer_experiment_subtype": [
      "multi"
    ],
    "transfer_data_type": [
      "r",
      "rule"
    ],
    "transfer_performance_metrics": [
      "j",
      "tt"
    ],
    "implementation": [
      "Custom",
      "CS",
      "Formulas",
      "Figures"
    ],
    "policy_type": [
      "Q"
    ],
    "task_mappings": [
      "N/A"
    ],
    "autonomous_transfer": 0,
    "is_deep_rl": 0,
    "tags": [
      "2D",
      "3D",
      "2Dto3D",
      "MountainCar",
      "MountainCar3D",
      "Simulation",
      "ClassicControl"
    ],
    "allowed_learner": [
      "TD"
    ],
    "country": [
      "China"
    ],
    "uni": [
      "Zhejiang University"
    ],
    "department": [
      "Aeronautics and Astronautic"
    ],
    "source_task_selection": [
      "h"
    ],
    "was_in_survey": [
      "mag"
    ],
    "in_title": [
      "t",
      "r",
      "l",
      "k"
    ],
    "in_abs": [
      "t",
      "r",
      "l",
      "k"
    ],
    "in_contet": [
      "t",
      "r",
      "l",
      "k",
      "s"
    ],
    "rejected_or_unpublished": null,
    "duplication_status": null,
    "paper_for_thesis": 0,
    "paper_available": 1,
    "behind_paywall": 1,
    "paywall_circumventable": 1,
    "author_names": [
      "Fang Zhou",
      "Li Bo",
      "Wang Guo-fang",
      "Li Ping"
    ]
  },
  {
    "id": 2896329788,
    "title": "Missile aerodynamic design using reinforcement learning and transfer learning",
    "abst": null,
    "url": "http://engine.scichina.com/publisher/scp/journal/SCIS/61/11/10.1007/s11432-018-9463-x?slug=fulltext",
    "lang": "zh_chs",
    "authors": [
      2135412054,
      2895989157,
      2896214358,
      2897097551
    ],
    "fos": [
      3019438201,
      1633027,
      97541855,
      65244806,
      2778857364,
      33923547,
      150899416,
      13393347
    ],
    "journals": [
      91852382
    ],
    "conferences": [],
    "conference_series": [],
    "references": [
      2007837190,
      2010038479,
      2161381512,
      2173248099,
      2211853781,
      2526258257,
      2585298970
    ],
    "filter_matches": [
      "tr",
      "tl",
      "trl",
      "rl"
    ],
    "rank": 21018,
    "citation_count": 2,
    "estimated_citation_count": 2,
    "publication_date": "2018-09-17",
    "found_in": 1,
    "transfer_experiment_type": [
      "t"
    ],
    "transfer_experiment_subtype": [
      "same_all"
    ],
    "transfer_data_type": [
      "fea"
    ],
    "transfer_performance_metrics": [
      "j",
      "tr",
      "tt"
    ],
    "implementation": [
      "Custom",
      "CS",
      "Formulas",
      "Figures"
    ],
    "policy_type": [
      "DDPG",
      "SL-DDPG"
    ],
    "task_mappings": [],
    "autonomous_transfer": 0,
    "is_deep_rl": 1,
    "tags": [
      "LiftDragRatio",
      "Simulation",
      "DDPG",
      "Aerodynamic",
      "Design",
      "Military",
      "Weapons"
    ],
    "allowed_learner": [
      "TD"
    ],
    "country": [
      "China"
    ],
    "uni": [
      "Tsinghua University"
    ],
    "department": [
      "Computer Science and Technology"
    ],
    "source_task_selection": [
      "h"
    ],
    "was_in_survey": [
      "mag"
    ],
    "in_title": [
      "t",
      "r",
      "l"
    ],
    "in_abs": [
      "t",
      "r",
      "l"
    ],
    "in_contet": [
      "t",
      "r",
      "l",
      "k"
    ],
    "rejected_or_unpublished": null,
    "duplication_status": null,
    "paper_for_thesis": 0,
    "paper_available": 1,
    "behind_paywall": 1,
    "paywall_circumventable": 1,
    "author_names": [
      "Jihong Zhu",
      "Xiangyang Wang",
      "Minchi Kuang",
      "Xinghui Yan"
    ]
  },
  {
    "id": 2792514232,
    "title": "Flexible Robotic Grasping with Sim-to-Real Transfer based Reinforcement Learning.",
    "abst": "Robotic manipulation requires a highly flexible and compliant system. Task-specific heuristics are usually not able to cope with the diversity of the world outside of specific assembly lines and cannot generalize well. Reinforcement learning methods provide a way to cope with uncertainty and allow robots to explore their action space to solve specific tasks. However, this comes at a cost of high training times, sparse and therefore hard to sample useful actions, strong local minima, etc. In this paper we show a real robotic system, trained in simulation on a pick and lift task, that is able to cope with different objects. We introduce an adaptive learning mechanism that allows the algorithm to find feasible solutions even for tasks that would otherwise be intractable. Furthermore, in order to improve the performance on difficult objects, we use a prioritized sampling scheme. We validate the efficacy of our approach with a real robot in a pick and lift task of different objects.",
    "url": "http://hdl.handle.net/20.500.11850/322242",
    "lang": "en",
    "authors": [
      1988215978,
      2429964634,
      2722818964,
      2790558001,
      2949969410
    ],
    "fos": [
      2985139394,
      186633575,
      2780061701,
      97541855,
      2982816539,
      90509273,
      127705205,
      41008148,
      154945302,
      125014702
    ],
    "journals": [],
    "conferences": [],
    "conference_series": [],
    "references": [],
    "filter_matches": [
      "tr",
      "tl",
      "trl",
      "rl"
    ],
    "rank": 21052,
    "citation_count": 3,
    "estimated_citation_count": 3,
    "publication_date": "2018-03-13",
    "found_in": 1,
    "transfer_experiment_type": [
      "t"
    ],
    "transfer_experiment_subtype": [
      "pure",
      "sim2real"
    ],
    "transfer_data_type": [
      "pi"
    ],
    "transfer_performance_metrics": [
      "j"
    ],
    "implementation": [
      "Custom",
      "CS",
      "Formulas",
      "rllab",
      "PyBullet",
      "Figures",
      "Tables"
    ],
    "policy_type": [
      "TRPO"
    ],
    "task_mappings": [],
    "autonomous_transfer": 0,
    "is_deep_rl": 1,
    "tags": [
      "3D",
      "CNN",
      "DepthCamera",
      "Robotics",
      "Grasping",
      "RealWorld",
      "Simulation"
    ],
    "allowed_learner": [
      "TD"
    ],
    "country": [
      "Switzerland"
    ],
    "uni": [
      "Eidgenssische Technische Hochschule Zrich"
    ],
    "department": [
      "Autonomous Systems Lab"
    ],
    "source_task_selection": [
      "h"
    ],
    "was_in_survey": [
      "mag"
    ],
    "in_title": [
      "t",
      "r",
      "l"
    ],
    "in_abs": [
      "r",
      "l"
    ],
    "in_contet": [
      "t",
      "r",
      "l",
      "s",
      "k"
    ],
    "rejected_or_unpublished": null,
    "duplication_status": null,
    "paper_for_thesis": 0,
    "paper_available": 1,
    "behind_paywall": 0,
    "paywall_circumventable": 0,
    "author_names": [
      "Roland Siegwart",
      "Fadri Furrer",
      "Tonci Novkovic",
      "Michel Breyer",
      "Juan Nieto"
    ]
  },
  {
    "id": 2039109166,
    "title": "Rule abstraction and transfer in reinforcement learning by decision tree",
    "abst": "Reinforcement learning agents store their knowledge such as state-action value in look-up tables. However, loop-up table requires large memory space when number of states become large. Learning from look-up table is tabularasa therefore is very slow. To overcome this disadvantage, generalization methods are used to abstract knowledge. In this paper, decision tree technology is used to enable the agent to represent abstract knowledge in rule from during learning progress and form rule base for each individual task.",
    "url": "http://www.robot.t.u-tokyo.ac.jp/~yamashita/paper/B/B085Final.pdf",
    "lang": null,
    "authors": [
      62261399,
      2165633131,
      2581969916
    ],
    "fos": [
      97541855,
      41008148,
      154945302,
      10229987,
      141291300,
      119857082,
      183931066,
      199190896,
      172967692,
      24138899,
      5481197
    ],
    "journals": [],
    "conferences": [],
    "conference_series": [
      2624205572
    ],
    "references": [
      190675879,
      1555801537,
      1783866091,
      2097381042,
      2119475352,
      2121863487,
      2126357802,
      2149706766,
      2153947321,
      2158316397,
      2952485636,
      3020831056
    ],
    "filter_matches": [
      "tr",
      "tl",
      "trl",
      "rl"
    ],
    "rank": 21080,
    "citation_count": 3,
    "estimated_citation_count": 3,
    "publication_date": "2012-12-01",
    "found_in": 1,
    "transfer_experiment_type": [
      "s_i",
      "s_f",
      "levels"
    ],
    "transfer_experiment_subtype": [
      "multi"
    ],
    "transfer_data_type": [
      "rule"
    ],
    "transfer_performance_metrics": [
      "j",
      "tt"
    ],
    "implementation": [
      "Custom",
      "CS",
      "Formulas",
      "Figures",
      "Pseudo",
      "Tables"
    ],
    "policy_type": [
      "Q"
    ],
    "task_mappings": [
      "N/A"
    ],
    "autonomous_transfer": 0,
    "is_deep_rl": 0,
    "tags": [
      "2D",
      "Navigation",
      "Simulation",
      "Obstacle",
      "CollisionAvoidance"
    ],
    "allowed_learner": [
      "TD",
      "RRL"
    ],
    "country": [
      "Japan"
    ],
    "uni": [
      "University of Tokyo"
    ],
    "department": [
      "Precision Engineering"
    ],
    "source_task_selection": [
      "h"
    ],
    "was_in_survey": [
      "mag"
    ],
    "in_title": [
      "t",
      "r",
      "l"
    ],
    "in_abs": [
      "r",
      "l",
      "k"
    ],
    "in_contet": [
      "t",
      "r",
      "l",
      "k"
    ],
    "rejected_or_unpublished": null,
    "duplication_status": null,
    "paper_for_thesis": 0,
    "paper_available": 1,
    "behind_paywall": 0,
    "paywall_circumventable": 0,
    "author_names": [
      "Hajime Asama",
      "Atsushi Yamashita",
      "Min Wu"
    ]
  },
  {
    "id": 2995352224,
    "title": "Single episode transfer for differing environmental dynamics in reinforcement learning",
    "abst": "Transfer and adaptation to new unknown environmental dynamics is a key challenge for reinforcement learning (RL). An even greater challenge is performing near-optimally in a single attempt at test time, possibly without access to dense rewards, which is not addressed by current methods that require multiple experience rollouts for adaptation. To achieve single episode transfer in a family of environments with related dynamics, we propose a general algorithm that optimizes a probe and an inference model to rapidly estimate underlying latent variables of test dynamics, which are then immediately used as input to a universal control policy. This modular approach enables integration of state-of-the-art algorithms for variational inference or RL. Moreover, our approach does not require access to rewards at test time, allowing it to perform in settings where existing adaptive approaches cannot. In diverse experimental domains with a single episode test constraint, our method significantly outperforms existing adaptive approaches and shows favorable performance against baselines for robust transfer.",
    "url": "https://openreview.net/pdf?id=rJeQoCNYDS",
    "lang": null,
    "authors": [
      2099091510,
      2636290417,
      2725665757,
      2788100770
    ],
    "fos": [
      154945302,
      150899416,
      97541855,
      2910112148,
      51167844,
      41008148,
      2993214414,
      119857082,
      101468663,
      109364899,
      2776214188
    ],
    "journals": [],
    "conferences": [
      2965267707
    ],
    "conference_series": [
      2584161585
    ],
    "references": [
      567721252,
      1487586669,
      1860450641,
      1959608418,
      2063280109,
      2097381042,
      2119717200,
      2121863487,
      2132514348,
      2133853511,
      2145339207,
      2155968351,
      2168359464,
      2333635821,
      2604763608,
      2736601468,
      2753738274,
      2770572337,
      2783793006,
      2786438063,
      2799245749,
      2799720196,
      2889893435,
      2890260778,
      2901168740,
      2903077804,
      2905438253,
      2912926578,
      2914752403,
      2962818798,
      2963344681,
      2963412383,
      2963459627,
      2963477884,
      2963614114,
      2964112702,
      2964173023,
      2968021416
    ],
    "filter_matches": [
      "tr",
      "tl",
      "trl",
      "rl"
    ],
    "rank": 21143,
    "citation_count": 0,
    "estimated_citation_count": 0,
    "publication_date": "2020-04-30",
    "found_in": 1,
    "transfer_experiment_type": [
      "t"
    ],
    "transfer_experiment_subtype": [
      "same_all"
    ],
    "transfer_data_type": [
      "pi",
      "VAE"
    ],
    "transfer_performance_metrics": [
      "tt",
      "tr"
    ],
    "implementation": [
      "OSS",
      "TensorFlow",
      "Pseudo",
      "Formulas",
      "Figures"
    ],
    "policy_type": [
      "DDQN",
      "SEPT"
    ],
    "task_mappings": [
      "exp"
    ],
    "autonomous_transfer": 0,
    "is_deep_rl": 1,
    "tags": [
      "2D",
      "Simulation",
      "Navigation",
      "Grid",
      "Acrobot",
      "HIV",
      "SEPT",
      "ClassicControl"
    ],
    "allowed_learner": [
      "TD"
    ],
    "country": [
      "USA"
    ],
    "uni": [
      "Georgia Institute of Technology",
      "Lawrence Livermore National Laboratory"
    ],
    "department": [
      "Computer Science"
    ],
    "source_task_selection": [
      "lib"
    ],
    "was_in_survey": [
      "mag"
    ],
    "in_title": [
      "t",
      "r",
      "l"
    ],
    "in_abs": [
      "t",
      "r",
      "l"
    ],
    "in_contet": [
      "t",
      "r",
      "l",
      "s",
      "k"
    ],
    "rejected_or_unpublished": null,
    "duplication_status": null,
    "paper_for_thesis": 0,
    "paper_available": 1,
    "behind_paywall": 0,
    "paywall_circumventable": 0,
    "author_names": [
      "Hongyuan Zha",
      "Jiachen Yang",
      "Brenden K. Petersen",
      "Daniel Faissol"
    ]
  },
  {
    "id": 2515676083,
    "title": "Transferring knowledge from human-demonstration trajectories to reinforcement learning",
    "abst": "Nowadays, transfer learning (TL) has become a crucial technique to accelerate the slow optimization procedure of reinforcement learning (RL) by re-utilizing knowledge acquired in a previous related task. Nevertheless, most of the current relevant research acquires knowledge through RL training in the source task, which would be too time-consuming. In view of this situation, in this paper, we propose a novel TL framework where the agent extracts knowledge from human-demonstration trajectories of the source task and reuses the knowledge in RL in the target task. As for what to transfer, two forms of knowledge deduced from the demonstration trajectories, which are the k-nearest neighbour of the current state in source samples and visit frequency of homologous states, are adopted. For how to transfer, the two forms of knowledge are respectively used to recommend a preferred action when random exploration is needed and to shape an instantaneous reward for RL. Simulation experiments of balancing Cart-Poles with...",
    "url": "http://journals.sagepub.com/doi/pdf/10.1177/0142331216649655",
    "lang": "en",
    "authors": [
      2126098870,
      2480995612,
      2535363395,
      2628511581
    ],
    "fos": [
      41008148,
      119857082,
      3020024350,
      150899416,
      97541855,
      154945302
    ],
    "journals": [
      24148485
    ],
    "conferences": [],
    "conference_series": [],
    "references": [
      328766226,
      1494902192,
      1506146479,
      1514621373,
      1515851193,
      1563109146,
      1626155273,
      1777239053,
      1949804828,
      1977655452,
      1980620643,
      1986014385,
      2004030284,
      2021055657,
      2031727428,
      2097381042,
      2097498341,
      2113921460,
      2114580749,
      2129427976,
      2130005627,
      2137375617,
      2143435603,
      2165698076,
      2172968643,
      2262174858,
      2397581010
    ],
    "filter_matches": [
      "tr",
      "tl",
      "trl",
      "rl"
    ],
    "rank": 21166,
    "citation_count": 4,
    "estimated_citation_count": 4,
    "publication_date": "2018-01-01",
    "found_in": 1,
    "transfer_experiment_type": [
      "t"
    ],
    "transfer_experiment_subtype": [
      "same_all"
    ],
    "transfer_data_type": [
      "r",
      "KNN"
    ],
    "transfer_performance_metrics": [
      "j",
      "ap",
      "tr",
      "tt"
    ],
    "implementation": [
      "Custom",
      "CS",
      "Pseudo",
      "Formulas",
      "Figures"
    ],
    "policy_type": [
      "LSPI",
      "Q"
    ],
    "task_mappings": [
      "N/A"
    ],
    "autonomous_transfer": 0,
    "is_deep_rl": 0,
    "tags": [
      "HumanDemonstration",
      "KNN",
      "Frequency",
      "CartPole",
      "Simulation",
      "2D",
      "ClassicControl"
    ],
    "allowed_learner": [
      "TD",
      "batch"
    ],
    "country": [
      "China"
    ],
    "uni": [
      "Zheijang University"
    ],
    "department": [
      "Aeronautics and Astronautics",
      "Control Science and Engineering"
    ],
    "source_task_selection": [
      "h"
    ],
    "was_in_survey": [
      "mag"
    ],
    "in_title": [
      "t",
      "r",
      "l",
      "k"
    ],
    "in_abs": [
      "t",
      "r",
      "l",
      "k"
    ],
    "in_contet": [
      "t",
      "r",
      "l",
      "k",
      "s"
    ],
    "rejected_or_unpublished": null,
    "duplication_status": null,
    "paper_for_thesis": 0,
    "paper_available": 1,
    "behind_paywall": 1,
    "paywall_circumventable": 1,
    "author_names": [
      "Zhou Fang",
      "Guo-fang Wang",
      "Bo Li",
      "Ping Li"
    ]
  },
  {
    "id": 115717799,
    "title": "Transfer learning for reinforcement learning through goal and policy parametrization",
    "abst": "Relational reinforcement learning has allowed results from reinforcement learning tasks to be re-used in other, closely related, tasks. This transfer of knowledge is made possible by the use of parameters in the representations of the task-description and the learned policy. In this paper, we will give a description of the current state of the art of transfer learning with relational reinforcement learning, make some observations about the usefulness and limitations of this current state and discuss some directions for future research. We also present a first small step along one of those directions.",
    "url": "https://lirias.kuleuven.be/bitstream/123456789/131381/1/2006_wtl_driessens.pdf",
    "lang": null,
    "authors": [
      1982051687,
      2016312802,
      2168801554
    ],
    "fos": [
      47932503,
      119857082,
      106199856,
      12298181,
      154945302,
      41008148,
      97541855,
      178980831,
      150899416,
      183759332
    ],
    "journals": [],
    "conferences": [
      2784769503
    ],
    "conference_series": [
      1180662882
    ],
    "references": [
      1564663916,
      2109722166,
      2109910161,
      2124125910,
      2128547596,
      2148411640,
      2152669282,
      2154441654,
      2396715201,
      3020831056
    ],
    "filter_matches": [
      "tr",
      "tl",
      "trl",
      "rl"
    ],
    "rank": 21203,
    "citation_count": 10,
    "estimated_citation_count": 10,
    "publication_date": "2006-01-01",
    "found_in": 1,
    "transfer_experiment_type": [],
    "transfer_experiment_subtype": [
      "theory"
    ],
    "transfer_data_type": [],
    "transfer_performance_metrics": [],
    "implementation": [],
    "policy_type": [],
    "task_mappings": [],
    "autonomous_transfer": 2,
    "is_deep_rl": 2,
    "tags": [
      "Report",
      "State-of-the-Art"
    ],
    "allowed_learner": [],
    "country": [
      "Belgium"
    ],
    "uni": [
      "Katholieke Universiteit Leuven"
    ],
    "department": [
      "Computer Science"
    ],
    "source_task_selection": [],
    "was_in_survey": [
      "mag"
    ],
    "in_title": [
      "t",
      "r",
      "l"
    ],
    "in_abs": [
      "t",
      "r",
      "l",
      "k"
    ],
    "in_contet": [
      "t",
      "r",
      "l",
      "s",
      "k"
    ],
    "rejected_or_unpublished": null,
    "duplication_status": null,
    "paper_for_thesis": 0,
    "paper_available": 1,
    "behind_paywall": 0,
    "paywall_circumventable": 0,
    "author_names": [
      "Kurt Driessens",
      "Tom Croonenborghs",
      "Jan Ramon"
    ]
  },
  {
    "id": 2803180393,
    "title": "Importance Weighted Transfer of Samples in Reinforcement Learning",
    "abst": "We consider the transfer of experience samples (i.e., tuples ) in reinforcement learning (RL), collected from a set of source tasks to improve the learning process in a given target task. Most of the related approaches focus on selecting the most relevant source samples for solving the target task, but then all the transferred samples are used without considering anymore the discrepancies between the task models. In this paper, we propose a model-based technique that automatically estimates the relevance (importance weight) of each source sample for solving the target task. In the proposed approach, all the samples are transferred and used by a batch RL algorithm to solve the target task, but their contribution to the learning process is proportional to their importance weight. By extending the results for importance weighting provided in supervised learning literature, we develop a finite-sample analysis of the proposed batch RL algorithm. Furthermore, we empirically compare the proposed algorithm to state-of-the-art approaches, showing that it achieves better learning performance and is very robust to negative transfer, even when some source tasks are significantly different from the target task.",
    "url": "http://export.arxiv.org/abs/1805.10886",
    "lang": "en",
    "authors": [
      2043994882,
      2103877068,
      2803494408,
      2803636064
    ],
    "fos": [
      154945302,
      2779178101,
      97541855,
      119857082,
      33923547,
      2911072482,
      118930307,
      136389625,
      183115368
    ],
    "journals": [
      2597173376
    ],
    "conferences": [],
    "conference_series": [],
    "references": [
      91088564,
      158722652,
      1508786153,
      1746819321,
      1830873897,
      2004030284,
      2097381042,
      2110292307,
      2111355007,
      2119567691,
      2120346334,
      2124175081,
      2148440006,
      2151268438,
      2252888482,
      2460675832,
      2605369401,
      2952905979,
      2962818798
    ],
    "filter_matches": [
      "tr",
      "tl",
      "trl",
      "rl"
    ],
    "rank": 21216,
    "citation_count": 0,
    "estimated_citation_count": 0,
    "publication_date": "2018-05-28",
    "found_in": 1,
    "transfer_experiment_type": [
      "t",
      "s_f"
    ],
    "transfer_experiment_subtype": [
      "multi"
    ],
    "transfer_data_type": [
      "pi"
    ],
    "transfer_performance_metrics": [
      "j",
      "tr",
      "tt"
    ],
    "implementation": [
      "Custom",
      "CS",
      "Formulas",
      "Pseudo",
      "Theorem",
      "Lemma",
      "Tables"
    ],
    "policy_type": [
      "FQI",
      "RBT",
      "SDT",
      "IWFQI"
    ],
    "task_mappings": [
      "exp"
    ],
    "autonomous_transfer": 1,
    "is_deep_rl": 0,
    "tags": [
      "Simulation",
      "Puddleworld",
      "WaterReservoirControl",
      "Acrobot",
      "ClassicControl"
    ],
    "allowed_learner": [
      "TD",
      "MB",
      "batch"
    ],
    "country": [
      "Italy",
      "France"
    ],
    "uni": [
      "Politecnico di Milano",
      "INRIA Lille"
    ],
    "department": [
      "Computer Science"
    ],
    "source_task_selection": [
      "h"
    ],
    "was_in_survey": [
      "mag"
    ],
    "in_title": [
      "t",
      "r",
      "l"
    ],
    "in_abs": [
      "t",
      "r",
      "l"
    ],
    "in_contet": [
      "t",
      "r",
      "l",
      "k"
    ],
    "rejected_or_unpublished": null,
    "duplication_status": null,
    "paper_for_thesis": 0,
    "paper_available": 1,
    "behind_paywall": 0,
    "paywall_circumventable": 0,
    "author_names": [
      "Matteo Pirotta",
      "Marcello Restelli",
      "Andrea Sessa",
      "Andrea Tirinzoni"
    ]
  },
  {
    "id": 2910593956,
    "title": "Design of Transfer Reinforcement Learning Mechanisms for Autonomous Collision Avoidance",
    "abst": "It is often hard for a reinforcement learning (RL) agent to utilize previous experience to solve new similar but more complex tasks. In this research, we combine the transfer learning with reinforcement learning and investigate how the hyperparameters of both transfer learning and reinforcement learning impact the learning effectiveness and task performance in the context of autonomous robotic collision avoidance. A deep reinforcement learning algorithm was first implemented for a robot to learn, from its experience, how to avoid randomly generated single obstacles. After that the effect of transfer of previously learned experience was studied by introducing two important concepts, transfer belief&#x2014;i.e., how much a robot should believe in its previous experience&#x2014;and transfer period&#x2014;i.e., how long the previous experience should be applied in the new context. The proposed approach has been tested for collision avoidance problems by altering transfer period. It is shown that transfer learnings on average had ~50% speed increase at ~30% competence levels, and there exists an optimal transfer period where the variance is the lowest and learning speed is the fastest.",
    "url": "https://link.springer.com/chapter/10.1007/978-3-030-05363-5_17",
    "lang": "en",
    "authors": [
      2225372412,
      2899151158
    ],
    "fos": [
      154945302,
      150899416,
      90509273,
      97541855,
      2984650650,
      121704057,
      41008148,
      8642999
    ],
    "journals": [],
    "conferences": [],
    "conference_series": [],
    "references": [
      1506146479,
      1507591516,
      1589549454,
      1757796397,
      1821462560,
      2017325967,
      2024388749,
      2031727428,
      2059652044,
      2076450475,
      2079381666,
      2088173505,
      2089080976,
      2103120971,
      2108826648,
      2160815625,
      2162390675,
      2164114810,
      2164284962,
      2165698076,
      2168904841,
      2257979135,
      2342840547,
      2397746618,
      2404399993,
      2415570156,
      2463627759,
      2467923710,
      2472587927,
      2528485485,
      2618530766,
      2899403804,
      2919115771,
      2951799221,
      2952523895,
      2963477884,
      2963809389
    ],
    "filter_matches": [
      "tr",
      "tl",
      "trl",
      "rl"
    ],
    "rank": 21218,
    "citation_count": 0,
    "estimated_citation_count": 0,
    "publication_date": "2018-07-02",
    "found_in": 1,
    "transfer_experiment_type": [
      "s",
      "levels",
      "#"
    ],
    "transfer_experiment_subtype": [
      "multi"
    ],
    "transfer_data_type": [
      "advisor"
    ],
    "transfer_performance_metrics": [
      "tt"
    ],
    "implementation": [
      "Custom",
      "CS",
      "Formulas",
      "Figures",
      "TensorFlow",
      "Tables"
    ],
    "policy_type": [
      "DQN"
    ],
    "task_mappings": [
      "N/A"
    ],
    "autonomous_transfer": 0,
    "is_deep_rl": 1,
    "tags": [
      "2D",
      "Simulation",
      "Navigation",
      "Collision",
      "Avoidance"
    ],
    "allowed_learner": [
      "TD"
    ],
    "country": [
      "USA"
    ],
    "uni": [
      "University of Southern California"
    ],
    "department": [
      "Aerospace & Mechanical Engineering"
    ],
    "source_task_selection": [
      "h"
    ],
    "was_in_survey": [
      "mag"
    ],
    "in_title": [
      "t",
      "r",
      "l"
    ],
    "in_abs": [
      "t",
      "r",
      "l"
    ],
    "in_contet": [
      "t",
      "r",
      "l",
      "k",
      "s"
    ],
    "rejected_or_unpublished": null,
    "duplication_status": 5,
    "paper_for_thesis": 3010746488,
    "paper_available": 1,
    "behind_paywall": 0,
    "paywall_circumventable": 0,
    "author_names": [
      "Yan Jin",
      "Xiongqing Liu"
    ]
  },
  {
    "id": 3014664339,
    "title": "DECAF: Deep Case-based Policy Inference for knowledge transfer in Reinforcement Learning",
    "abst": "Abstract Having the ability to solve increasingly complex problems using Reinforcement Learning (RL) has prompted researchers to start developing a greater interest in systematic approaches to retain and reuse knowledge over a variety of tasks. With Case-based Reasoning (CBR) there exists a general methodology that provides a framework for knowledge transfer which has been underrepresented in the RL literature so far. We formulate a terminology for the CBR framework targeted towards RL researchers with the goal of facilitating communication between the respective research communities. Based on this framework, we propose the Deep Case-based Policy Inference (DECAF) algorithm to accelerate learning by building a library of cases and reusing them if they are similar to a new task when training a new policy. DECAF guides the training by dynamically selecting and blending policies according to their usefulness for the current target task, reusing previously learned policies for a more effective exploration but still enabling the adaptation to particularities of the new task. We show an empirical evaluation in the Atari game playing domain depicting the benefits of our algorithm with regards to sample efficiency, robustness against negative transfer, and performance increase when compared to state-of-the-art methods.",
    "url": "https://www.sciencedirect.com/science/article/pii/S095741742030244X",
    "lang": null,
    "authors": [
      2112806856,
      2124955968,
      2231858596,
      2289046660
    ],
    "fos": [
      547195049,
      97541855,
      206588197,
      2776214188,
      119857082,
      2776960227,
      2779178101,
      136979486,
      154945302,
      41008148,
      97970142
    ],
    "journals": [
      13144211
    ],
    "conferences": [],
    "conference_series": [],
    "references": [
      778742492,
      1500151553,
      1515851193,
      1592209052,
      1949804828,
      1977203863,
      1991564165,
      2020573190,
      2031727428,
      2034332963,
      2076063813,
      2082189729,
      2097381042,
      2097498341,
      2103288465,
      2119567691,
      2126385963,
      2131600418,
      2133040789,
      2144653551,
      2145339207,
      2155791599,
      2161133420,
      2163808368,
      2165698076,
      2166798247,
      2271262891,
      2295877626,
      2426267443,
      2521032087,
      2567015638,
      2580175322,
      2585821313,
      2604737628,
      2620645529,
      2745868649,
      2749807327,
      2766253973,
      2779774899,
      2780395198,
      2792621174,
      2803849126,
      2885522812,
      2888438160,
      2906575710,
      2922003348,
      2963946410,
      2970398475,
      2988458498
    ],
    "filter_matches": [
      "tr",
      "tl",
      "trl",
      "rl"
    ],
    "rank": 21219,
    "citation_count": 0,
    "estimated_citation_count": 0,
    "publication_date": "2020-03-31",
    "found_in": 1,
    "transfer_experiment_type": [
      "s",
      "s_i",
      "s_f",
      "t"
    ],
    "transfer_experiment_subtype": [
      "multi",
      "lit"
    ],
    "transfer_data_type": [
      "PolicyLibrary"
    ],
    "transfer_performance_metrics": [
      "tt"
    ],
    "implementation": [
      "OSS",
      "TensorFlow",
      "Gym",
      "OpenCV",
      "Formulas",
      "Figures",
      "Pseudo"
    ],
    "policy_type": [
      "Q",
      "DQN",
      "PLBI",
      "CBPI",
      "DECAF",
      "A2T"
    ],
    "task_mappings": [
      "N/A"
    ],
    "autonomous_transfer": 0,
    "is_deep_rl": 1,
    "tags": [
      "Simulation",
      "2D",
      "Navigation",
      "Grid",
      "Games",
      "VideoGames",
      "Atari"
    ],
    "allowed_learner": [
      "CBR",
      "TD"
    ],
    "country": [
      "Brazil"
    ],
    "uni": [
      "University of Sao Paulo",
      "FEIs University Centre"
    ],
    "department": [],
    "source_task_selection": [
      "h",
      "lib"
    ],
    "was_in_survey": [
      "mag"
    ],
    "in_title": [
      "t",
      "r",
      "l",
      "k"
    ],
    "in_abs": [
      "t",
      "r",
      "l",
      "k"
    ],
    "in_contet": [
      "t",
      "r",
      "l",
      "k"
    ],
    "rejected_or_unpublished": null,
    "duplication_status": null,
    "paper_for_thesis": 0,
    "paper_available": 1,
    "behind_paywall": 1,
    "paywall_circumventable": 1,
    "author_names": [
      "Anna Helena Reali Costa",
      "Reinaldo A. C. Bianchi",
      "Ruben Glatt",
      "Felipe Leno da Silva"
    ]
  },
  {
    "id": 2978426310,
    "title": "Strategy Selection in Complex Game Environments Based on Transfer Reinforcement Learning",
    "abst": "Boosting the learning process in the new task by making use of previously obtained knowledge has been a challenging task in many fields of industrial engineering and scientific. In this paper, we propose a transfer reinforcement learning model with knowledge Inheritance and decision-making Assistance (trIA). In the stage of knowledge inheritance, trIA adopts a model that employs a simultaneous multi-task and multi-instance learning strategy to compress acquired experts knowledge from distinct task into a global multi-task agent. In the stage of decision-making assistance, trIA adopts a dual-column progressive neural network framework to fully utilize the previous knowledge in the global multi-task agent and the acquired knowledge in the new task. The experimental results on the Atari domain demonstrate that the proposed knowledge inheritance model can performed at nearly the same level as the experts on the distinct source task environments. The results also demonstrate that the decision-making assistance model can transfer knowledge from the source tasks to the target tasks effectively. Moreover, the comparative results with the state-ofthe-art algorithms validate the effectiveness of the proposed trIA for strategy selection in complex game environments.",
    "url": "https://ieeexplore.ieee.org/document/8852019",
    "lang": "en",
    "authors": [
      2777756777,
      2936026734,
      2939498572,
      2981043732
    ],
    "fos": [
      3017684168,
      46686674,
      119857082,
      97541855,
      41008148,
      50644808,
      154945302
    ],
    "journals": [],
    "conferences": [
      2892285143
    ],
    "conference_series": [
      1140449422
    ],
    "references": [
      1757796397,
      1821462560,
      1949804828,
      2097113539,
      2097381042,
      2117341272,
      2119717200,
      2145339207,
      2162768030,
      2165150801,
      2165698076,
      2174786457,
      2257979135,
      2426267443,
      2533806771,
      2586101872,
      2596367596,
      2607014226,
      2949369413,
      2950872548,
      2963911037,
      2964043796,
      3022194887
    ],
    "filter_matches": [
      "tr",
      "tl",
      "trl",
      "rl"
    ],
    "rank": 21220,
    "citation_count": 0,
    "estimated_citation_count": 0,
    "publication_date": "2019-07-14",
    "found_in": 1,
    "transfer_experiment_type": [
      "a",
      "r",
      "t",
      "#"
    ],
    "transfer_experiment_subtype": [
      "lit"
    ],
    "transfer_data_type": [
      "advisor"
    ],
    "transfer_performance_metrics": [
      "tt"
    ],
    "implementation": [
      "Custom",
      "CS",
      "Formulas",
      "Figures",
      "Pseudo",
      "Tables"
    ],
    "policy_type": [
      "A3C"
    ],
    "task_mappings": [
      "N/A"
    ],
    "autonomous_transfer": 0,
    "is_deep_rl": 1,
    "tags": [
      "2D",
      "Games",
      "VideoGames",
      "Atari",
      "Simulation",
      "Imitation"
    ],
    "allowed_learner": [
      "TD"
    ],
    "country": [
      "China",
      "Canada"
    ],
    "uni": [
      "Dalian University of Technology",
      "McGill University"
    ],
    "department": [
      "Computer Science and Technology"
    ],
    "source_task_selection": [
      "lib"
    ],
    "was_in_survey": [
      "mag"
    ],
    "in_title": [
      "t",
      "r",
      "l"
    ],
    "in_abs": [
      "t",
      "r",
      "l",
      "k"
    ],
    "in_contet": [
      "t",
      "r",
      "l",
      "k"
    ],
    "rejected_or_unpublished": null,
    "duplication_status": null,
    "paper_for_thesis": 0,
    "paper_available": 1,
    "behind_paywall": 1,
    "paywall_circumventable": 1,
    "author_names": [
      "Hongwei Ge",
      "Mingde Zhao",
      "Kai Zhang",
      "Liang Sun"
    ]
  },
  {
    "id": 2810881727,
    "title": "Transfer with Model Features in Reinforcement Learning.",
    "abst": "A key question in Reinforcement Learning is which representation an agent can learn to efficiently reuse knowledge between different tasks. Recently the Successor Representation was shown to have empirical benefits for transferring knowledge between tasks with shared transition dynamics. This paper presents Model Features: a feature representation that clusters behaviourally equivalent states and that is equivalent to a Model-Reduction. Further, we present a Successor Feature model which shows that learning Successor Features is equivalent to learning a Model-Reduction. A novel optimization objective is developed and we provide bounds showing that minimizing this objective results in an increasingly improved approximation of a Model-Reduction. Further, we provide transfer experiments on randomly generated MDPs which vary in their transition and reward functions but approximately preserve behavioural equivalence between states. These results demonstrate that Model Features are suitable for transfer between tasks with varying transition and reward functions.",
    "url": "https://arxiv.org/pdf/1807.01736",
    "lang": "en",
    "authors": [
      1075592465,
      2739590467
    ],
    "fos": [
      206588197,
      97541855,
      101814296,
      176935170,
      33923547,
      119857082,
      154945302,
      75306776
    ],
    "journals": [
      2597173376
    ],
    "conferences": [],
    "conference_series": [],
    "references": [
      1491843047,
      1522301498,
      2056354534,
      2058735307,
      2097381042,
      2107726111,
      2182573229,
      2567020712,
      2742143911,
      2953319434,
      2962717849
    ],
    "filter_matches": [
      "tr",
      "tl",
      "trl",
      "rl"
    ],
    "rank": 21220,
    "citation_count": 2,
    "estimated_citation_count": 2,
    "publication_date": "2018-07-04",
    "found_in": 1,
    "transfer_experiment_type": [
      "r",
      "t"
    ],
    "transfer_experiment_subtype": [
      "multi"
    ],
    "transfer_data_type": [
      "model",
      "fea"
    ],
    "transfer_performance_metrics": [
      "tt"
    ],
    "implementation": [
      "Custom",
      "CS",
      "Formulas",
      "Figures",
      "Pseudo",
      "Theorem",
      "Lemma"
    ],
    "policy_type": [
      "Q",
      "Random"
    ],
    "task_mappings": [
      "exp"
    ],
    "autonomous_transfer": 0,
    "is_deep_rl": 0,
    "tags": [
      "2D",
      "Navigation",
      "Grid",
      "Simulation"
    ],
    "allowed_learner": [
      "TD"
    ],
    "country": [
      "USA"
    ],
    "uni": [
      "Brown University"
    ],
    "department": [
      "Computer Science"
    ],
    "source_task_selection": [
      "h"
    ],
    "was_in_survey": [
      "mag"
    ],
    "in_title": [
      "t",
      "r",
      "l"
    ],
    "in_abs": [
      "t",
      "r",
      "l",
      "k"
    ],
    "in_contet": [
      "t",
      "r",
      "l",
      "k"
    ],
    "rejected_or_unpublished": 2,
    "duplication_status": null,
    "paper_for_thesis": 0,
    "paper_available": 1,
    "behind_paywall": 0,
    "paywall_circumventable": 0,
    "author_names": [
      "Michael L. Littman",
      "Lucas Lehnert"
    ]
  },
  {
    "id": 929682,
    "title": "Speeding-up reinforcement learning through abstraction and transfer learning",
    "abst": "We are interested in the following general question: is it possible to abstract knowledge that is generated while learning the solution of a problem, so that this abstraction can accelerate the learning process? Moreover, is it possible to transfer and reuse the acquired abstract knowledge to accelerate the learning process for future similar tasks? We propose a framework for conducting simultaneously two levels of reinforcement learning, where an abstract policy is learned while learning of a concrete policy for the problem, such that both policies are refined through exploration and interaction of the agent with the environment. We explore abstraction both to accelerate the learning process for an optimal concrete policy for the current problem, and to allow the application of the generated abstract policy in learning solutions for new problems. We report experiments in a robot navigation environment that show our framework to be effective in speeding up policy construction for practical problems and in generating abstractions that can be used to accelerate learning in new similar problems.",
    "url": "https://dl.acm.org/citation.cfm?id=2484942",
    "lang": "en",
    "authors": [
      1992822092,
      2102845975,
      2112806856,
      2168431149
    ],
    "fos": [
      24138899,
      32254414,
      41008148,
      77967617,
      188888258,
      119857082,
      154945302,
      97541855,
      77075516,
      28006648,
      12298181
    ],
    "journals": [],
    "conferences": [
      2785395174
    ],
    "conference_series": [
      1168671587
    ],
    "references": [
      1687873425,
      1997477668,
      2031727428,
      2056584142,
      2098524868,
      2101826491,
      2108535023,
      2119567691,
      2126834960,
      2134197408,
      2140135625,
      2156347136,
      2167840144,
      2183243664,
      2183728818,
      2187770737,
      2200611301,
      2221828264,
      2253637894,
      2964108826
    ],
    "filter_matches": [
      "tr",
      "tl",
      "trl",
      "rl"
    ],
    "rank": 21225,
    "citation_count": 6,
    "estimated_citation_count": 6,
    "publication_date": "2013-05-06",
    "found_in": 1,
    "transfer_experiment_type": [
      "s_i",
      "s_f"
    ],
    "transfer_experiment_subtype": [
      "multi"
    ],
    "transfer_data_type": [
      "pi"
    ],
    "transfer_performance_metrics": [
      "tr",
      "ap"
    ],
    "implementation": [
      "Custom",
      "CS",
      "Formulas",
      "Pseudo",
      "Figures"
    ],
    "policy_type": [
      "S2L-RL",
      "Q"
    ],
    "task_mappings": [
      "N/A"
    ],
    "autonomous_transfer": 0,
    "is_deep_rl": 0,
    "tags": [
      "2D",
      "Navigation",
      "Grid",
      "Simulation",
      "Robotics"
    ],
    "allowed_learner": [
      "TD",
      "RRL"
    ],
    "country": [
      "Brazil"
    ],
    "uni": [
      "Universidade de So Paulo"
    ],
    "department": [],
    "source_task_selection": [
      "h"
    ],
    "was_in_survey": [
      "mag"
    ],
    "in_title": [
      "t",
      "r",
      "l"
    ],
    "in_abs": [
      "t",
      "r",
      "l",
      "k"
    ],
    "in_contet": [
      "t",
      "r",
      "l",
      "k"
    ],
    "rejected_or_unpublished": null,
    "duplication_status": null,
    "paper_for_thesis": 0,
    "paper_available": 1,
    "behind_paywall": 0,
    "paywall_circumventable": 0,
    "author_names": [
      "Fabio Gagliardi Cozman",
      "Marcelo Li Koga",
      "Anna Helena Reali Costa",
      "Valdinei Freire da Silva"
    ]
  },
  {
    "id": 2018629084,
    "title": "Towards reinforcement learning representation transfer",
    "abst": "Transfer learning problems are typically framed as leveraging knowledge learned on a source task to improve learning on a related, but different, target task. Current transfer methods are able to successfully transfer knowledge between agents in different reinforcement learning tasks, reducing the time needed to learn the target. However, the complimentary task of representation transfer, i.e. transferring knowledge between agents with different internal representations, has not been well explored. The goal in both types of transfer problems is the same: reduce the time needed to learn the target with transfer, relative to learning the target without transfer. This work introduces one such representation transfer algorithm which is implemented in a complex multiagent domain. Experiments demonstrate that transferring the learned knowledge between different representations is both possible and beneficial.",
    "url": "https://dl.acm.org/citation.cfm?id=1329125.1329248",
    "lang": "en",
    "authors": [
      2147180669,
      2148762994
    ],
    "fos": [
      97541855,
      28006648,
      47932503,
      77075516,
      119857082,
      150899416,
      154945302,
      41008148
    ],
    "journals": [],
    "conferences": [
      2786151659
    ],
    "conference_series": [
      1168671587
    ],
    "references": [
      29328521,
      36691172,
      65244443,
      1500245617,
      1515851193,
      1517018472,
      2079247031,
      2097113539,
      2104641222,
      2113913482,
      2122982548,
      2128905965,
      2169428619,
      2169659168
    ],
    "filter_matches": [
      "tr",
      "tl",
      "trl",
      "rl"
    ],
    "rank": 21258,
    "citation_count": 9,
    "estimated_citation_count": 9,
    "publication_date": "2007-05-14",
    "found_in": 1,
    "transfer_experiment_type": [
      "a",
      "v"
    ],
    "transfer_experiment_subtype": [
      "diff-it"
    ],
    "transfer_data_type": [
      "Q"
    ],
    "transfer_performance_metrics": [
      "tt"
    ],
    "implementation": [
      "Custom",
      "CS",
      "Pseudo",
      "Formulas",
      "Figures",
      "Web",
      "Dead"
    ],
    "policy_type": [
      "SARSA"
    ],
    "task_mappings": [
      "sup"
    ],
    "autonomous_transfer": 0,
    "is_deep_rl": 0,
    "tags": [
      "Simulation",
      "RoboCup",
      "KeepAway",
      "MultiAgent",
      "3v2"
    ],
    "allowed_learner": [
      "TD"
    ],
    "country": [
      "USA"
    ],
    "uni": [
      "The University of Texas"
    ],
    "department": [
      "Computer Science"
    ],
    "source_task_selection": [
      "h"
    ],
    "was_in_survey": [
      "mag"
    ],
    "in_title": [
      "t",
      "r",
      "l"
    ],
    "in_abs": [
      "t",
      "r",
      "l",
      "k"
    ],
    "in_contet": [
      "t",
      "r",
      "l",
      "k"
    ],
    "rejected_or_unpublished": null,
    "duplication_status": null,
    "paper_for_thesis": 0,
    "paper_available": 1,
    "behind_paywall": 0,
    "paywall_circumventable": 0,
    "author_names": [
      "Peter Stone",
      "Matthew D. Taylor"
    ]
  },
  {
    "id": 2240785302,
    "title": "Collaborative reinforcement learning for a two-robot job transfer flow-shop scheduling problem",
    "abst": "A two-robot flow-shop scheduling problem with n identical jobs and m machines is defined and evaluated for four robot collaboration levels corresponding to different levels of information sharing, learning and assessment : Full -- robots work together, performing self and joint learning sharing full information; Pull -- one robot decides when and if to learn from the other robot; Push -- one robot may force the second to learn from it and None -- each robot learns independently with no information sharing. Robots operate on parallel tracks, transporting jobs between successive machines, returning empty to a machine to move another job. The objective is to obtain a robot schedule that minimises makespan ( C max ) for machines with varying processing times. A new reinforcement learning algorithm is developed, using dual Q - learning functions. A novel feature in the collaborative algorithm is the assignment of different reward functions to robots; minimising robot idle time and minimising job waiting time. Such delays increase makespan. Simulation analyses with fast, medium and slow speed robots indicated that Full collaboration with a fast--fast robot pair was best according to minimum average upper bound error. The new collaborative algorithm provides a tool for finding optimal and near-optimal solutions to difficult collaborative multi-robot scheduling problems.",
    "url": "http://www.tandfonline.com/doi/full/10.1080/00207543.2015.1057297",
    "lang": "en",
    "authors": [
      250141666,
      319050920,
      2154355101
    ],
    "fos": [
      77553402,
      21547014,
      97541855,
      188888258,
      2776854237,
      113200698,
      158336966,
      55416958,
      41008148,
      90509273
    ],
    "journals": [
      65690446
    ],
    "conferences": [],
    "conference_series": [],
    "references": [
      32283066,
      1970502488,
      1974960482,
      1988835048,
      1992329757,
      2011024242,
      2025853862,
      2027524952,
      2028145591,
      2033538145,
      2039233423,
      2048804129,
      2049183520,
      2053343507,
      2057715281,
      2064777332,
      2080379331,
      2100662461,
      2107544712,
      2118686230,
      2121863487,
      2122385370,
      2129068093,
      2155888416,
      2169511809,
      2570123661
    ],
    "filter_matches": [
      "tr",
      "tl",
      "trl",
      "rl"
    ],
    "rank": 21273,
    "citation_count": 7,
    "estimated_citation_count": 7,
    "publication_date": "2016-02-16",
    "found_in": 1,
    "transfer_experiment_type": [
      "t"
    ],
    "transfer_experiment_subtype": [
      "multi"
    ],
    "transfer_data_type": [
      "pi"
    ],
    "transfer_performance_metrics": [
      "tr"
    ],
    "implementation": [
      "Custom",
      "CS",
      "Formulas",
      "Figures",
      "Tables"
    ],
    "policy_type": [
      "Q"
    ],
    "task_mappings": [
      "N/A"
    ],
    "autonomous_transfer": 0,
    "is_deep_rl": 0,
    "tags": [
      "Simulation",
      "Robotics",
      "FlowShop",
      "Collaboration",
      "Scheduling"
    ],
    "allowed_learner": [
      "TD"
    ],
    "country": [
      "Israel"
    ],
    "uni": [
      "Ben-Gurion University of the Negev"
    ],
    "department": [
      "Industrial Engineering and Management"
    ],
    "source_task_selection": [],
    "was_in_survey": [
      "mag"
    ],
    "in_title": [
      "t",
      "r",
      "l"
    ],
    "in_abs": [
      "t",
      "r",
      "l"
    ],
    "in_contet": [
      "t",
      "r",
      "l"
    ],
    "rejected_or_unpublished": null,
    "duplication_status": null,
    "paper_for_thesis": 0,
    "paper_available": 1,
    "behind_paywall": 0,
    "paywall_circumventable": 0,
    "author_names": [
      "Yael Edan",
      "Kfir Arviv",
      "Helman Stern"
    ]
  },
  {
    "id": 2344013593,
    "title": "Relational transfer in reinforcement learning",
    "abst": "Transfer learning is an inherent aspect of human learning. When humans learn to perform a task, we rarely start from scratch. Instead, we recall relevant knowledge from previous learning experiences and apply that knowledge to help us master the new task more quickly. This principle can be applied to machine learning as well. Machine learning often addresses single learning tasks in isolation. Even though multiple related tasks may exist in a domain, many algorithms for machine learning have no way to utilize those relationships. Algorithms that allow successful transfer from one task (the source) to another task (the target) are necessary steps towards making machine learning as adaptable as human learning. This thesis investigates transfer methods for reinforcement learning (RL), where an agent takes series of actions in an environment. RL often requires substantial amounts of nearly random exploration, particularly in the early stages of learning. The ability to transfer knowledge from previous tasks can therefore be an important asset for RL agents. Transfer from related source tasks can reduce the low initial performance that is common in challenging target tasks. I focus on transferring relational knowledge that guides action choices. Relational knowledge typically uses first-order logic to express information about relationships among objects. First-order logic, unlike propositional logic, can use variables that generalize over classes of objects. This greater generalization makes first-order logic more effective for transfer. This thesis contributes six transfer algorithms in three categories: advice-based transfer, macro transfer, and MLN transfer. Advice-based transfer uses source-task knowledge to provide advice for a target-task learner, which can follow, refine, or ignore the advice according to its value. Macro-transfer and MLN-transfer methods use source-task experience to demonstrate good behavior for a target-task learner. I evaluate these algorithms experimentally in the complex reinforcement-learning domain of RoboCup simulated soccer. All of my algorithms provide empirical benefits compared to non-transfer approaches, either by increasing initial performance or by enabling faster learning in the target task.",
    "url": "https://minds.wisconsin.edu/handle/1793/60678",
    "lang": "en",
    "authors": [
      738944226,
      2079278047
    ],
    "fos": [
      8038995,
      119857082,
      28006648,
      77967617,
      177877439,
      41008148,
      150899416,
      154945302,
      24138899,
      77075516,
      188888258
    ],
    "journals": [],
    "conferences": [],
    "conference_series": [],
    "references": [
      36691172,
      48809960,
      55333328,
      57528462,
      99485931,
      107556075,
      115717799,
      198956113,
      203338875,
      1484446963,
      1486892717,
      1490769087,
      1506146479,
      1528140509,
      1536943637,
      1582256513,
      1582436621,
      1584120419,
      1585529040,
      1587567107,
      1594602740,
      1601600618,
      1822705290,
      1947191636,
      1976115983,
      1977970897,
      1988790447,
      1992967856,
      2004030284,
      2005374071,
      2012036715,
      2031727428,
      2036043322,
      2056102643,
      2062179223,
      2076167926,
      2076337359,
      2079247031,
      2084549025,
      2097826433,
      2098027503,
      2098723043,
      2100677568,
      2104260616,
      2107298017,
      2110292307,
      2112648537,
      2121517924,
      2121863487,
      2122838776,
      2122982548,
      2123261262,
      2123470622,
      2123995443,
      2128905965,
      2130903752,
      2131600418,
      2134845968,
      2136504847,
      2140584963,
      2145454741,
      2145574906,
      2145983895,
      2153353285,
      2154328025,
      2154913025,
      2155791599,
      2156493855,
      2158150115,
      2160644528,
      2160927913,
      2161252410,
      2162888803,
      2164077610,
      2164114810,
      2164524038,
      2165744911,
      2166798247,
      2169743339,
      2540540486,
      2913340405,
      2971260373,
      3022194887,
      3027269366
    ],
    "filter_matches": [
      "tr",
      "tl",
      "trl",
      "rl"
    ],
    "rank": 21277,
    "citation_count": 6,
    "estimated_citation_count": 6,
    "publication_date": "2009-01-01",
    "found_in": 1,
    "transfer_experiment_type": [
      "#",
      "t",
      "a",
      "v"
    ],
    "transfer_experiment_subtype": [
      "multi"
    ],
    "transfer_data_type": [
      "advisor",
      "rule",
      "Q",
      "MLN"
    ],
    "transfer_performance_metrics": [
      "j",
      "tt",
      "ap",
      "tr"
    ],
    "implementation": [
      "Custom",
      "CS",
      "Formulas",
      "Figures",
      "Tables",
      "Pseudo",
      "Web",
      "Dead"
    ],
    "policy_type": [
      "Q",
      "MLN"
    ],
    "task_mappings": [
      "sup"
    ],
    "autonomous_transfer": 0,
    "is_deep_rl": 0,
    "tags": [
      "Simulation",
      "RoboCup",
      "MultiAgent",
      "KeepAway",
      "BreakAway",
      "MoveDownfield",
      "2v1",
      "3v2",
      "4v3"
    ],
    "allowed_learner": [
      "TD",
      "RRL",
      "B"
    ],
    "country": [
      "USA"
    ],
    "uni": [
      "University of Wisconsin"
    ],
    "department": [
      "Computer Sciences"
    ],
    "source_task_selection": [
      "h"
    ],
    "was_in_survey": [
      "mag"
    ],
    "in_title": [
      "t",
      "r",
      "l"
    ],
    "in_abs": [
      "t",
      "r",
      "l",
      "k"
    ],
    "in_contet": [
      "t",
      "r",
      "l",
      "k",
      "s"
    ],
    "rejected_or_unpublished": null,
    "duplication_status": null,
    "paper_for_thesis": 1,
    "paper_available": 1,
    "behind_paywall": 0,
    "paywall_circumventable": 0,
    "author_names": [
      "Lisa Torrey",
      "Jude W. Shavlik"
    ]
  },
  {
    "id": 2969408081,
    "title": "Sim-to-real transfer reinforcement learning for control of thermal effects of an atmospheric pressure plasma jet",
    "abst": null,
    "url": "https://ui.adsabs.harvard.edu/abs/2019PSST...28i5019W/abstract",
    "lang": "en",
    "authors": [
      2156917138,
      2167152368,
      2475677491,
      2486131789,
      2633482356
    ],
    "fos": [
      57879066,
      3018651601,
      185592680,
      97541855,
      85344455,
      184779094,
      204530211
    ],
    "journals": [
      57082650
    ],
    "conferences": [],
    "conference_series": [],
    "references": [
      1490800343,
      1495330892,
      1861182278,
      1935290342,
      1942945997,
      1971438819,
      1994483619,
      1998578566,
      2018507720,
      2038410048,
      2077365766,
      2099514329,
      2111270263,
      2145339207,
      2154525076,
      2163150190,
      2168890066,
      2724171116,
      2765557211,
      2792380552,
      2801229243,
      2923554444,
      2942635952,
      2951141375
    ],
    "filter_matches": [
      "tr",
      "tl",
      "trl",
      "rl"
    ],
    "rank": 21277,
    "citation_count": 2,
    "estimated_citation_count": 2,
    "publication_date": "2019-09-24",
    "found_in": 1,
    "transfer_experiment_type": [
      "t"
    ],
    "transfer_experiment_subtype": [
      "pure",
      "sim2real"
    ],
    "transfer_data_type": [
      "pi"
    ],
    "transfer_performance_metrics": [
      "tt"
    ],
    "implementation": [
      "Custom",
      "CS",
      "Formulas",
      "Figures",
      "Tables",
      "Pseudo"
    ],
    "policy_type": [
      "RLC",
      "G-RLC"
    ],
    "task_mappings": [],
    "autonomous_transfer": 2,
    "is_deep_rl": 1,
    "tags": [
      "Robotics",
      "FeedbackControl",
      "AtmosphericPressurePlasma",
      "RealWorld"
    ],
    "allowed_learner": [
      "TD"
    ],
    "country": [
      "USA",
      "Switzerland"
    ],
    "uni": [
      "University of California",
      "Ecole Polytechnique Fdrale de Lausanne"
    ],
    "department": [
      "Chemical and Biomolecular Engineering",
      "Molecular Simulation"
    ],
    "source_task_selection": [],
    "was_in_survey": [
      "mag"
    ],
    "in_title": [
      "t",
      "r",
      "l"
    ],
    "in_abs": [
      "r",
      "l"
    ],
    "in_contet": [
      "t",
      "r",
      "l"
    ],
    "rejected_or_unpublished": null,
    "duplication_status": null,
    "paper_for_thesis": 0,
    "paper_available": 1,
    "behind_paywall": 0,
    "paywall_circumventable": 0,
    "author_names": [
      "David B. Graves",
      "Berend Smit",
      "Dogan Gidon",
      "Matthew Witman",
      "Ali Mesbah"
    ]
  },
  {
    "id": 2906148830,
    "title": "Relationship Between the Order for Motor Skill Transfer and Motion Complexity in Reinforcement Learning",
    "abst": "We propose a method to generate an order for learning and transferring motor skills based on motion complexity, then evaluate the order to learn motor skills of a task and transfer them to another task as a form of reinforcement learning (RL). Here, motion complexity refers to the complexity calculated from multiple motion trajectories of a task. To do this, multiple human demonstrations are extracted and clustered to calculate motion complexity and identify the motor skills involved in a task. The motion trajectories of the task are then used to calculate the motion complexity considering temporal entropy and spatial entropy. Finally, both orders [Simple-to-Complex] and [Complex-to-Simple] are generated to learn and transfer motor skills based on the motion complexities of multiple tasks. To evaluate these orders, two tasks [Drawing] and [Fitting] are performed using an actual robotic arm. To verify the learning and transfer processes, we apply our method to three different figures as well as to pegs and holes of three different shapes and analyze the experimental results. In addition, we provide guidelines for using the [Simple-to-Complex] and [Complex-to-Simple] orders in RL.",
    "url": "https://doi.org/10.1109/LRA.2018.2889026",
    "lang": "en",
    "authors": [
      2238234819,
      2260553223,
      2305855931,
      2675277650
    ],
    "fos": [
      127413603,
      73555534,
      169976356,
      90509273,
      154945302,
      175154964,
      13662910,
      2994535726,
      133731056,
      97541855,
      150415221
    ],
    "journals": [],
    "conferences": [
      2891769531
    ],
    "conference_series": [
      1163902177
    ],
    "references": [
      1528875963,
      1991082029,
      2023944350,
      2026933318,
      2048732280,
      2086812152,
      2118459920,
      2152166054,
      2153233077,
      2154543878,
      2471821491,
      2569254955,
      2574767923,
      2606299512,
      2808324312
    ],
    "filter_matches": [
      "tr",
      "tl",
      "trl",
      "rl"
    ],
    "rank": 21284,
    "citation_count": 1,
    "estimated_citation_count": 1,
    "publication_date": "2019-04-01",
    "found_in": 1,
    "transfer_experiment_type": [
      "s",
      "t"
    ],
    "transfer_experiment_subtype": [
      "sim2real",
      "curriculum",
      "multi"
    ],
    "transfer_data_type": [
      "pi"
    ],
    "transfer_performance_metrics": [
      "j",
      "tt"
    ],
    "implementation": [
      "Custom",
      "CS",
      "Formulas",
      "Figures",
      "Pseudo",
      "Tables"
    ],
    "policy_type": [
      "Q",
      "PoWER"
    ],
    "task_mappings": [
      "N/A"
    ],
    "autonomous_transfer": 0,
    "is_deep_rl": 0,
    "tags": [
      "Robotics",
      "Simulation",
      "RealWorld",
      "MotorSkill",
      "Demonstration",
      "Expert",
      "Manipulation",
      "Drawing",
      "Fitting"
    ],
    "allowed_learner": [
      "TD",
      "B",
      "MB"
    ],
    "country": [
      "South Korea"
    ],
    "uni": [
      "Hanyang University",
      "Korea Institute of Industrial Technology"
    ],
    "department": [
      "Department of Electronics and Computer Engineering",
      "Smart Research Group"
    ],
    "source_task_selection": [
      "h"
    ],
    "was_in_survey": [
      "mag"
    ],
    "in_title": [
      "t",
      "r",
      "l",
      "s"
    ],
    "in_abs": [
      "t",
      "r",
      "l",
      "s"
    ],
    "in_contet": [
      "t",
      "r",
      "l",
      "s",
      "k"
    ],
    "rejected_or_unpublished": null,
    "duplication_status": null,
    "paper_for_thesis": 0,
    "paper_available": 1,
    "behind_paywall": 1,
    "paywall_circumventable": 1,
    "author_names": [
      "Il Hong Suh",
      "Hong-Seok Kim",
      "Sang Hyoung Lee",
      "Nam Jun Cho"
    ]
  },
  {
    "id": 2148179544,
    "title": "Structural knowledge transfer by spatial abstraction for reinforcement learning agents",
    "abst": "In this article we investigate the role of abstraction principles for knowledge transfer in agent control learning tasks. We analyze abstraction from a formal point of view and characterize three distinct facets: aspectualization, coarsening, and conceptual classification. The taxonomy we develop allows us to interrelate existing approaches to abstraction, leading to a code of practice for designing knowledge representations that support knowledge transfer. We detail how aspectualization can be utilized to achieve knowledge transfer in reinforcement learning. We propose the use of so-called structure space aspectualizable knowledge representations that explicate structural properties of the state space and present a posteriori structure space aspectualization (APSST) as a method to extract generally sensible behavior from a learned policy. This new policy can be used for knowledge transfer to support learning new tasks in different environments. Finally, we present a case study that demonstrates transfer of generally sensible navigation skills from simple simulation to a real-world robotic platform.",
    "url": "https://dblp.uni-trier.de/db/journals/adb/adb18.html#FrommbergerW10",
    "lang": "en",
    "authors": [
      163492721,
      2135816447
    ],
    "fos": [
      2776960227,
      124304363,
      97541855,
      41008148,
      75553542,
      119857082,
      154945302,
      72434380,
      186528591,
      77075516,
      150899416
    ],
    "journals": [
      183337005
    ],
    "conferences": [],
    "conference_series": [],
    "references": [
      136800639,
      273035880,
      1488730473,
      1492014007,
      1506146479,
      1515851193,
      1523309175,
      1583380718,
      1589542963,
      1688218840,
      1754881896,
      1801951652,
      2031727428,
      2064406997,
      2064994289,
      2065087844,
      2065356613,
      2079247031,
      2079381666,
      2107726111,
      2109910161,
      2114451917,
      2115986989,
      2118022839,
      2121863487,
      2128281152,
      2128905965,
      2130422193,
      2154418813,
      2158548602,
      2162076796,
      2164114810,
      2166855139,
      2397240726,
      2946461872,
      3011120880
    ],
    "filter_matches": [
      "tr",
      "tl",
      "trl",
      "rl"
    ],
    "rank": 21294,
    "citation_count": 11,
    "estimated_citation_count": 11,
    "publication_date": "2010-12-01",
    "found_in": 1,
    "transfer_experiment_type": [
      "t",
      "s_i",
      "s_f"
    ],
    "transfer_experiment_subtype": [
      "pure",
      "sim2real"
    ],
    "transfer_data_type": [
      "fea",
      "Q"
    ],
    "transfer_performance_metrics": [
      "j",
      "tt",
      "tr",
      "ap"
    ],
    "implementation": [
      "Custom",
      "CS",
      "Formulas",
      "Pseudo",
      "Theorem",
      "Figures",
      "Tables",
      "Open-SLAM"
    ],
    "policy_type": [
      "APSST",
      "Q"
    ],
    "task_mappings": [],
    "autonomous_transfer": 0,
    "is_deep_rl": 0,
    "tags": [
      "Robotics",
      "Navigation",
      "RealWorld",
      "LRF",
      "Simulation"
    ],
    "allowed_learner": [
      "TD"
    ],
    "country": [
      "Germany"
    ],
    "uni": [
      "University of Bremen"
    ],
    "department": [
      "Cognitive Systems"
    ],
    "source_task_selection": [],
    "was_in_survey": [
      "mag"
    ],
    "in_title": [
      "t",
      "r",
      "l",
      "k"
    ],
    "in_abs": [
      "t",
      "r",
      "l",
      "s",
      "k"
    ],
    "in_contet": [
      "t",
      "r",
      "l",
      "s",
      "k"
    ],
    "rejected_or_unpublished": null,
    "duplication_status": null,
    "paper_for_thesis": 0,
    "paper_available": 1,
    "behind_paywall": 0,
    "paywall_circumventable": 0,
    "author_names": [
      "Lutz Frommberger",
      "Diedrich Wolter"
    ]
  },
  {
    "id": 2756467676,
    "title": "Transfer learning via linear multi-variable mapping under reinforcement learning framework",
    "abst": "Though popular in many agent learning tasks, reinforcement learning still faces problems, such as long learning time in complex environment. Transfer learning could shorten the learning time and improve the performance in reinforcement learning by reusing the knowledge acquired from different but related source task. Due to the difference in state space and/or action space of the target and source task, transfer via inter-task mapping is a popular method. The design of the inter-task mapping is very critical to this transfer learning method. In this paper, we propose a linear multi-variable mapping (LMVM) for the transfer learning to make a better use of the knowledge learned from the source task. Unlike the inter-task mapping used before, the LMVM is not a one-to-one mapping but a one-to-many mapping, which is based on the idea that the element in target task is related with several similar elements from source task. We test transfer learning via our new mapping on the Keepaway platform. The experimental results show that our method could make the reinforcement learning agents learn much faster than those without transfer and those transfer with simpler mappings.",
    "url": "http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=8028754",
    "lang": null,
    "authors": [
      2096807613,
      2101925657,
      2244529586
    ],
    "fos": [
      150899416,
      41008148,
      8038995,
      154945302,
      188888258,
      119857082,
      58973888,
      28006648,
      199190896,
      77075516,
      77967617
    ],
    "journals": [],
    "conferences": [
      2626267033
    ],
    "conference_series": [
      2623792093
    ],
    "references": [
      2097381042,
      2097498341,
      2121863487,
      2133040789,
      2158150115,
      2169771756,
      2169911641,
      2181867278
    ],
    "filter_matches": [
      "tr",
      "tl",
      "trl",
      "rl"
    ],
    "rank": 21303,
    "citation_count": 2,
    "estimated_citation_count": 2,
    "publication_date": "2017-07-01",
    "found_in": 1,
    "transfer_experiment_type": [
      "#",
      "v"
    ],
    "transfer_experiment_subtype": [
      "multi"
    ],
    "transfer_data_type": [
      "Q"
    ],
    "transfer_performance_metrics": [
      "tt"
    ],
    "implementation": [
      "Custom",
      "CS",
      "Formulas",
      "Figures",
      "Tables"
    ],
    "policy_type": [
      "SARSA"
    ],
    "task_mappings": [
      "svg",
      "Ma"
    ],
    "autonomous_transfer": 0,
    "is_deep_rl": 0,
    "tags": [
      "2D",
      "Simulation",
      "KeepAway",
      "Navigation",
      "MultiAgent",
      "RoboCup"
    ],
    "allowed_learner": [
      "TD"
    ],
    "country": [
      "China"
    ],
    "uni": [
      "National University of Defense Technology"
    ],
    "department": [
      "College of Mechatronics and Automation"
    ],
    "source_task_selection": [
      "h"
    ],
    "was_in_survey": [
      "mag"
    ],
    "in_title": [
      "t",
      "r",
      "l"
    ],
    "in_abs": [
      "t",
      "r",
      "l",
      "k"
    ],
    "in_contet": [
      "t",
      "r",
      "l",
      "k"
    ],
    "rejected_or_unpublished": null,
    "duplication_status": null,
    "paper_for_thesis": 0,
    "paper_available": 1,
    "behind_paywall": 0,
    "paywall_circumventable": 0,
    "author_names": [
      "Lincheng Shen",
      "Xiangke Wang",
      "Qiao Cheng"
    ]
  },
  {
    "id": 2093192040,
    "title": "Transfer in inverse reinforcement learning for multiple strategies",
    "abst": "We consider the problem of incrementally learning different strategies of performing a complex sequential task from multiple demonstrations of an expert or a set of experts. While the task is the same, each expert differs in his/her way of performing it. We assume that this variety across experts&#039; demonstration is due to the fact that each expert/strategy is driven by a different reward function, where reward function is expressed as a linear combination of a set of known features. Consequently, we can learn all the expert strategies by forming a convex set of optimal deterministic policies, from which one can match any unseen expert strategy drawn from this set. Instead of learning from scratch every optimal policy in this set, the learner transfers knowledge from the set of learned policies to bootstrap its search for new optimal policy. We demonstrate our approach on a simulated mini-golf task where the 7 degrees of freedom Barrett WAM robot arm learns to sequentially putt on different holes in accordance with the playing strategies of the expert.",
    "url": "http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=6696817",
    "lang": "en",
    "authors": [
      405223864,
      2051335485
    ],
    "fos": [
      119857082,
      150415221,
      150899416,
      188888258,
      154945302,
      199190896,
      8038995,
      41008148,
      9260844,
      188116033,
      49870271
    ],
    "journals": [],
    "conferences": [
      116436602
    ],
    "conference_series": [
      1143279144
    ],
    "references": [
      1567876833,
      1591675293,
      1684361744,
      1999874108,
      2061562262,
      2097381042,
      2098774185,
      2102847492,
      2113023245,
      2119567691,
      2119914747,
      2132057084,
      2169498096,
      2181849516,
      2950989964
    ],
    "filter_matches": [
      "tr",
      "tl",
      "trl",
      "rl"
    ],
    "rank": 21337,
    "citation_count": 5,
    "estimated_citation_count": 5,
    "publication_date": "2013-11-01",
    "found_in": 1,
    "transfer_experiment_type": [
      "s_i",
      "s_f",
      "levels"
    ],
    "transfer_experiment_subtype": [
      "multi"
    ],
    "transfer_data_type": [
      "advisor"
    ],
    "transfer_performance_metrics": [
      "tt",
      "j"
    ],
    "implementation": [
      "Custom",
      "CS",
      "Formulas",
      "Figures",
      "Pseudo",
      "Lemma"
    ],
    "policy_type": [
      "StochasticPolicy"
    ],
    "task_mappings": [
      "N/A"
    ],
    "autonomous_transfer": 0,
    "is_deep_rl": 0,
    "tags": [
      "Robotics",
      "Simulation",
      "MiniGolf",
      "Arm",
      "2D",
      "Navigation",
      "Grid"
    ],
    "allowed_learner": [
      "TD"
    ],
    "country": [
      "Switzerland"
    ],
    "uni": [
      "Ecole Polytechnique Federale de Lausann"
    ],
    "department": [
      "Learning Algorithms and Systems Laboratory (LASA)"
    ],
    "source_task_selection": [
      "all"
    ],
    "was_in_survey": [
      "mag"
    ],
    "in_title": [
      "t",
      "r",
      "l"
    ],
    "in_abs": [
      "t",
      "l",
      "k"
    ],
    "in_contet": [
      "t",
      "r",
      "l",
      "k",
      "s"
    ],
    "rejected_or_unpublished": null,
    "duplication_status": null,
    "paper_for_thesis": 0,
    "paper_available": 1,
    "behind_paywall": 0,
    "paywall_circumventable": 0,
    "author_names": [
      "Ajay Kumar Tanwani",
      "Aude Billard"
    ]
  },
  {
    "id": 1912083604,
    "title": "Autonomous inter-task transfer in reinforcement learning domains",
    "abst": "Reinforcement learning (RL) methods have become popular in recent years because of their ability to solve complex tasks with minimal feedback. While these methods have had experimental successes and have been shown to exhibit some desirable properties in theory, the basic learning algorithms have often been found slow in practice. Therefore, much of the current RL research focuses on speeding up learning by taking advantage of domain knowledge, or by better utilizing agents&#039; experience. The ambitious goal of transfer learning, when applied to RL tasks, is to accelerate learning on some target task after training on a different, but related, source task. This dissertation demonstrates that transfer learning methods can successfully improve learning in RL tasks via experience from previously learned tasks. Transfer learning can increase RL&#039;s applicability to difficult tasks by allowing agents to generalize their experience across learning problems. This dissertation presents inter-task mappings, the first transfer mechanism in this area to successfully enable transfer between tasks with different state variables and actions. Inter-task mappings have subsequently been used by a number of transfer researchers. A set of six transfer learning algorithms are then introduced. While these transfer methods differ in terms of what base RL algorithms they are compatible with, what type of knowledge they transfer, and what their strengths are, all utilize the same inter-task mapping mechanism. These transfer methods can all successfully use mappings constructed by a human from domain knowledge, but there may be situations in which domain knowledge is unavailable, or insufficient, to describe how two given tasks are related. We therefore also study how inter-task mappings can be learned autonomously by leveraging existing machine learning algorithms. Our methods use classification and regression techniques to successfully discover similarities between data gathered in pairs of tasks, culminating in what is currently one of the most robust mapping-learning algorithms for RL transfer. Combining transfer methods with these similarity-learning algorithms allows us to empirically demonstrate the plausibility of autonomous transfer. We fully implement these methods in four domains (each with different salient characteristics), show that transfer can significantly improve an agent&#039;s ability to learn in each domain, and explore the limits of transfer&#039;s applicability.",
    "url": "http://www.dtic.mil/dtic/tr/fulltext/u2/1024624.pdf",
    "lang": "en",
    "authors": [
      2148762994
    ],
    "fos": [
      28006648,
      199190896,
      119857082,
      97541855,
      207685749,
      154945302,
      77075516,
      41008148,
      150899416,
      50644808,
      58973888
    ],
    "journals": [],
    "conferences": [
      2785613717
    ],
    "conference_series": [
      1184914352
    ],
    "references": [
      24477102,
      29328521,
      36691172,
      65244443,
      111328409,
      173396443,
      183249136,
      203338875,
      214983502,
      1490954610,
      1492014007,
      1495978126,
      1496855202,
      1500151553,
      1500245617,
      1505937442,
      1506146479,
      1510402218,
      1514621373,
      1515851193,
      1517018472,
      1533597678,
      1534331386,
      1550698229,
      1564393562,
      1568161011,
      1570448133,
      1579667258,
      1582256513,
      1584313244,
      1585546214,
      1585603966,
      1591803298,
      1598748993,
      1601974704,
      1607318605,
      1612195517,
      1670263352,
      1688218840,
      1783866091,
      1799762961,
      1822705290,
      1835855307,
      1853223271,
      1947179618,
      1949804828,
      1976115983,
      1981446214,
      1984542317,
      1990911977,
      1993277309,
      2012036715,
      2014512216,
      2022775778,
      2031727428,
      2041367235,
      2042357378,
      2048226872,
      2049633694,
      2079247031,
      2089561656,
      2093090592,
      2097113539,
      2098723043,
      2099233925,
      2100211715,
      2100677568,
      2104641222,
      2106953752,
      2107726111,
      2109910161,
      2110292307,
      2111935653,
      2113913482,
      2114013702,
      2115931363,
      2116339921,
      2117341272,
      2117629901,
      2119053738,
      2119567691,
      2119717200,
      2120346334,
      2120479337,
      2121517924,
      2122982548,
      2123995443,
      2124175081,
      2124290836,
      2125074935,
      2125614502,
      2126385963,
      2128547596,
      2128905965,
      2130005627,
      2132057084,
      2133013156,
      2133040789,
      2133120480,
      2134153324,
      2134845968,
      2137865376,
      2138497321,
      2140332127,
      2144446635,
      2147492008,
      2149126181,
      2153353285,
      2153947321,
      2154328025,
      2156493855,
      2158150115,
      2161795906,
      2163533082,
      2164114810,
      2164998010,
      2165792602,
      2166798247,
      2168939893,
      2169428619,
      2169743339,
      2169803171,
      2180809782,
      2181617391,
      2198041288,
      2341171179,
      2396715201,
      2397240726,
      2489939061,
      2964331425,
      3008721399,
      3011120880,
      3020831056,
      3022194887,
      3023407077
    ],
    "filter_matches": [
      "tr",
      "tl",
      "trl",
      "rl"
    ],
    "rank": 21370,
    "citation_count": 13,
    "estimated_citation_count": 13,
    "publication_date": "2007-07-22",
    "found_in": 1,
    "transfer_experiment_type": [
      "v",
      "a",
      "#",
      "t",
      "s_i",
      "s_f",
      "s",
      "r"
    ],
    "transfer_experiment_subtype": [
      "same_all",
      "multi",
      "diff-no",
      "diff-it",
      "lit"
    ],
    "transfer_data_type": [
      "pi",
      "rule",
      "advisor",
      "Q"
    ],
    "transfer_performance_metrics": [
      "j",
      "tt",
      "tr",
      "ap"
    ],
    "implementation": [
      "Custom",
      "CS",
      "Formulas",
      "Figures",
      "Tables",
      "Pseudo",
      "KeepAway",
      "Web",
      "Dead"
    ],
    "policy_type": [
      "Q",
      "SARSA",
      "NEAT",
      "FRM"
    ],
    "task_mappings": [
      "exp",
      "sup",
      "svg",
      "Ma"
    ],
    "autonomous_transfer": 1,
    "is_deep_rl": 0,
    "tags": [
      "Simulation",
      "2D",
      "3D",
      "2Dto3D",
      "MountainCar",
      "MountainCar3D",
      "ServerJob",
      "Scheduling",
      "KeepAway",
      "Ringworld",
      "KnightJoust",
      "MultiAgent",
      "ClassicControl",
      "RoboCup"
    ],
    "allowed_learner": [
      "TD",
      "PS",
      "MB",
      "RRL"
    ],
    "country": [
      "USA"
    ],
    "uni": [
      "The University of Texas"
    ],
    "department": [
      "Computer Science"
    ],
    "source_task_selection": [
      "h",
      "all",
      "lib",
      "mod"
    ],
    "was_in_survey": [
      "mag",
      "bone"
    ],
    "in_title": [
      "t",
      "r",
      "l"
    ],
    "in_abs": [
      "t",
      "r",
      "l",
      "k"
    ],
    "in_contet": [
      "t",
      "r",
      "l",
      "k",
      "s"
    ],
    "rejected_or_unpublished": null,
    "duplication_status": null,
    "paper_for_thesis": 1,
    "paper_available": 1,
    "behind_paywall": 0,
    "paywall_circumventable": 0,
    "author_names": [
      "Matthew D. Taylor"
    ]
  },
  {
    "id": 1481405077,
    "title": "Reinforcement learning transfer via common subspaces",
    "abst": "Agents in reinforcement learning tasks may learn slowly in large or complex tasks -- transfer learning is one technique to speed up learning by providing an informative prior. How to best enable transfer between tasks with different state representations and/or actions is currently an open question. This paper introduces the concept of a common task subspace, which is used to autonomously learn how two tasks are related. Experiments in two different nonlinear domains empirically show that a learned inter-state mapping can successfully be used by fitted value iteration, to (1) improving the performance of a policy learned with a fixed number of samples, and (2) reducing the time required to converge to a (near-) optimal policy with unlimited samples.",
    "url": "https://dblp.uni-trier.de/db/conf/atal/ala2011.html#Bou-AmmarT11",
    "lang": "en",
    "authors": [
      2148762994,
      2215740056
    ],
    "fos": [
      32834561,
      150899416,
      188116033,
      106189395,
      97541855,
      41008148,
      8038995,
      119857082,
      154945302,
      28006648,
      199190896
    ],
    "journals": [],
    "conferences": [
      65165642
    ],
    "conference_series": [
      2760724673
    ],
    "references": [
      36691172,
      1559271588,
      1583833196,
      1626155273,
      1689445748,
      1854866626,
      1969834017,
      2079247031,
      2097381042,
      2098723043,
      2110292307,
      2114537044,
      2121863487,
      2123995443,
      2133040789,
      2153353285,
      2492778706
    ],
    "filter_matches": [
      "tr",
      "tl",
      "trl",
      "rl"
    ],
    "rank": 21371,
    "citation_count": 9,
    "estimated_citation_count": 9,
    "publication_date": "2011-05-02",
    "found_in": 1,
    "transfer_experiment_type": [
      "v",
      "a",
      "#",
      "t"
    ],
    "transfer_experiment_subtype": [
      "lit"
    ],
    "transfer_data_type": [
      "fea",
      "sub"
    ],
    "transfer_performance_metrics": [
      "j",
      "tt"
    ],
    "implementation": [
      "Custom",
      "CS",
      "Formulas",
      "Pseudo",
      "Figures",
      "Tables"
    ],
    "policy_type": [
      "FVI"
    ],
    "task_mappings": [
      "exp"
    ],
    "autonomous_transfer": 1,
    "is_deep_rl": 0,
    "tags": [
      "2D",
      "CartPole",
      "Swingup",
      "MassSystem",
      "Simulation",
      "ClassicControl"
    ],
    "allowed_learner": [
      "TD"
    ],
    "country": [
      "Netherlands",
      "USA"
    ],
    "uni": [
      "Maastricht University",
      "Lafayette College"
    ],
    "department": [],
    "source_task_selection": [
      "h"
    ],
    "was_in_survey": [
      "mag",
      "zhu"
    ],
    "in_title": [
      "t",
      "r",
      "l"
    ],
    "in_abs": [
      "t",
      "r",
      "l"
    ],
    "in_contet": [
      "t",
      "r",
      "l",
      "k"
    ],
    "rejected_or_unpublished": null,
    "duplication_status": null,
    "paper_for_thesis": 0,
    "paper_available": 1,
    "behind_paywall": 1,
    "paywall_circumventable": 1,
    "author_names": [
      "Matthew D. Taylor",
      "Haitham Bou Ammar"
    ]
  },
  {
    "id": 2998488389,
    "title": "Compositional Transfer in Hierarchical Reinforcement Learning",
    "abst": "The successful application of general reinforcement learning algorithms to real-world robotics applications is often limited by their high data requirements. We introduce Regularized Hierarchical Policy Optimization (RHPO) to improve data-efficiency for domains with multiple dominant tasks and ultimately reduce required platform time. To this end, we employ compositional inductive biases on multiple levels and corresponding mechanisms for sharing off-policy transition data across low-level controllers and tasks as well as scheduling of tasks. The presented algorithm enables stable and fast learning for complex, real-world domains in the parallel multitask and sequential transfer case. We show that the investigated types of hierarchy enable positive transfer while partially mitigating negative interference and evaluate the benefits of additional incentives for efficient, compositional task solutions in single task domains. Finally, we demonstrate substantial data-efficiency and final performance gains over competitive baselines in a week-long, physical robot stacking experiment.",
    "url": "https://arxiv.org/abs/1906.11228",
    "lang": null,
    "authors": [
      8329356,
      224057516,
      1971811636,
      2003780605,
      2121442106,
      2200442376,
      2278261595,
      2935113903,
      2955655514,
      3028452433
    ],
    "fos": [
      154945302,
      33923547,
      119857082,
      33347731,
      97541855,
      47414280,
      113200698,
      90509273,
      34413123,
      163517525,
      31170391
    ],
    "journals": [
      2597173376
    ],
    "conferences": [],
    "conference_series": [],
    "references": [
      1522301498,
      1579853615,
      1585861384,
      1612541191,
      1757796397,
      1771410628,
      1959608418,
      2060277733,
      2097381042,
      2100677568,
      2109910161,
      2111967991,
      2145339207,
      2160371091,
      2165698076,
      2534060593,
      2547875792,
      2553882142,
      2594829461,
      2604636228,
      2735995851,
      2738129230,
      2763323349,
      2766447205,
      2785342287,
      2785738552,
      2785940258,
      2788781499,
      2803281228,
      2885163910,
      2894605519,
      2902286283,
      2911935295,
      2913340405,
      2921528247,
      2926028799,
      2950872548,
      2952012721,
      2952165242,
      2952581030,
      2955497741,
      2962799400,
      2963142324,
      2963674921,
      2963722050,
      2963912551,
      2963985863,
      2964001908,
      2964006217,
      2980113592
    ],
    "filter_matches": [
      "tr",
      "tl",
      "trl",
      "rl"
    ],
    "rank": 21412,
    "citation_count": 0,
    "estimated_citation_count": 0,
    "publication_date": "2019-09-25",
    "found_in": 1,
    "transfer_experiment_type": [
      "t",
      "#",
      "s_i",
      "s_f",
      "s"
    ],
    "transfer_experiment_subtype": [
      "multi",
      "sim2real"
    ],
    "transfer_data_type": [
      "off",
      "sub"
    ],
    "transfer_performance_metrics": [
      "tt",
      "tr",
      "j"
    ],
    "implementation": [
      "Custom",
      "CS",
      "Pseudo",
      "Formulas",
      "Figures",
      "TensorFlow",
      "Videos",
      "Web"
    ],
    "policy_type": [
      "RHPO",
      "SAC"
    ],
    "task_mappings": [
      "N/A"
    ],
    "autonomous_transfer": 0,
    "is_deep_rl": 1,
    "tags": [
      "3D",
      "Robotics",
      "Simulation",
      "RealWorld",
      "Lift",
      "Stack",
      "Pile"
    ],
    "allowed_learner": [
      "H",
      "TD"
    ],
    "country": [
      "UK"
    ],
    "uni": [
      "DeepMind"
    ],
    "department": [
      "Industry"
    ],
    "source_task_selection": [],
    "was_in_survey": [
      "mag"
    ],
    "in_title": [
      "t",
      "r",
      "l"
    ],
    "in_abs": [
      "t",
      "r",
      "l"
    ],
    "in_contet": [
      "t",
      "r",
      "l",
      "k",
      "s"
    ],
    "rejected_or_unpublished": null,
    "duplication_status": null,
    "paper_for_thesis": 0,
    "paper_available": 1,
    "behind_paywall": 0,
    "paywall_circumventable": 0,
    "author_names": [
      "Jost Tobias Springenberg",
      "Michael Neunert",
      "Thomas Lampe",
      "Martin Riedmiller",
      "Roland Hafner",
      "Markus Wulfmeier",
      "Nicolas Heess",
      "Tim Hertweck",
      "Noah Y. Siegel",
      "Abbas Abdolmaleki"
    ]
  },
  {
    "id": 2150385772,
    "title": "Reinforcement learning transfer based on subgoal discovery and subtask similarity",
    "abst": "This paper studies the problem of transfer learning in the context of reinforcement learning. We propose a novel transfer learning method that can speed up reinforcement learning with the aid of previously learnt tasks. Before performing extensive learning episodes, our method attempts to analyze the learning task via some exploration in the environment, and then attempts to reuse previous learning experience whenever it is possible and appropriate. In particular, our proposed method consists of four stages: 1) subgoal discovery, 2) option construction, 3) similarity searching, and 4) option reusing. Especially, in order to fulfill the task of identifying similar options, we propose a novel similarity measure between options, which is built upon the intuition that similar options have similar state-action probabilities. We examine our algorithm using extensive experiments, comparing it with existing methods. The results show that our method outperforms conventional non-transfer reinforcement learning algorithms, as well as existing transfer learning methods, by a wide margin.",
    "url": "https://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&amp;arnumber=7004683",
    "lang": "en",
    "authors": [
      2168026051,
      2231309555,
      2434140268,
      2594397125,
      2992037913
    ],
    "fos": [
      28006648,
      188888258,
      41008148,
      24138899,
      97541855,
      77967617,
      58973888,
      119857082,
      8038995,
      199190896,
      154945302
    ],
    "journals": [
      2484288132
    ],
    "conferences": [],
    "conference_series": [],
    "references": [
      36691172,
      1556824961,
      1564393562,
      2004030284,
      2090170171,
      2105833908,
      2109910161,
      2111625828,
      2114580749,
      2115403315,
      2121863487,
      2132057084,
      2143435603,
      2158150115
    ],
    "filter_matches": [
      "tr",
      "tl",
      "trl",
      "rl"
    ],
    "rank": 21416,
    "citation_count": 6,
    "estimated_citation_count": 6,
    "publication_date": "2014-07-01",
    "found_in": 1,
    "transfer_experiment_type": [
      "s",
      "t",
      "levels"
    ],
    "transfer_experiment_subtype": [
      "multi"
    ],
    "transfer_data_type": [
      "sub",
      "options"
    ],
    "transfer_performance_metrics": [
      "j",
      "tt"
    ],
    "implementation": [
      "Custom",
      "CS",
      "Pseudo",
      "Formulas",
      "Figures",
      "Tables"
    ],
    "policy_type": [
      "Q",
      "SARSA",
      "SDHRL"
    ],
    "task_mappings": [
      "N/A"
    ],
    "autonomous_transfer": 0,
    "is_deep_rl": 0,
    "tags": [
      "2D",
      "Navigation",
      "Maze",
      "Grid",
      "Simulation"
    ],
    "allowed_learner": [
      "TD",
      "H"
    ],
    "country": [
      "China"
    ],
    "uni": [
      "Nanjing University"
    ],
    "department": [
      "Computer Science and Technology"
    ],
    "source_task_selection": [
      "h"
    ],
    "was_in_survey": [
      "mag"
    ],
    "in_title": [
      "t",
      "r",
      "l"
    ],
    "in_abs": [
      "t",
      "r",
      "l"
    ],
    "in_contet": [
      "t",
      "r",
      "l",
      "k"
    ],
    "rejected_or_unpublished": null,
    "duplication_status": null,
    "paper_for_thesis": 0,
    "paper_available": 1,
    "behind_paywall": 0,
    "paywall_circumventable": 0,
    "author_names": [
      "Xingguo Chen",
      "Jinhua Song",
      "Shunguo Fan",
      "Hao Wang",
      "Yang Gao"
    ]
  },
  {
    "id": 3017464978,
    "title": "Sonic to knuckles: Evaluations on transfer reinforcement learning",
    "abst": "Reinforcement Learning holds the potential to enable many systems with rapid, intelligent automated decision- making. However, reinforcement learning on embodied systems is a much greater challenge than the simulated environments and tasks which have been solved to date. A learner in an embodied system cannot run millions of trials or easily tolerate fatal trajectories. Therefore, the ability to train agents in simulated environments and effectively transfer their knowledge to real-world environments will be crucial, and likely an integral part of constructing future robotic systems. We perform experiments in an original transfer reinforcement learning task we constructed using the game &#x201C;Sonic 3 and Knuckles,&quot; evaluating two transfer learning techniques from the literature.",
    "url": "https://www.spiedigitallibrary.org/conference-proceedings-of-spie/11425/114250J/Sonic-to-knuckles-Evaluations-on-transfer-reinforcement-learning/10.1117/12.2559546.full",
    "lang": null,
    "authors": [
      2622447406,
      2681369031,
      2947430287,
      2963738381,
      3018643043,
      3020595862
    ],
    "fos": [
      108583219,
      97541855,
      150899416,
      154945302,
      2982816539,
      100609095,
      107457646,
      41008148
    ],
    "journals": [],
    "conferences": [],
    "conference_series": [],
    "references": [],
    "filter_matches": [
      "tr",
      "tl",
      "trl",
      "rl"
    ],
    "rank": 21416,
    "citation_count": 0,
    "estimated_citation_count": 0,
    "publication_date": "2020-04-23",
    "found_in": 1,
    "transfer_experiment_type": [],
    "transfer_experiment_subtype": [],
    "transfer_data_type": [],
    "transfer_performance_metrics": [],
    "implementation": [
      "Custom",
      "CS"
    ],
    "policy_type": [
      "PPO"
    ],
    "task_mappings": [],
    "autonomous_transfer": 2,
    "is_deep_rl": 1,
    "tags": [
      "Control System",
      "Games",
      "VideoGames",
      "Sonic3",
      "Simulation",
      "Military"
    ],
    "allowed_learner": [
      "TD"
    ],
    "country": [
      "USA"
    ],
    "uni": [
      "Vanderbilt University",
      "Ohio University",
      "Air Force Research Lab"
    ],
    "department": [],
    "source_task_selection": [],
    "was_in_survey": [
      "mag"
    ],
    "in_title": [
      "t",
      "r",
      "l"
    ],
    "in_abs": [
      "t",
      "r",
      "l",
      "k"
    ],
    "in_contet": [],
    "rejected_or_unpublished": null,
    "duplication_status": null,
    "paper_for_thesis": 0,
    "paper_available": 0,
    "behind_paywall": 1,
    "paywall_circumventable": 0,
    "author_names": [
      "Todd Jenkins",
      "Nathaniel Hamilton",
      "Chad Waddington",
      "Taylor T. Johnson",
      "Lena Schlemmer",
      "Christopher Menart"
    ]
  },
  {
    "id": 2887063207,
    "title": "Intelligent Land-Vehicle Model Transfer Trajectory Planning Method Based on Deep Reinforcement Learning.",
    "abst": ": To address the problem of model error and tracking dependence in the process of intelligent vehicle motion planning, an intelligent vehicle model transfer trajectory planning method based on deep reinforcement learning is proposed, which is able to obtain an effective control action sequence directly. Firstly, an abstract model of the real environment is extracted. On this basis, a deep deterministic policy gradient (DDPG) and a vehicle dynamic model are adopted to jointly train a reinforcement learning model, and to decide the optimal intelligent driving maneuver. Secondly, the actual scene is transferred to an equivalent virtual abstract scene using a transfer model. Furthermore, the control action and trajectory sequences are calculated according to the trained deep reinforcement learning model. Thirdly, the optimal trajectory sequence is selected according to an evaluation function in the real environment. Finally, the results demonstrate that the proposed method can deal with the problem of intelligent vehicle trajectory planning for continuous input and continuous output. The model transfer method improves the model&#039;s generalization performance. Compared with traditional trajectory planning, the proposed method outputs continuous rotation-angle control sequences. Moreover, the lateral control errors are also reduced.",
    "url": "https://www.preprints.org/manuscript/201808.0049/v1/download",
    "lang": "en",
    "authors": [
      2653733146,
      2704938613,
      2789945711,
      2887744335
    ],
    "fos": [
      13662910,
      97541855,
      127413603,
      74296488,
      65244806,
      2991987220,
      24326235,
      131806220,
      2988528222,
      179024874,
      81074085
    ],
    "journals": [
      101949793
    ],
    "conferences": [],
    "conference_series": [],
    "references": [],
    "filter_matches": [
      "tr",
      "tl",
      "trl",
      "rl"
    ],
    "rank": 21416,
    "citation_count": 5,
    "estimated_citation_count": 5,
    "publication_date": "2018-08-02",
    "found_in": 1,
    "transfer_experiment_type": [
      "t"
    ],
    "transfer_experiment_subtype": [
      "sim2real",
      "multi",
      "lit"
    ],
    "transfer_data_type": [
      "model"
    ],
    "transfer_performance_metrics": [],
    "implementation": [
      "Custom",
      "CS",
      "Pseudo",
      "Formulas",
      "Figures",
      "TensorFlow"
    ],
    "policy_type": [
      "DDPG",
      "DRL-MTTP",
      "DQN"
    ],
    "task_mappings": [
      "N/A"
    ],
    "autonomous_transfer": 0,
    "is_deep_rl": 1,
    "tags": [
      "3D",
      "Navigation",
      "Driving",
      "Trajectory",
      "Planning",
      "Autonomous",
      "Mobility",
      "Simulation",
      "RealWorld",
      "Vehicle"
    ],
    "allowed_learner": [
      "TD"
    ],
    "country": [
      "China"
    ],
    "uni": [
      "Central South University",
      "Harbin Institute of Technology",
      "Chongqing University",
      "Hunan University of Commerce"
    ],
    "department": [
      "Information Science and Engineering",
      "State Key Laboratory of Robotics and System",
      "State Key Laboratory of Mechanical Transmissions",
      "School of Computer and Information Engineering"
    ],
    "source_task_selection": [
      "all"
    ],
    "was_in_survey": [
      "mag"
    ],
    "in_title": [
      "t",
      "r",
      "l"
    ],
    "in_abs": [
      "t",
      "r",
      "l"
    ],
    "in_contet": [
      "t",
      "r",
      "l"
    ],
    "rejected_or_unpublished": null,
    "duplication_status": null,
    "paper_for_thesis": 0,
    "paper_available": 1,
    "behind_paywall": 0,
    "paywall_circumventable": 0,
    "author_names": [
      "Kaijun Zhou",
      "Lingli Yu",
      "Xuanya Shao",
      "Yadong Wei"
    ]
  },
  {
    "id": 1931215804,
    "title": "Transferring evolved reservoir features in reinforcement learning tasks",
    "abst": "The major goal of transfer learning is to transfer knowledge acquired on a source task in order to facilitate learning on another, different, but usually related, target task. In this paper, we are using neuroevolution to evolve echo state networks on the source task and transfer the best performing reservoirs to be used as initial population on the target task. The idea is that any non-linear, temporal features, represented by the neurons of the reservoir and evolved on the source task, along with reservoir properties, will be a good starting point for a stochastic search on the target task. In a step towards full autonomy and by taking advantage of the random and fully connected nature of echo state networks, we examine a transfer method that renders any inter-task mappings of states and actions unnecessary. We tested our approach and that of inter-task mappings in two RL testbeds: the mountain car and the server job scheduling domains. Under various setups the results we obtained in both cases are promising.",
    "url": "https://rd.springer.com/chapter/10.1007%2F978-3-642-29946-9_22",
    "lang": "en",
    "authors": [
      127847919,
      272889888,
      338035643,
      695239088
    ],
    "fos": [
      119857082,
      2908647359,
      41008148,
      150899416,
      154945302,
      111873713,
      97541855,
      118070581
    ],
    "journals": [],
    "conferences": [
      37046440
    ],
    "conference_series": [
      2757237056
    ],
    "references": [
      1486382900,
      1487385582,
      1506146479,
      1515851193,
      1854866626,
      1923871101,
      2014512216,
      2031727428,
      2054217036,
      2097381042,
      2103048296,
      2110292307,
      2111935653,
      2113913482,
      2116339921,
      2121863487,
      2123850191,
      2154328025,
      2158150115,
      2163992944,
      2175288397,
      2795855215
    ],
    "filter_matches": [
      "tr",
      "tl",
      "trl",
      "rl"
    ],
    "rank": 21452,
    "citation_count": 6,
    "estimated_citation_count": 6,
    "publication_date": "2011-09-09",
    "found_in": 1,
    "transfer_experiment_type": [
      "v"
    ],
    "transfer_experiment_subtype": [
      "lit"
    ],
    "transfer_data_type": [
      "fea"
    ],
    "transfer_performance_metrics": [
      "j",
      "tr",
      "tt"
    ],
    "implementation": [
      "Custom",
      "CS",
      "Formulas",
      "Figures",
      "Tables",
      "RL-Glue",
      "RL-Glue(deadlinks)"
    ],
    "policy_type": [
      "NEAT"
    ],
    "task_mappings": [
      "exp"
    ],
    "autonomous_transfer": 0,
    "is_deep_rl": 1,
    "tags": [
      "Simulation",
      "MountainCar",
      "2D",
      "3D",
      "2Dto3D",
      "Scheduling",
      "ServerJob",
      "ServerJobScheduling",
      "ClassicControl"
    ],
    "allowed_learner": [
      "TD",
      "PS"
    ],
    "country": [
      "Greece"
    ],
    "uni": [
      "Aristotle University of Thessaloniki",
      "Centre for Research and Technology Hellas"
    ],
    "department": [
      "Electrical & Computer Engineering",
      "Informatics and Telematics"
    ],
    "source_task_selection": [
      "h"
    ],
    "was_in_survey": [
      "mag"
    ],
    "in_title": [
      "t",
      "r",
      "l"
    ],
    "in_abs": [
      "t",
      "r",
      "l",
      "k"
    ],
    "in_contet": [
      "t",
      "r",
      "l",
      "k"
    ],
    "rejected_or_unpublished": null,
    "duplication_status": null,
    "paper_for_thesis": 0,
    "paper_available": 1,
    "behind_paywall": 0,
    "paywall_circumventable": 0,
    "author_names": [
      "Ioannis Partalas",
      "Kyriakos C. Chatzidimitriou",
      "Pericles A. Mitkas",
      "Ioannis Vlahavas"
    ]
  },
  {
    "id": 2808217720,
    "title": "Knowledge transfer in reinforcement learning",
    "abst": null,
    "url": "https://www.didaktorika.gr/eadd/handle/10442/38511",
    "lang": "el",
    "authors": [
      222830299,
      2808696655
    ],
    "fos": [
      2776960227,
      3018945416,
      150899416,
      41008148,
      97541855,
      145420912
    ],
    "journals": [],
    "conferences": [],
    "conference_series": [],
    "references": [
      24477102,
      1499408472,
      1562462135,
      1586944634,
      1777239053,
      1982948368,
      1996847178,
      2009533501,
      2021061679,
      2048226872,
      2071311198,
      2114451917,
      2121863487,
      2128905965,
      2130750514,
      2137351138,
      2143958939,
      2556741007,
      2914048451
    ],
    "filter_matches": [
      "tr",
      "tl",
      "trl",
      "rl"
    ],
    "rank": 21485,
    "citation_count": 10,
    "estimated_citation_count": 10,
    "publication_date": "2016-01-01",
    "found_in": 1,
    "transfer_experiment_type": [
      "s_i",
      "s_f",
      "t",
      "r"
    ],
    "transfer_experiment_subtype": [
      "same_all",
      "multi"
    ],
    "transfer_data_type": [
      "I",
      "fea"
    ],
    "transfer_performance_metrics": [
      "j",
      "tr",
      "ap"
    ],
    "implementation": [
      "Custom",
      "CS",
      "Pseudo",
      "Figures",
      "Tables",
      "Formulas"
    ],
    "policy_type": [
      "Q",
      "MAXQ",
      "FQI",
      "LSPI",
      "LSTDQ"
    ],
    "task_mappings": [
      "N/A"
    ],
    "autonomous_transfer": 0,
    "is_deep_rl": 0,
    "tags": [
      "MiniGolf",
      "MountainCar",
      "Boat",
      "2D",
      "Navigation",
      "Simulation",
      "ClassicControl"
    ],
    "allowed_learner": [
      "batch",
      "H",
      "B",
      "TD"
    ],
    "country": [
      "Italy"
    ],
    "uni": [
      "Politecnico di Milano"
    ],
    "department": [
      "Electronics and Information"
    ],
    "source_task_selection": [
      "lib",
      "all"
    ],
    "was_in_survey": [
      "taylor",
      "lazaric",
      "mag"
    ],
    "in_title": [
      "t",
      "r",
      "l",
      "k"
    ],
    "in_abs": [
      "t",
      "r",
      "l",
      "k"
    ],
    "in_contet": [
      "t",
      "r",
      "l",
      "k"
    ],
    "rejected_or_unpublished": null,
    "duplication_status": null,
    "paper_for_thesis": 1,
    "paper_available": 1,
    "behind_paywall": 1,
    "paywall_circumventable": 0,
    "author_names": [
      "Anestis Fachantidis",
      "&#x0391;&#x03BD;&#x03AD;&#x03C3;&#x03C4;&#x03B7;&#x03C2; &#x03A6;&#x03B1;&#x03C7;&#x03B1;&#x03BD;&#x03C4;&#x03AF;&#x03B4;&#x03B7;&#x03C2;"
    ]
  },
  {
    "id": 2940691063,
    "title": "Human Causal Transfer: Challenges for Deep Reinforcement Learning.",
    "abst": null,
    "url": "https://mindmodeling.org/cogsci2018/papers/0080/index.html",
    "lang": null,
    "authors": [
      2113246774,
      2228814312,
      2343632159,
      2407361859,
      2596709214,
      2941197329,
      2953831643
    ],
    "fos": [
      180747234,
      97541855,
      15744967
    ],
    "journals": [
      78735424
    ],
    "conferences": [],
    "conference_series": [],
    "references": [],
    "filter_matches": [
      "tr",
      "tl",
      "trl",
      "rl"
    ],
    "rank": 21514,
    "citation_count": 2,
    "estimated_citation_count": 2,
    "publication_date": "2018-01-01",
    "found_in": 1,
    "transfer_experiment_type": [],
    "transfer_experiment_subtype": [
      "psychology",
      "pure"
    ],
    "transfer_data_type": [],
    "transfer_performance_metrics": [],
    "implementation": [
      "Custom",
      "CS",
      "Gym",
      "Videos",
      "Formulas",
      "Figures"
    ],
    "policy_type": [
      "DDQN"
    ],
    "task_mappings": [],
    "autonomous_transfer": 0,
    "is_deep_rl": 1,
    "tags": [
      "2D",
      "Navigation",
      "Grid",
      "Simulation",
      "Human",
      "Causal",
      "Comparison"
    ],
    "allowed_learner": [
      "TD"
    ],
    "country": [
      "USA"
    ],
    "uni": [
      "UCLA",
      "Caltech",
      "UW"
    ],
    "department": [
      "Computer Science",
      "Psychology",
      "Jet Propulsion Laboratory",
      "Statistics"
    ],
    "source_task_selection": [],
    "was_in_survey": [
      "mag"
    ],
    "in_title": [
      "t",
      "r",
      "l"
    ],
    "in_abs": [
      "t",
      "r",
      "l",
      "k"
    ],
    "in_contet": [
      "t",
      "r",
      "l",
      "k"
    ],
    "rejected_or_unpublished": null,
    "duplication_status": null,
    "paper_for_thesis": 0,
    "paper_available": 1,
    "behind_paywall": 0,
    "paywall_circumventable": 0,
    "author_names": [
      "Hongjing Lu",
      "Yixin Zhu",
      "Song-Chun Zhu",
      "James Kubricht",
      "Brandon Rothrock",
      "Colin Summers",
      "Mark Edmonds"
    ]
  },
  {
    "id": 2125946670,
    "title": "A framework for the adaptive transfer of robot skill knowledge using reinforcement learning agents",
    "abst": "A framework, called skill advice guided exploration (SAGE), for the adaptive transfer of robot skill knowledge using reinforcement learning (RL) agents is presented. A skill is viewed as a reactive policy which maps world states to agent actions. It may be acquired via learning or it may be hand-coded by the designer. The SAGE framework allows multiple, possibly conflicting, sources of knowledge to be incorporated simultaneously. An abstraction for knowledge in an RL system, called advice, is introduced. The advice abstraction permits the transfer of information between RL agents with differing internal representations. A SAGE-based system can learn to disregard misleading advice. The potential of this methodology is demonstrated on a set of discrete learning tasks. Results show that SAGE-based systems can benefit from relevant information and that incorrect information does not prevent learning of the task solution. The benefits, limitations, and possible extensions of this work are discussed.",
    "url": "https://dblp.uni-trier.de/db/conf/icra/icra2001.html#MalakK01",
    "lang": null,
    "authors": [
      2105288403,
      2973316630
    ],
    "fos": [
      97541855,
      161301231,
      90509273,
      5894958,
      84685590,
      154945302,
      124304363,
      52970973,
      2776960227,
      41008148
    ],
    "journals": [],
    "conferences": [],
    "conference_series": [
      1163902177
    ],
    "references": [
      1540685400,
      1576452626,
      1982948368,
      2012036715,
      2141559645,
      3011120880,
      3022194887
    ],
    "filter_matches": [
      "tr",
      "tl",
      "trl",
      "rl"
    ],
    "rank": 21514,
    "citation_count": 13,
    "estimated_citation_count": 13,
    "publication_date": "2001-05-21",
    "found_in": 1,
    "transfer_experiment_type": [
      "s_i",
      "s_f",
      "s",
      "levels"
    ],
    "transfer_experiment_subtype": [
      "multi"
    ],
    "transfer_data_type": [
      "advisor"
    ],
    "transfer_performance_metrics": [
      "tt"
    ],
    "implementation": [
      "Custom",
      "CS",
      "Formulas",
      "Figures"
    ],
    "policy_type": [
      "Q"
    ],
    "task_mappings": [
      "exp"
    ],
    "autonomous_transfer": 0,
    "is_deep_rl": 0,
    "tags": [
      "2D",
      "Navigation",
      "Grid",
      "Maze",
      "Robotics",
      "Simulation"
    ],
    "allowed_learner": [
      "TD"
    ],
    "country": [
      "USA"
    ],
    "uni": [
      "Carnegie Mellon University"
    ],
    "department": [
      "Electrical and Computer Engineering"
    ],
    "source_task_selection": [
      "lib"
    ],
    "was_in_survey": [
      "mag"
    ],
    "in_title": [
      "t",
      "r",
      "l",
      "k",
      "s"
    ],
    "in_abs": [
      "t",
      "r",
      "l",
      "k",
      "s"
    ],
    "in_contet": [
      "t",
      "r",
      "l",
      "k",
      "s"
    ],
    "rejected_or_unpublished": null,
    "duplication_status": null,
    "paper_for_thesis": 0,
    "paper_available": 1,
    "behind_paywall": 0,
    "paywall_circumventable": 0,
    "author_names": [
      "R.J. Malak",
      "P.K. Khosla"
    ]
  },
  {
    "id": 2398715346,
    "title": "Advice Taking and Transfer Learning: Naturally Inspired Extensions to Reinforcement Learning.",
    "abst": "Reinforcement learning (RL) is a machine learning technique with strong links to natural learning. However, it shares several &#x201C;unnatural&#x201D; limitations with many other successful machine learning algorithms. RL agents are not typically able to take advice or to adjust to new situations beyond the specific problem they are asked to learn. Due to limitations like these, RL remains slower and less adaptable than natural learning. Our recent work focuses on extending RL to include the naturally inspired abilities of advice taking and transfer learning. Through experiments in the RoboCup domain, we show that doing so can make RL faster and more adaptable.",
    "url": "https://experts.umn.edu/en/publications/advice-taking-and-transfer-learning-naturally-inspired-extensions",
    "lang": "en",
    "authors": [
      738944226,
      2047441381,
      2079278047,
      2120363087
    ],
    "fos": [
      97541855,
      47932503,
      150899416,
      119857082,
      154945302,
      41008148,
      77075516
    ],
    "journals": [],
    "conferences": [
      2786958024
    ],
    "conference_series": [
      1184914352
    ],
    "references": [
      198956113,
      1490954610,
      1506146479,
      1512137381,
      1535439311,
      1976115983,
      1977970897,
      2056102643,
      2100677568,
      2121863487,
      2122982548,
      2128905965,
      2140584963,
      2155791599,
      2156493855,
      2164114810,
      3011120880,
      3022194887
    ],
    "filter_matches": [
      "tr",
      "tl",
      "trl",
      "rl"
    ],
    "rank": 21518,
    "citation_count": 2,
    "estimated_citation_count": 2,
    "publication_date": "2008-12-01",
    "found_in": 1,
    "transfer_experiment_type": [
      "#"
    ],
    "transfer_experiment_subtype": [
      "multi"
    ],
    "transfer_data_type": [
      "rule",
      "advisor"
    ],
    "transfer_performance_metrics": [
      "j"
    ],
    "implementation": [
      "Custom",
      "CS",
      "Formulas",
      "Figures",
      "Tables"
    ],
    "policy_type": [
      "Q"
    ],
    "task_mappings": [
      "N/A"
    ],
    "autonomous_transfer": 0,
    "is_deep_rl": 0,
    "tags": [
      "2D",
      "Navigation",
      "RoboCup",
      "BreakAway",
      "MultiAgent",
      "Simulation",
      "3v2",
      "2v1"
    ],
    "allowed_learner": [
      "TD",
      "RRL"
    ],
    "country": [
      "USA"
    ],
    "uni": [
      "University of Wisconsin",
      "University of Minnesota"
    ],
    "department": [
      "Computer Sciences"
    ],
    "source_task_selection": [],
    "was_in_survey": [
      "mag"
    ],
    "in_title": [
      "t",
      "r",
      "l"
    ],
    "in_abs": [
      "t",
      "l"
    ],
    "in_contet": [
      "t",
      "r",
      "l",
      "k",
      "s"
    ],
    "rejected_or_unpublished": null,
    "duplication_status": null,
    "paper_for_thesis": 0,
    "paper_available": 1,
    "behind_paywall": 0,
    "paywall_circumventable": 0,
    "author_names": [
      "Lisa Torrey",
      "Richard Maclin",
      "Jude W. Shavlik",
      "Trevor Walker"
    ]
  },
  {
    "id": 1573527757,
    "title": "Transfer of task representation in reinforcement learning using policy-based proto-value functions",
    "abst": "Reinforcement Learning research is traditionally devoted to solve single-task problems. Therefore, anytime a new task is faced, learning must be restarted from scratch. Recently, several studies have addressed the issue of reusing the knowledge acquired in solving previous related tasks by transferring information about policies and value functions. In this paper, we analyze the use of proto-value functions under the transfer learning perspective. Proto-value functions are effective basis functions for the approximation of value functions defined over the graph obtained by a random walk on the environment. The definition of this graph is a key aspect in transfer transfer problems in which both the reward function and the dynamics change. Therefore, we introduce policy-based proto-value functions, which can be obtained by considering the graph generated by a random walk guided by the optimal policy of one of the tasks at hand. We compare the effectiveness of policy-based and standard proto-value functions, on different transfer problems defined on a simple grid-world environment.",
    "url": "http://doi.acm.org/10.1145/1402821.1402864",
    "lang": "en",
    "authors": [
      131065259,
      2103877068,
      2106323334
    ],
    "fos": [
      41008148,
      74003402,
      28006648,
      77075516,
      150899416,
      119857082,
      80444323,
      121194460,
      5917680,
      154945302,
      2777965961,
      97541855
    ],
    "journals": [],
    "conferences": [
      2787693658
    ],
    "conference_series": [
      1168671587
    ],
    "references": [
      115446000,
      143164768,
      1486707268,
      1492014007,
      1510402218,
      1515851193,
      1536990779,
      1578099820,
      1601389419,
      1607318605,
      1968768508,
      1980269662,
      2079247031,
      2098723043,
      2106953752,
      2121517924,
      2121863487,
      2125710293,
      2128905965,
      2130005627,
      2132057084,
      2133040789,
      2143435603,
      2145454741,
      2154328025,
      2161795906,
      2165792602,
      2170563643
    ],
    "filter_matches": [
      "tr",
      "tl",
      "trl",
      "rl"
    ],
    "rank": 21540,
    "citation_count": 16,
    "estimated_citation_count": 16,
    "publication_date": "2008-05-12",
    "found_in": 1,
    "transfer_experiment_type": [
      "s_f",
      "t"
    ],
    "transfer_experiment_subtype": [
      "multi"
    ],
    "transfer_data_type": [
      "pvf"
    ],
    "transfer_performance_metrics": [
      "j",
      "tr",
      "ap",
      "tt"
    ],
    "implementation": [
      "Custom",
      "CS",
      "Formulas",
      "Pseudo",
      "Figures",
      "Tables"
    ],
    "policy_type": [
      "Q"
    ],
    "task_mappings": [
      "N/A"
    ],
    "autonomous_transfer": 0,
    "is_deep_rl": 0,
    "tags": [
      "2D",
      "Navigation",
      "Simulation",
      "Grid",
      "ThreeRoomMaze"
    ],
    "allowed_learner": [
      "TD"
    ],
    "country": [
      "Italy"
    ],
    "uni": [
      "Politecnico di Milano"
    ],
    "department": [],
    "source_task_selection": [],
    "was_in_survey": [
      "mag",
      "lazaric"
    ],
    "in_title": [
      "t",
      "r",
      "l"
    ],
    "in_abs": [
      "t",
      "r",
      "l",
      "k"
    ],
    "in_contet": [
      "t",
      "r",
      "l",
      "k",
      "s"
    ],
    "rejected_or_unpublished": null,
    "duplication_status": null,
    "paper_for_thesis": 0,
    "paper_available": 1,
    "behind_paywall": 0,
    "paywall_circumventable": 0,
    "author_names": [
      "Alessandro Lazaric",
      "Marcello Restelli",
      "Eliseo Ferrante"
    ]
  },
  {
    "id": 1255659923,
    "title": "Fixed vs. Dynamic Sub-Transfer in Reinforcement Learning.",
    "abst": "We survey various task transfer methods in Qlearning and present a variation on fixed sub-transfer which we call dynamic sub-transfer. We discuss the benefits and drawbacks of dynamic sub-transfer as compared with the other transfer methods, and we describe qualitatively the situations where this method would be preferred over the fixed version of sub-transfer. We test this method against several other transfer methods in a simple three room grid world where portions of the source&#x2019;s policy are relevant to the target task and other portions are not. In this situation we found that dynamic sub-transfer converged to the optimal solution, avoiding the suboptimality inherent in fixed sub-transfer, while also avoiding some of the convergence problems often experienced by fixed sub-transfer.",
    "url": "https://dblp.uni-trier.de/db/conf/icmla/icmla2002.html#CarrollP02",
    "lang": null,
    "authors": [
      2169019634,
      2607676574
    ],
    "fos": [
      97541855,
      187691185,
      41008148,
      71923881,
      119857082,
      47932503,
      154945302
    ],
    "journals": [],
    "conferences": [],
    "conference_series": [
      1138691947
    ],
    "references": [
      1258105458,
      1514621373,
      1853223271,
      1991564165,
      1996847178,
      2114451917,
      2122451452,
      2914746235
    ],
    "filter_matches": [
      "tr",
      "tl",
      "trl",
      "rl"
    ],
    "rank": 21546,
    "citation_count": 13,
    "estimated_citation_count": 13,
    "publication_date": "2002-01-01",
    "found_in": 1,
    "transfer_experiment_type": [
      "s_i",
      "s_f",
      "s",
      "levels"
    ],
    "transfer_experiment_subtype": [
      "multi"
    ],
    "transfer_data_type": [
      "pi_fix",
      "pi",
      "pi_dyn"
    ],
    "transfer_performance_metrics": [
      "tr",
      "ap",
      "j"
    ],
    "implementation": [
      "Custom",
      "CS",
      "Formulas",
      "Figures"
    ],
    "policy_type": [
      "Q"
    ],
    "task_mappings": [
      "N/A"
    ],
    "autonomous_transfer": 0,
    "is_deep_rl": 0,
    "tags": [
      "2D",
      "Navigation",
      "Simulation",
      "Maze",
      "Grid",
      "ThreeRoomMaze"
    ],
    "allowed_learner": [
      "TD"
    ],
    "country": [
      "USA"
    ],
    "uni": [
      "Brigham Young University"
    ],
    "department": [],
    "source_task_selection": [],
    "was_in_survey": [
      "mag"
    ],
    "in_title": [
      "t",
      "r",
      "l"
    ],
    "in_abs": [
      "t",
      "l"
    ],
    "in_contet": [
      "t",
      "r",
      "l",
      "s"
    ],
    "rejected_or_unpublished": null,
    "duplication_status": null,
    "paper_for_thesis": 0,
    "paper_available": 1,
    "behind_paywall": 0,
    "paywall_circumventable": 0,
    "author_names": [
      "James L. Carroll",
      "Todd Peterson"
    ]
  },
  {
    "id": 2945961113,
    "title": "Effects of Task Similarity on Policy Transfer with Selective Exploration in Reinforcement Learning",
    "abst": "The SEAPoT algorithm [9] is a knowledge transfer mechanism in model-based reinforcement learning. By constructing subspaces around the changed regions, and selectively and efficiently exploring the target task, the transfer is most effective when the source and target tasks share similar objectives but differ in the transition dynamics. In this work, we identify the similarity between tasks using a new light-weight metric, based on the Jensen-Shannon distance, and show how the degree of similarity affects the transfer efficacy. We also empirically show that SEAPoT performs better in terms of jump starts and average rewards, as compared to the state-of-the-art policy reuse methods.",
    "url": "https://dl.acm.org/citation.cfm?id=3332034",
    "lang": "en",
    "authors": [
      2172797452,
      2228911865
    ],
    "fos": [
      41008148,
      2776960227,
      80444323,
      154945302,
      97541855,
      119857082,
      2780695682,
      2987306524,
      206588197,
      12362212,
      2776731479
    ],
    "journals": [],
    "conferences": [
      2942726028
    ],
    "conference_series": [
      1168671587
    ],
    "references": [],
    "filter_matches": [
      "tr",
      "tl",
      "trl",
      "rl"
    ],
    "rank": 21552,
    "citation_count": 0,
    "estimated_citation_count": 0,
    "publication_date": "2019-05-08",
    "found_in": 1,
    "transfer_experiment_type": [
      "t",
      "s_i",
      "s_f",
      "s",
      "levels"
    ],
    "transfer_experiment_subtype": [
      "multi"
    ],
    "transfer_data_type": [
      "model"
    ],
    "transfer_performance_metrics": [
      "j",
      "tr"
    ],
    "implementation": [
      "Custom",
      "CS",
      "Formulas",
      "Figures"
    ],
    "policy_type": [
      "Q"
    ],
    "task_mappings": [
      "N/A"
    ],
    "autonomous_transfer": 0,
    "is_deep_rl": 0,
    "tags": [
      "2D",
      "Navigation",
      "Simulation",
      "Taxi",
      "Grid"
    ],
    "allowed_learner": [
      "TD",
      "MB"
    ],
    "country": [
      "Singapore"
    ],
    "uni": [
      "National Unversity of Singapore"
    ],
    "department": [
      "Computing"
    ],
    "source_task_selection": [
      "lib"
    ],
    "was_in_survey": [
      "mag"
    ],
    "in_title": [
      "t",
      "r",
      "l"
    ],
    "in_abs": [
      "t",
      "r",
      "l",
      "k"
    ],
    "in_contet": [
      "t",
      "r",
      "l",
      "k"
    ],
    "rejected_or_unpublished": null,
    "duplication_status": null,
    "paper_for_thesis": 0,
    "paper_available": 1,
    "behind_paywall": 0,
    "paywall_circumventable": 0,
    "author_names": [
      "Tze-Yun Leong",
      "Akshay Narayan"
    ]
  },
  {
    "id": 2885067833,
    "title": "Concept-Aware Feature Extraction for Knowledge Transfer in Reinforcement Learning.",
    "abst": null,
    "url": "https://dblp.uni-trier.de/db/conf/aaai/aaai2018w.html#Winderd18",
    "lang": null,
    "authors": [
      2126990796,
      2505127676
    ],
    "fos": [
      119857082,
      154945302,
      52622490,
      2776960227,
      97541855,
      41008148
    ],
    "journals": [],
    "conferences": [
      2784145466
    ],
    "conference_series": [
      1184914352
    ],
    "references": [],
    "filter_matches": [
      "tr",
      "tl",
      "trl",
      "rl"
    ],
    "rank": 21556,
    "citation_count": 1,
    "estimated_citation_count": 1,
    "publication_date": "2018-01-01",
    "found_in": 1,
    "transfer_experiment_type": [],
    "transfer_experiment_subtype": [
      "unavailable"
    ],
    "transfer_data_type": [
      "fea"
    ],
    "transfer_performance_metrics": [],
    "implementation": [],
    "policy_type": [],
    "task_mappings": [
      "exp"
    ],
    "autonomous_transfer": 0,
    "is_deep_rl": 2,
    "tags": [
      "ConceptAware",
      "Features",
      "FeatureExtraction"
    ],
    "allowed_learner": [],
    "country": [],
    "uni": [],
    "department": [],
    "source_task_selection": [],
    "was_in_survey": [
      "mag"
    ],
    "in_title": [],
    "in_abs": [],
    "in_contet": [],
    "rejected_or_unpublished": null,
    "duplication_status": null,
    "paper_for_thesis": 0,
    "paper_available": 0,
    "behind_paywall": 0,
    "paywall_circumventable": 0,
    "author_names": [
      "Marie desJardins",
      "John Winder"
    ]
  },
  {
    "id": 2991117878,
    "title": "Playing Games in the Dark: An approach for cross-modality transfer in reinforcement learning.",
    "abst": "In this work we explore the use of latent representations obtained from multiple input sensory modalities (such as images or sounds) in allowing an agent to learn and exploit policies over different subsets of input modalities. We propose a three-stage architecture that allows a reinforcement learning agent trained over a given sensory modality, to execute its task on a different sensory modality-for example, learning a visual policy over image inputs, and then execute such policy when only sound inputs are available. We show that the generalized policies achieve better out-of-the-box performance when compared to different baselines. Moreover, we show this holds in different OpenAI gym and video game environments, even when using different multimodal generative models and reinforcement learning algorithms.",
    "url": "https://arxiv.org/pdf/1911.12851.pdf",
    "lang": null,
    "authors": [
      2108671403,
      2225530511,
      2303533963,
      2613234758,
      2908006669
    ],
    "fos": [
      154945302,
      2779903281,
      94487597,
      3018412434,
      119857082,
      165696696,
      26486553,
      39890363,
      123657996,
      41008148,
      97541855
    ],
    "journals": [
      2596500785
    ],
    "conferences": [],
    "conference_series": [],
    "references": [
      1986856036,
      2064877840,
      2121863487,
      2145339207,
      2150468603,
      2173248099,
      2556013083,
      2587822999,
      2767286248,
      2889347284,
      2948210913,
      2951004968,
      2963611966,
      2963634205,
      3011120880
    ],
    "filter_matches": [
      "tr",
      "tl",
      "trl",
      "rl"
    ],
    "rank": 21575,
    "citation_count": 0,
    "estimated_citation_count": 0,
    "publication_date": "2019-11-28",
    "found_in": 1,
    "transfer_experiment_type": [
      "v"
    ],
    "transfer_experiment_subtype": [
      "same_all"
    ],
    "transfer_data_type": [
      "model"
    ],
    "transfer_performance_metrics": [
      "tr"
    ],
    "implementation": [
      "Custom",
      "CS",
      "Formulas",
      "PyTorch",
      "Gym",
      "Figures",
      "Tables"
    ],
    "policy_type": [
      "DQN",
      "AVAE"
    ],
    "task_mappings": [],
    "autonomous_transfer": 1,
    "is_deep_rl": 1,
    "tags": [
      "Input Abstraction",
      "VideoToSound",
      "Atari",
      "Simulation",
      "VideoGames",
      "Games"
    ],
    "allowed_learner": [
      "TD"
    ],
    "country": [
      "Portugal",
      "USA"
    ],
    "uni": [
      "INESC-ID",
      "Instituto Superior Tcnico",
      "Carnegie Mellon University"
    ],
    "department": [
      "Computer Science"
    ],
    "source_task_selection": [
      "h"
    ],
    "was_in_survey": [
      "mag"
    ],
    "in_title": [
      "t",
      "r",
      "l"
    ],
    "in_abs": [
      "r",
      "l"
    ],
    "in_contet": [
      "t",
      "r",
      "l"
    ],
    "rejected_or_unpublished": null,
    "duplication_status": null,
    "paper_for_thesis": 0,
    "paper_available": 1,
    "behind_paywall": 0,
    "paywall_circumventable": 0,
    "author_names": [
      "Manuela Veloso",
      "Rui Silva",
      "Francisco S. Melo",
      "Ana Paiva",
      "Miguel Vasco"
    ]
  },
  {
    "id": 2942204155,
    "title": "Transferring Task Goals via Hierarchical Reinforcement Learning",
    "abst": null,
    "url": "https://openreview.net/pdf?id=S1Y6TtJvG",
    "lang": "en",
    "authors": [
      662855036,
      1354816936,
      2278261595,
      2551560347,
      2901314117,
      2902991869,
      2941865070
    ],
    "fos": [
      97541855,
      154945302,
      41008148
    ],
    "journals": [],
    "conferences": [],
    "conference_series": [],
    "references": [],
    "filter_matches": [
      "tr",
      "tl",
      "trl",
      "rl"
    ],
    "rank": 21576,
    "citation_count": 0,
    "estimated_citation_count": 0,
    "publication_date": "2018-02-12",
    "found_in": 1,
    "transfer_experiment_type": [
      "t",
      "a"
    ],
    "transfer_experiment_subtype": [
      "multi"
    ],
    "transfer_data_type": [
      "r",
      "Hierarchical"
    ],
    "transfer_performance_metrics": [
      "j",
      "tr",
      "ap",
      "tt"
    ],
    "implementation": [
      "Custom",
      "CS",
      "Formulas",
      "Mujoco",
      "Figures",
      "Videos"
    ],
    "policy_type": [
      "HRL"
    ],
    "task_mappings": [
      "exp"
    ],
    "autonomous_transfer": 1,
    "is_deep_rl": 1,
    "tags": [
      "2D",
      "3D",
      "Robotics",
      "Navigation",
      "Simulation",
      "Ball",
      "Ant"
    ],
    "allowed_learner": [
      "TD",
      "H"
    ],
    "country": [
      "USA",
      "UK"
    ],
    "uni": [
      "University of California",
      "DeepMind"
    ],
    "department": [
      "Industry",
      "Uni",
      "Colab"
    ],
    "source_task_selection": [
      "h"
    ],
    "was_in_survey": [
      "mag"
    ],
    "in_title": [
      "t",
      "r",
      "l"
    ],
    "in_abs": [],
    "in_contet": [
      "t",
      "r",
      "l",
      "k"
    ],
    "rejected_or_unpublished": 1,
    "duplication_status": null,
    "paper_for_thesis": 0,
    "paper_available": 1,
    "behind_paywall": 0,
    "paywall_circumventable": 0,
    "author_names": [
      "Razvan Pascanu",
      "Yee Whye Teh",
      "Nicolas Heess",
      "Saining Xie",
      "Siqi Liu",
      "Alexandre Galashov",
      "Shaobo Hou"
    ]
  },
  {
    "id": 1990654808,
    "title": "Context transfer in reinforcement learning using action-value functions",
    "abst": "This paper discusses the notion of context transfer in reinforcement learning tasks. Context transfer, as defined in this paper, implies knowledge transfer between source and target tasks that share the same environment dynamics and reward function but have different states or action spaces. In other words, the agents learn the same task while using different sensors and actuators. This requires the existence of an underlying common Markov decision process (MDP) to which all the agents&#039; MDPs can be mapped. This is formulated in terms of the notion of MDP homomorphism. The learning framework is Q-learning. To transfer the knowledge between these tasks, the feature space is used as a translator and is expressed as a partial mapping between the state-action spaces of different tasks. The Q-values learned during the learning process of the source tasks are mapped to the sets of Q-values for the target task. These transferred Q-values are merged together and used to initialize the learning process of the target task. An interval-based approach is used to represent and merge the knowledge of the source tasks. Empirical results show that the transferred initialization can be beneficial to the learning process of the target task.",
    "url": "https://core.ac.uk/display/88336304",
    "lang": "en",
    "authors": [
      442130,
      2164566917,
      2330815663
    ],
    "fos": [
      77075516,
      106189395,
      2776960227,
      28006648,
      154945302,
      97541855,
      83665646,
      119857082,
      41008148,
      98763669,
      114466953
    ],
    "journals": [
      72372694
    ],
    "conferences": [],
    "conference_series": [],
    "references": [
      36691172,
      1515851193,
      1533853869,
      1534331386,
      1573317636,
      1612195517,
      2005312264,
      2058879737,
      2079247031,
      2093713825,
      2097381042,
      2097498341,
      2102048877,
      2110292307,
      2123043338,
      2133040789,
      2138497321,
      2156493855,
      2164114810
    ],
    "filter_matches": [
      "tr",
      "tl",
      "trl",
      "rl"
    ],
    "rank": 21584,
    "citation_count": 5,
    "estimated_citation_count": 5,
    "publication_date": "2014-01-01",
    "found_in": 1,
    "transfer_experiment_type": [
      "v",
      "a"
    ],
    "transfer_experiment_subtype": [
      "diff-no"
    ],
    "transfer_data_type": [
      "fea",
      "Q"
    ],
    "transfer_performance_metrics": [
      "j",
      "tt",
      "ap",
      "tr"
    ],
    "implementation": [
      "Custom",
      "CS",
      "Formulas",
      "Theorem",
      "Figures"
    ],
    "policy_type": [
      "Q"
    ],
    "task_mappings": [
      "sup"
    ],
    "autonomous_transfer": 0,
    "is_deep_rl": 0,
    "tags": [
      "Simulation",
      "2D",
      "Navigation",
      "Grid",
      "Collect",
      "Robots",
      "CrossroadTrafficController"
    ],
    "allowed_learner": [
      "TD"
    ],
    "country": [
      "Iran"
    ],
    "uni": [
      "University of Tehran",
      "Institute for Research in Fundamental Sciences"
    ],
    "department": [
      "Cognitive Robotics Lab",
      "College of Engineering",
      "Electrical and Computer Engineering"
    ],
    "source_task_selection": [
      "h"
    ],
    "was_in_survey": [
      "mag"
    ],
    "in_title": [
      "t",
      "r",
      "l"
    ],
    "in_abs": [
      "t",
      "r",
      "l",
      "k"
    ],
    "in_contet": [
      "t",
      "r",
      "l",
      "k"
    ],
    "rejected_or_unpublished": null,
    "duplication_status": null,
    "paper_for_thesis": 0,
    "paper_available": 1,
    "behind_paywall": 0,
    "paywall_circumventable": 0,
    "author_names": [
      "Babak Nadjar Araabi",
      "Majid Nili Ahmadabadi",
      "Amin Mousavi"
    ]
  },
  {
    "id": 2910911697,
    "title": "Boosting Reinforcement Learning in Competitive Influence Maximization with Transfer Learning",
    "abst": "Companies aim to promote their products under competitions and try to gain more profit than other companies. This problem is formulated as a Competitive Influence Maximization (CIM). Recently, a reinforcement learning has been used to solve the CIM problem, that is, to find an optimal strategy against competitor in order to maximize the commutative reward under the competition from other agents. However, reinforcement learning agents require huge training time to find an optimal strategy whenever the settings of the agents or the networks change. To tackle this issue, we propose a transfer learning method in reinforcement learning to reduce the training time and utilize the knowledge gained on source network to target network. Our method relies on two ideas, the first one is the state representation of the source and target networks in order to efficiently utilize the knowledge gained on source network to target network. The second idea is to transfer the final Q-solution of source network while learning on the target network. We validate our transfer learning method in similar or different settings of source and target networks while competing against the competitor&#039;s known strategies. Experimental results show that our proposed transfer learning method achieves similar or better performance as a baseline model while significantly reducing training time in all settings.",
    "url": "https://dblp.uni-trier.de/db/conf/webi/webi2018.html#AliWC18",
    "lang": null,
    "authors": [
      2286816785,
      2905783848,
      2906538370
    ],
    "fos": [
      150899416,
      119857082,
      2776330181,
      41008148,
      154945302,
      97541855,
      84685590,
      46686674,
      124101348,
      2989414621,
      175154964,
      183778304
    ],
    "journals": [],
    "conferences": [
      2901949724
    ],
    "conference_series": [
      1200067787
    ],
    "references": [
      107556075,
      1502487903,
      1512602432,
      2004030284,
      2009305899,
      2118686230,
      2121863487,
      2128905965,
      2139297408,
      2145983895,
      2169743339,
      2269573953,
      2395439732,
      2511460510,
      2755088640,
      2949683268
    ],
    "filter_matches": [
      "tr",
      "tl",
      "trl",
      "rl"
    ],
    "rank": 21598,
    "citation_count": 2,
    "estimated_citation_count": 2,
    "publication_date": "2018-12-01",
    "found_in": 1,
    "transfer_experiment_type": [
      "s",
      "s_i",
      "s_f"
    ],
    "transfer_experiment_subtype": [
      "same_all"
    ],
    "transfer_data_type": [
      "pi",
      "Q"
    ],
    "transfer_performance_metrics": [
      "tt"
    ],
    "implementation": [
      "Custom",
      "CS",
      "graph-tool",
      "Formulas",
      "Figures"
    ],
    "policy_type": [
      "Q"
    ],
    "task_mappings": [
      "N/A"
    ],
    "autonomous_transfer": 0,
    "is_deep_rl": 0,
    "tags": [
      "Simulation",
      "SocialNetworks",
      "Social",
      "Graph",
      "Influence",
      "Maximization"
    ],
    "allowed_learner": [
      "TD"
    ],
    "country": [
      "Taiwan"
    ],
    "uni": [
      "Academia Sinica",
      "National Tsing Hua University"
    ],
    "department": [
      "Social Networks and Human-Centered Computing",
      "Information Science",
      "Computer Science"
    ],
    "source_task_selection": [
      "h"
    ],
    "was_in_survey": [
      "mag"
    ],
    "in_title": [
      "t",
      "r",
      "l"
    ],
    "in_abs": [
      "t",
      "r",
      "l",
      "k"
    ],
    "in_contet": [
      "t",
      "r",
      "l",
      "k"
    ],
    "rejected_or_unpublished": null,
    "duplication_status": null,
    "paper_for_thesis": 0,
    "paper_available": 1,
    "behind_paywall": 1,
    "paywall_circumventable": 1,
    "author_names": [
      "Chih-Yu Wang",
      "Khurshed Ali",
      "Yi-Shin Chen"
    ]
  },
  {
    "id": 3008535267,
    "title": "&quot;Good Robot!&quot;: Efficient Reinforcement Learning for Multi-Step Visual Tasks with Sim to Real Transfer.",
    "abst": "In order to effectively learn multi-step tasks, robots must be able to understand the context by which task progress is defined. In reinforcement learning, much of this information is provided to the learner by the reward function. However, comparatively little work has examined how the reward function captures - or fails to capture - task context in robotics, particularly in long-horizon tasks where failure is highly consequential. To address this issue, we describe the Schedule for Positive Task (SPOT) Reward and the SPOT-Q reinforcement learning algorithm, which efficiently learn multi-step block manipulation tasks in both simulation and real-world environments. SPOT-Q is remarkably effective compared to past benchmarks. It successfully completes simulated trials of a variety of tasks including stacking cubes (98%), clearing toys by pushing and grasping arranged in random (100%) and adversarial (95%) patterns, and creating rows of cubes (93%). Furthermore, we demonstrate direct sim to real transfer. By directly loading the simulation-trained model on the real robot, we are able to create real stacks in 90% of trials and rows in 80% of trials with no additional real-world fine-tuning. Our system is also quite efficient - models train within 1-10k actions, depending on the task. As a result, our algorithm makes learning complex, multi-step tasks both efficient and practical for real world manipulation tasks. Code is available at https://github.com/jhu-lcsr/good_robot .",
    "url": "http://export.arxiv.org/abs/1909.11730",
    "lang": null,
    "authors": [
      2070051019,
      2225997307,
      2898380890,
      2975950970,
      3006949375,
      3008566962,
      3014677744
    ],
    "fos": [
      127413603,
      143542225,
      90509273,
      97541855,
      135598885,
      2984650650,
      44154836,
      34413123,
      154945302
    ],
    "journals": [
      2596519289
    ],
    "conferences": [],
    "conference_series": [],
    "references": [
      1892339738,
      1999156278,
      2161168419,
      2293598046,
      2400532028,
      2553882142,
      2600030077,
      2605102758,
      2799140957,
      2892232679,
      2910474428,
      2925173345,
      2955035422,
      2962715211,
      2962736495,
      2962793652,
      2962899390,
      2963033241,
      2963149945,
      2963170432,
      2963326767,
      2963477884,
      2963484919,
      2963513937,
      2963572125,
      2963689319,
      2963713397,
      2963894672,
      2964055695,
      2964161785,
      2964262254,
      2965407719,
      2982626199,
      2986303149,
      2991687685,
      3003651690
    ],
    "filter_matches": [
      "tr",
      "tl",
      "trl",
      "rl"
    ],
    "rank": 21649,
    "citation_count": 0,
    "estimated_citation_count": 0,
    "publication_date": "2019-09-25",
    "found_in": 1,
    "transfer_experiment_type": [
      "t",
      "s",
      "s_i",
      "s_f"
    ],
    "transfer_experiment_subtype": [
      "sim2real",
      "multi"
    ],
    "transfer_data_type": [
      "r",
      "ExperienceReplay"
    ],
    "transfer_performance_metrics": [
      "tr",
      "ap"
    ],
    "implementation": [
      "OSS",
      "Custom",
      "PyTorch",
      "Gym",
      "Figures",
      "Formulas",
      "Pseudo",
      "Tables",
      "Videos"
    ],
    "policy_type": [
      "VPG",
      "DQN"
    ],
    "task_mappings": [
      "N/A"
    ],
    "autonomous_transfer": 0,
    "is_deep_rl": 1,
    "tags": [
      "3D",
      "VisualGrasping",
      "Robotics",
      "Simulation",
      "RealWorld",
      "Arm",
      "2D",
      "Navigation",
      "Grid"
    ],
    "allowed_learner": [
      "TD"
    ],
    "country": [
      "USA"
    ],
    "uni": [
      "The Johns Hpkins University",
      "NVIDIA"
    ],
    "department": [
      "Industry",
      "Uni",
      "Colab"
    ],
    "source_task_selection": [],
    "was_in_survey": [
      "mag"
    ],
    "in_title": [
      "t",
      "r",
      "l"
    ],
    "in_abs": [
      "t",
      "r",
      "l",
      "k"
    ],
    "in_contet": [
      "t",
      "r",
      "l",
      "k",
      "s"
    ],
    "rejected_or_unpublished": null,
    "duplication_status": null,
    "paper_for_thesis": 0,
    "paper_available": 1,
    "behind_paywall": 0,
    "paywall_circumventable": 0,
    "author_names": [
      "Gregory D. Hager",
      "Chris Paxton",
      "Andrew Hundt",
      "Heeyeon Kwon",
      "Hongtao Wu",
      "Nicholas Greene",
      "Benjamin Killeen"
    ]
  },
  {
    "id": 2982398001,
    "title": "A Building Energy Consumption Prediction Method Based on Integration of a Deep Neural Network and Transfer Reinforcement Learning",
    "abst": "With respect to the problem of the low accuracy of traditional building energy prediction methods, this paper proposes a novel prediction method for building energy consumption, which is based on t...",
    "url": "https://www.worldscientific.com/doi/10.1142/S0218001420520059",
    "lang": "en",
    "authors": [
      2144215932,
      2579359418,
      2651248976,
      2660978440,
      2982521965,
      2998951518
    ],
    "fos": [
      119857082,
      154945302,
      2982932961,
      97541855,
      33923547,
      3018689401,
      50644808,
      2985964769
    ],
    "journals": [
      41486457
    ],
    "conferences": [],
    "conference_series": [],
    "references": [
      1992805456,
      2165698076,
      2281071090,
      2543909292,
      2601096366,
      2605614336,
      2616881109,
      2763500568,
      2785367375,
      2792046648
    ],
    "filter_matches": [
      "tr",
      "tl",
      "trl",
      "rl"
    ],
    "rank": 21659,
    "citation_count": 0,
    "estimated_citation_count": 0,
    "publication_date": "2020-01-08",
    "found_in": 1,
    "transfer_experiment_type": [
      "s"
    ],
    "transfer_experiment_subtype": [
      "same_all"
    ],
    "transfer_data_type": [
      "fea"
    ],
    "transfer_performance_metrics": [
      "ap"
    ],
    "implementation": [
      "Custom",
      "CS",
      "Formulas",
      "Figures",
      "Pseudo",
      "Tables"
    ],
    "policy_type": [
      "DQN"
    ],
    "task_mappings": [
      "N/A"
    ],
    "autonomous_transfer": 0,
    "is_deep_rl": 1,
    "tags": [
      "SmartEnergy",
      "Power",
      "Consumption",
      "Prediction",
      "Simulation"
    ],
    "allowed_learner": [
      "TD"
    ],
    "country": [
      "China",
      "Canada"
    ],
    "uni": [
      "Suzhou University of Science and Technology",
      "McMaster University"
    ],
    "department": [
      "Electronics and Infromation Engineering",
      "Key Laboratory of Intelligent Building Energy Efficiency",
      "Engineering"
    ],
    "source_task_selection": [
      "lib"
    ],
    "was_in_survey": [
      "mag"
    ],
    "in_title": [
      "t",
      "r",
      "l"
    ],
    "in_abs": [
      "t",
      "r",
      "l"
    ],
    "in_contet": [
      "t",
      "r",
      "l",
      "k",
      "s"
    ],
    "rejected_or_unpublished": null,
    "duplication_status": null,
    "paper_for_thesis": 0,
    "paper_available": 1,
    "behind_paywall": 1,
    "paywall_circumventable": 1,
    "author_names": [
      "Hongjie Wu",
      "Qiming Fu",
      "Baochuan Fu",
      "Jianping Chen",
      "QingSong Liu",
      "Zhen Gao"
    ]
  },
  {
    "id": 2965163470,
    "title": "Value Function Transfer for Deep Multi-Agent Reinforcement Learning Based on N-Step Returns",
    "abst": null,
    "url": "https://www.ijcai.org/Proceedings/2019/65",
    "lang": null,
    "authors": [
      2134699430,
      2890294771,
      2964420635,
      2964471977,
      2964851122
    ],
    "fos": [
      97541855,
      41008148,
      154945302,
      119857082,
      14646407
    ],
    "journals": [],
    "conferences": [
      2890997312
    ],
    "conference_series": [
      1203999783
    ],
    "references": [],
    "filter_matches": [
      "tr",
      "tl",
      "trl",
      "rl"
    ],
    "rank": 21659,
    "citation_count": 2,
    "estimated_citation_count": 2,
    "publication_date": "2019-08-01",
    "found_in": 1,
    "transfer_experiment_type": [
      "t",
      "s_i",
      "s_f",
      "s",
      "levels"
    ],
    "transfer_experiment_subtype": [
      "multi"
    ],
    "transfer_data_type": [
      "Q"
    ],
    "transfer_performance_metrics": [
      "j",
      "tr",
      "tt",
      "ap"
    ],
    "implementation": [
      "Custom",
      "CS",
      "Formulas",
      "Figures",
      "Pseudo"
    ],
    "policy_type": [
      "Q",
      "DQN"
    ],
    "task_mappings": [
      "N/A"
    ],
    "autonomous_transfer": 0,
    "is_deep_rl": 1,
    "tags": [
      "2D",
      "Navigation",
      "Maze",
      "MARL",
      "Atari",
      "Games",
      "Simulation",
      "MultiAgent",
      "Pacman",
      "PredatorPrey"
    ],
    "allowed_learner": [
      "TD"
    ],
    "country": [
      "China"
    ],
    "uni": [
      "Nanjing University",
      "Netease"
    ],
    "department": [
      "National Key Laboratory for Novel Software Technology",
      "Fuxi AI Lab"
    ],
    "source_task_selection": [
      "h"
    ],
    "was_in_survey": [
      "mag"
    ],
    "in_title": [
      "t",
      "r",
      "l"
    ],
    "in_abs": [
      "t",
      "r",
      "l",
      "k"
    ],
    "in_contet": [
      "t",
      "r",
      "l",
      "k"
    ],
    "rejected_or_unpublished": null,
    "duplication_status": null,
    "paper_for_thesis": 0,
    "paper_available": 1,
    "behind_paywall": 0,
    "paywall_circumventable": 0,
    "author_names": [
      "Yingfeng Chen",
      "Changjie Fan",
      "Yong Liu",
      "Yujing Hu",
      "Yang Gao"
    ]
  },
  {
    "id": 3006698967,
    "title": "A Dynamic Financial Knowledge Graph Based on Reinforcement Learning and Transfer Learning",
    "abst": "The knowledge graph is a means of visualizing data to aid information analysis and understanding. In this paper, we construct a novel dynamic financial knowledge graph, which utilizes time information to capture data changes and trends over time. Firstly, the basic dynamic financial knowledge graph is constructed through structured and semi-structured data related to A-share. Then, using the transfer learning algorithms, we train the financial entity recognition models based on BERT, BiLSTM, and CRF. Next, we train the financial entity linking models based on similarity features and prior knowledge. After that, to alleviate the noise brought by distant supervision, we explore to train the financial relation classification models with the help of reinforcement learning. Finally, we implement the dynamic knowledge graph based on these models and their predictions. Additionally, a display website is designed and implemented to dynamically display the structural changes of the knowledge graph over time. The financial knowledge graph constructed in this paper is practical and the construction pipeline provides insights for a professional dynamic knowledge graph as well.",
    "url": "https://doi.org/10.1109/BigData47090.2019.9005691",
    "lang": null,
    "authors": [
      2096609276,
      3007461961,
      3008669038,
      3008719740
    ],
    "fos": [
      150899416,
      154945302,
      96711827,
      10138342,
      2985695469,
      97541855,
      119857082,
      41008148,
      3017590132,
      3018801639,
      2987255567
    ],
    "journals": [],
    "conferences": [],
    "conference_series": [
      2623113034
    ],
    "references": [
      179875071,
      1512387364,
      1515851193,
      1574911124,
      1623072288,
      1940872118,
      2064675550,
      2080133951,
      2081580037,
      2094728533,
      2107598941,
      2107658650,
      2117772341,
      2162638401,
      2163605009,
      2165698076,
      2250521169,
      2251135946,
      2296283641,
      2515462165,
      2552553554,
      2620787630,
      2776652360,
      2950577311
    ],
    "filter_matches": [
      "tr",
      "tl",
      "trl",
      "rl"
    ],
    "rank": 21664,
    "citation_count": 0,
    "estimated_citation_count": 0,
    "publication_date": "2019-12-01",
    "found_in": 1,
    "transfer_experiment_type": [],
    "transfer_experiment_subtype": [
      "same_all"
    ],
    "transfer_data_type": [
      "KnowledgeGraph"
    ],
    "transfer_performance_metrics": [
      "tr"
    ],
    "implementation": [
      "Custom",
      "CS",
      "Figures",
      "Tables"
    ],
    "policy_type": [
      "BERT"
    ],
    "task_mappings": [],
    "autonomous_transfer": 0,
    "is_deep_rl": 1,
    "tags": [
      "Finance",
      "Simulation",
      "KnowledgeGraph",
      "CNN",
      "NLP"
    ],
    "allowed_learner": [
      "TD"
    ],
    "country": [
      "China"
    ],
    "uni": [
      "Peking University",
      "Beijing Normal University"
    ],
    "department": [
      "EECS",
      "Government"
    ],
    "source_task_selection": [],
    "was_in_survey": [
      "mag"
    ],
    "in_title": [
      "t",
      "r",
      "l",
      "k"
    ],
    "in_abs": [
      "t",
      "r",
      "l",
      "k"
    ],
    "in_contet": [
      "t",
      "r",
      "l",
      "k"
    ],
    "rejected_or_unpublished": null,
    "duplication_status": null,
    "paper_for_thesis": 0,
    "paper_available": 1,
    "behind_paywall": 1,
    "paywall_circumventable": 1,
    "author_names": [
      "Hongfei Yan",
      "Xia Zhang",
      "Chong Chen",
      "Rui Miao"
    ]
  },
  {
    "id": 2971274224,
    "title": "A Study on Efficient Transfer Learning for Reinforcement Learning Using Sparse Coding",
    "abst": null,
    "url": "http://www.joace.org/uploadfile/2015/1023/20151023022908338.pdf",
    "lang": null,
    "authors": [
      2971210172,
      2971307402
    ],
    "fos": [
      97541855,
      41008148,
      150899416,
      119857082,
      77637269,
      154945302
    ],
    "journals": [
      2764645559
    ],
    "conferences": [],
    "conference_series": [],
    "references": [
      110451278,
      1569756368,
      1992405901,
      2020719522,
      2031727428,
      2056584142,
      2121863487,
      2145889472,
      2169673611,
      2188752309,
      3011120880
    ],
    "filter_matches": [
      "tr",
      "tl",
      "trl",
      "rl"
    ],
    "rank": 21669,
    "citation_count": 3,
    "estimated_citation_count": 3,
    "publication_date": "2016-01-01",
    "found_in": 1,
    "transfer_experiment_type": [
      "s_i",
      "s_f",
      "s",
      "levels"
    ],
    "transfer_experiment_subtype": [
      "multi"
    ],
    "transfer_data_type": [
      "sparse_coding"
    ],
    "transfer_performance_metrics": [
      "j",
      "tt"
    ],
    "implementation": [
      "Custom",
      "CS",
      "Formulas",
      "Figures",
      "Tables"
    ],
    "policy_type": [
      "Q"
    ],
    "task_mappings": [
      "N/A"
    ],
    "autonomous_transfer": 0,
    "is_deep_rl": 0,
    "tags": [
      "2D",
      "Navigation",
      "Maze",
      "Simulation"
    ],
    "allowed_learner": [
      "TD"
    ],
    "country": [
      "Japan"
    ],
    "uni": [
      "Ochanomizu University"
    ],
    "department": [
      "School of Humanities and Sciences"
    ],
    "source_task_selection": [
      "lib"
    ],
    "was_in_survey": [
      "mag"
    ],
    "in_title": [
      "t",
      "r",
      "l"
    ],
    "in_abs": [
      "t",
      "r",
      "l"
    ],
    "in_contet": [
      "t",
      "r",
      "l",
      "k"
    ],
    "rejected_or_unpublished": null,
    "duplication_status": null,
    "paper_for_thesis": 0,
    "paper_available": 1,
    "behind_paywall": 0,
    "paywall_circumventable": 0,
    "author_names": [
      "Midori Saito",
      "Ichiro Kobayashi"
    ]
  },
  {
    "id": 2900640207,
    "title": "ASD: A Framework for Generation of Task Hierarchies for Transfer in Reinforcement Learning",
    "abst": "We present ASD (Action, Sequence, and Divide), a new framework for Hierarchical Reinforcement Learning (HRL). Present HRL methods construct the task hierarchies but fail to avoid exploration when tasks are to be performed in a particular sequence, resulting in the agent needlessly exploring all permutations of the tasks. When the task hierarchies are used as an ASD framework, the RL agent encounters better constraints, preventing it from pursuing policies that are not valid, thus enabling the agent to achieve the optimal policy faster. The hierarchies created using the methods explained in this paper can be used to solve new episodes of the same environment, as well as similar instances of the problem. The hierarchies generated with an ASD framework can be used to establish an ordering of tasks. The objective is to not only to complete the tasks but also give the agent insights into the sequence of tasks that need to be performed in order to correctly solve a problem. We present an algorithm to generate the hierarchies as an ASD framework. The algorithm has been evaluated on some of the standard RL domains, namely, Taxi and Wargus, and is found to give correct results.",
    "url": "https://dblp.uni-trier.de/db/conf/iconip/iconip2018-3.html#GoyalMNR18",
    "lang": "en",
    "authors": [
      2228911865,
      2655663567,
      2901192619,
      2901358111
    ],
    "fos": [
      41008148,
      119857082,
      154945302,
      97541855,
      147849574,
      31170391
    ],
    "journals": [],
    "conferences": [
      2891292088
    ],
    "conference_series": [
      1183580825
    ],
    "references": [
      1510673313,
      1511612612,
      1585529040,
      1598052524,
      1983546080,
      2109910161,
      2121517924,
      2121863487,
      2127234468,
      2143042284,
      2153668164,
      2161252410,
      2202187378,
      2290288804,
      2572919868
    ],
    "filter_matches": [
      "tr",
      "tl",
      "trl",
      "rl"
    ],
    "rank": 21682,
    "citation_count": 0,
    "estimated_citation_count": 0,
    "publication_date": "2018-12-13",
    "found_in": 1,
    "transfer_experiment_type": [],
    "transfer_experiment_subtype": [
      "theory"
    ],
    "transfer_data_type": [],
    "transfer_performance_metrics": [],
    "implementation": [
      "Custom",
      "CS",
      "Pseudo",
      "Figures"
    ],
    "policy_type": [
      "HRL"
    ],
    "task_mappings": [
      "N/A"
    ],
    "autonomous_transfer": 0,
    "is_deep_rl": 0,
    "tags": [
      "2D",
      "Navigation",
      "Grid",
      "Taxi",
      "Wargus",
      "Collect",
      "MultiAgent",
      "Simulation"
    ],
    "allowed_learner": [
      "TD",
      "H"
    ],
    "country": [
      "India",
      "Singapore"
    ],
    "uni": [
      "IIIT Bangalore",
      "National University of Singapore"
    ],
    "department": [
      "International Institute of Information Technology"
    ],
    "source_task_selection": [],
    "was_in_survey": [
      "mag"
    ],
    "in_title": [
      "t",
      "r",
      "l"
    ],
    "in_abs": [
      "r",
      "l"
    ],
    "in_contet": [
      "t",
      "r",
      "l",
      "k"
    ],
    "rejected_or_unpublished": null,
    "duplication_status": null,
    "paper_for_thesis": 0,
    "paper_available": 1,
    "behind_paywall": 0,
    "paywall_circumventable": 0,
    "author_names": [
      "Akshay Narayan",
      "Shrisha Rao",
      "Abhijith Madan",
      "Jatin Goyal"
    ]
  },
  {
    "id": 2986543185,
    "title": "Fuzzy Reinforcement Learning and Curriculum Transfer Learning for Micromanagement in Multi-Robot Confrontation",
    "abst": "Multi-Robot Confrontation on physics-based simulators is a complex and time-consuming task, but simulators are required to evaluate the performance of the advanced algorithms. Recently, a few advanced algorithms have been able to produce considerably complex levels in the context of the robot confrontation system when the agents are facing multiple opponents. Meanwhile, the current confrontation decision-making system suffers from difficulties in optimization and generalization. In this paper, a fuzzy reinforcement learning (RL) and the curriculum transfer learning are applied to the micromanagement for robot confrontation system. Firstly, an improved Q-learning in the semi-Markov decision-making process is designed to train the agent and an efficient RL model is defined to avoid the curse of dimensionality. Secondly, a multi-agent RL algorithm with parameter sharing is proposed to train the agents. We use a neural network with adaptive momentum acceleration as a function approximator to estimate the state-action function. Then, a method of fuzzy logic is used to regulate the learning rate of RL. Thirdly, a curriculum transfer learning method is used to extend the RL model to more difficult scenarios, which ensures the generalization of the decision-making system. The experimental results show that the proposed method is effective.",
    "url": "https://doi.org/10.3390/info10110341",
    "lang": null,
    "authors": [
      2983860342,
      2984049293
    ],
    "fos": [
      150899416,
      47177190,
      117896860,
      119857082,
      111030470,
      41008148,
      90509273,
      2781329980,
      58166,
      154945302,
      50644808
    ],
    "journals": [
      12613394
    ],
    "conferences": [],
    "conference_series": [],
    "references": [
      1588910350,
      1871376293,
      1967644203,
      1976981061,
      2009303086,
      2031748952,
      2048145332,
      2049064507,
      2062130345,
      2074389427,
      2089217930,
      2130935555,
      2145339207,
      2165698076,
      2168405694,
      2523345745,
      2526283374,
      2530520848,
      2574978968,
      2783134749,
      2789901741,
      2808492790,
      2883705381,
      2891133400,
      2915454777,
      2919115771,
      2963795407,
      2963890729,
      2974826883,
      3016392643
    ],
    "filter_matches": [
      "tr",
      "tl",
      "trl",
      "rl"
    ],
    "rank": 21728,
    "citation_count": 2,
    "estimated_citation_count": 2,
    "publication_date": "2019-11-02",
    "found_in": 1,
    "transfer_experiment_type": [
      "#",
      "s_i",
      "s_f",
      "s"
    ],
    "transfer_experiment_subtype": [
      "same_all"
    ],
    "transfer_data_type": [
      "curriculum"
    ],
    "transfer_performance_metrics": [
      "j",
      "tt"
    ],
    "implementation": [
      "Custom",
      "CS",
      "Robocode",
      "Formulas",
      "Figures",
      "Pseudo",
      "Tables"
    ],
    "policy_type": [
      "SSAQ",
      "Q"
    ],
    "task_mappings": [
      "N/A"
    ],
    "autonomous_transfer": 0,
    "is_deep_rl": 1,
    "tags": [
      "2D",
      "Robotics",
      "RoboCode",
      "Simulation",
      "SharedParameters",
      "MultiAgent"
    ],
    "allowed_learner": [
      "TD",
      "MB"
    ],
    "country": [
      "China"
    ],
    "uni": [
      "Hubei University of Arts and Science",
      "Northwestern Polytechnical University"
    ],
    "department": [
      "Computer Engineering",
      "Computer Science"
    ],
    "source_task_selection": [
      "all"
    ],
    "was_in_survey": [
      "mag"
    ],
    "in_title": [
      "t",
      "r",
      "l"
    ],
    "in_abs": [
      "t",
      "r",
      "l"
    ],
    "in_contet": [
      "t",
      "r",
      "l",
      "k"
    ],
    "rejected_or_unpublished": null,
    "duplication_status": null,
    "paper_for_thesis": 0,
    "paper_available": 1,
    "behind_paywall": 0,
    "paywall_circumventable": 0,
    "author_names": [
      "Meng Xu",
      "Chunyang Hu"
    ]
  },
  {
    "id": 2042357378,
    "title": "Multitask Reinforcement Learning on the Distribution of MDPs",
    "abst": "In this paper we address a new problem in reinforcement learning. Here we consider an agent that faces multiple learning tasks within its lifetime. The agent&#x2019;s objective is to maximize its total reward in the lifetime as well as a conventional return in each task. To realize this, it has to be endowed an important ability to keep its past learning experiences and utilize them for improving future learning performance. This time we try to phrase this problem formally. The central idea is to introduce an environmental class, BV-MDPs that is defined with the distribution of MDPs. As an approach to exploiting past learning experiences, we focus on statistical information (mean and deviation) about the agent&#x2019;s value tables. The mean can be used as initial values of the table when a new task is presented. The deviation can be viewed as measuring reliability of the mean, and we utilize it in calculating priority of simulated backups. We conduct experiments in computer simulation to evaluate the effectiveness.",
    "url": "https://ui.adsabs.harvard.edu/abs/2003ITEIS.123.1004F/abstract",
    "lang": "en",
    "authors": [
      2329837719,
      2685624219
    ],
    "fos": [
      188116033,
      154945302,
      28006648,
      188888258,
      199190896,
      119857082,
      97541855,
      41008148,
      8038995,
      77967617,
      58973888
    ],
    "journals": [
      178577447
    ],
    "conferences": [],
    "conference_series": [],
    "references": [
      1491843047,
      1557517019,
      2012036715,
      2048226872,
      2100677568,
      2106639887,
      2107726111,
      2121863487
    ],
    "filter_matches": [
      "rl"
    ],
    "rank": 21731,
    "citation_count": 10,
    "estimated_citation_count": 10,
    "publication_date": "2003-01-01",
    "found_in": 1,
    "transfer_experiment_type": [
      "t"
    ],
    "transfer_experiment_subtype": [
      "multi"
    ],
    "transfer_data_type": [
      "Q"
    ],
    "transfer_performance_metrics": [
      "j",
      "tr"
    ],
    "implementation": [
      "Custom",
      "CS",
      "Pseudo",
      "Formulas",
      "Figures"
    ],
    "policy_type": [
      "Q",
      "BPSb"
    ],
    "task_mappings": [
      "N/A"
    ],
    "autonomous_transfer": 0,
    "is_deep_rl": 0,
    "tags": [
      "2D",
      "Grid",
      "Navigation",
      "Simulation"
    ],
    "allowed_learner": [
      "TD"
    ],
    "country": [
      "Japan"
    ],
    "uni": [
      "Tokyo Institute of Technology"
    ],
    "department": [
      "Computational Intelligence and Systems Science"
    ],
    "source_task_selection": [
      "all"
    ],
    "was_in_survey": [
      "taylor"
    ],
    "in_title": [
      "r",
      "l"
    ],
    "in_abs": [
      "r",
      "l"
    ],
    "in_contet": [
      "t",
      "r",
      "l"
    ],
    "rejected_or_unpublished": null,
    "duplication_status": null,
    "paper_for_thesis": 0,
    "paper_available": 1,
    "behind_paywall": 0,
    "paywall_circumventable": 0,
    "author_names": [
      "Yamamura Masayuki",
      "Tanaka Fumihide"
    ]
  },
  {
    "id": 2964998356,
    "title": "Skill based transfer learning with domain adaptation for continuous reinforcement learning domains",
    "abst": "Although reinforcement learning is known as an effective machine learning technique, it might perform poorly in complex problems, especially real-world problems, leading to a slow rate of convergence. This issue magnifies when facing continuous domains where the curse of dimensionality is inevitable, and generalization is mostly desired. Transfer learning is a successful technique to remedy such a problem which results in significant improvements in learning performance by providing generalization not only within a task but also across different but related or similar tasks. The critical issue in transfer learning is how to incorporate the knowledge acquired from learning in a different but related task in the past. Domain adaptation is an exciting paradigm that seeks to address this challenge. In this paper, we propose a novel skill based Transfer Learning with Domain Adaptation (TLDA) approach suitable for continuous RL problems. TLDA discovers and learns skills as high-level knowledge from source task and then uses domain adaptation technique to help agent discover state-action mapping as a relation between the source and target tasks. With such mapping, TLDA can adapt source skills and speed up learning on a new target task. The experimental results verify the achievement of an effective transfer learning method for continuous reinforcement learning problems.",
    "url": "https://dblp.uni-trier.de/db/journals/apin/apin50.html#ShoelehA20",
    "lang": "en",
    "authors": [
      222359281,
      2138632143
    ],
    "fos": [
      97541855,
      2776434776,
      119857082,
      2983815222,
      41008148,
      154945302,
      150899416,
      71923881,
      68339613,
      111030470,
      2993694375
    ],
    "journals": [
      74726891
    ],
    "conferences": [],
    "conference_series": [],
    "references": [
      22861983,
      36691172,
      73694616,
      110451278,
      158722652,
      1414356350,
      1500331901,
      1549884255,
      1556824961,
      1600437712,
      1647196294,
      1822705290,
      1842037955,
      1848094219,
      1963873191,
      1982696459,
      1985756506,
      1995688924,
      2004030284,
      2012231141,
      2013988526,
      2056354534,
      2091714857,
      2097381042,
      2103013841,
      2106008664,
      2106261932,
      2108535023,
      2108995755,
      2109910161,
      2114580749,
      2115403315,
      2125555727,
      2125865219,
      2142502798,
      2143435603,
      2150385772,
      2154328025,
      2161795906,
      2165698076,
      2172131460,
      2285854019,
      2399478587,
      2440926996,
      2517639096,
      2523124567,
      2534060593,
      2606433045,
      2616430965,
      2742143911,
      2767756420,
      2797734773,
      2804673281,
      2949600457,
      2950040888,
      2950614095
    ],
    "filter_matches": [
      "tr",
      "tl",
      "trl",
      "rl"
    ],
    "rank": 21734,
    "citation_count": 2,
    "estimated_citation_count": 2,
    "publication_date": "2020-02-01",
    "found_in": 1,
    "transfer_experiment_type": [
      "s_i",
      "s_f",
      "s",
      "levels"
    ],
    "transfer_experiment_subtype": [
      "multi"
    ],
    "transfer_data_type": [
      "Q",
      "graph"
    ],
    "transfer_performance_metrics": [
      "tt",
      "ap",
      "tr"
    ],
    "implementation": [
      "Custom",
      "CS",
      "Formulas",
      "Pseudo",
      "Figures",
      "Tables"
    ],
    "policy_type": [
      "GSL",
      "SARSA",
      "TLDA"
    ],
    "task_mappings": [
      "N/A"
    ],
    "autonomous_transfer": 0,
    "is_deep_rl": 0,
    "tags": [
      "Simulation",
      "2D",
      "Pinball",
      "GraphBased",
      "MountainCar",
      "3D",
      "MountainCar3D",
      "MountainCar2Dto3D",
      "ClassicControl"
    ],
    "allowed_learner": [
      "TD",
      "H"
    ],
    "country": [
      "Iran"
    ],
    "uni": [
      "University of Tehran"
    ],
    "department": [
      "Electrical and Computer Engineering"
    ],
    "source_task_selection": [
      "all"
    ],
    "was_in_survey": [
      "mag"
    ],
    "in_title": [
      "t",
      "r",
      "l",
      "s"
    ],
    "in_abs": [
      "t",
      "r",
      "l",
      "s",
      "k"
    ],
    "in_contet": [
      "t",
      "r",
      "l",
      "s",
      "k"
    ],
    "rejected_or_unpublished": null,
    "duplication_status": null,
    "paper_for_thesis": 0,
    "paper_available": 1,
    "behind_paywall": 1,
    "paywall_circumventable": 1,
    "author_names": [
      "Farzaneh Shoeleh",
      "Masoud Asadpour"
    ]
  },
  {
    "id": 2976782275,
    "title": "MULTIPOLAR: Multi-Source Policy Aggregation for Transfer Reinforcement Learning between Diverse Environmental Dynamics",
    "abst": "Transfer reinforcement learning (RL) aims at improving learning efficiency of an agent by exploiting knowledge from other source agents trained on relevant tasks. However, it remains challenging to transfer knowledge between different environmental dynamics without having access to the source environments. In this work, we explore a new challenge in transfer RL, where only a set of source policies collected under unknown diverse dynamics is available for learning a target task efficiently. To address this problem, the proposed approach, MULTI-source POLicy AggRegation (MULTIPOLAR), comprises two key techniques. We learn to aggregate the actions provided by the source policies adaptively to maximize the target task performance. Meanwhile, we learn an auxiliary network that predicts residuals around the aggregated actions, which ensures the target policy&#039;s expressiveness even when some of the source policies perform poorly. We demonstrated the effectiveness of MULTIPOLAR through an extensive experimental evaluation across six simulated environments ranging from classic control problems to challenging robotics simulations, under both continuous and discrete action spaces.",
    "url": "https://arxiv.org/pdf/1909.13111",
    "lang": "en",
    "authors": [
      1972037053,
      2226145510,
      2976352458
    ],
    "fos": [
      2993214414,
      97541855,
      119857082,
      2992779150,
      92811239,
      34413123,
      2780665216,
      33923547,
      115051666,
      154945302
    ],
    "journals": [
      2597173376
    ],
    "conferences": [],
    "conference_series": [],
    "references": [
      1515851193,
      1592847719,
      2004030284,
      2031727428,
      2097381042,
      2102865756,
      2109910161,
      2141559023,
      2464736835,
      2604636228,
      2736601468,
      2781726626,
      2891287243,
      2895531857,
      2903218963,
      2905364877,
      2962808049,
      2962861113,
      2962974944,
      2963065769,
      2963120839,
      2963161674,
      2963262099,
      2963859851,
      2964022604,
      2964136501,
      2964262254,
      2964267629,
      2964848402,
      2967727187,
      2975593089
    ],
    "filter_matches": [
      "tr",
      "tl",
      "trl",
      "rl"
    ],
    "rank": 21736,
    "citation_count": 0,
    "estimated_citation_count": 0,
    "publication_date": "2019-09-25",
    "found_in": 1,
    "transfer_experiment_type": [
      "v",
      "a",
      "r",
      "t",
      "s",
      "s_i",
      "s_f"
    ],
    "transfer_experiment_subtype": [
      "diff-no",
      "lit"
    ],
    "transfer_data_type": [
      "pi",
      "pi_gen",
      "distil"
    ],
    "transfer_performance_metrics": [
      "j",
      "tr"
    ],
    "implementation": [
      "Custom",
      "CS",
      "bootstrapped",
      "Gym",
      "stable-baselines",
      "Figures",
      "Formulas",
      "Tables"
    ],
    "policy_type": [
      "MLP",
      "RPL",
      "MULTIPOLAR"
    ],
    "task_mappings": [
      "N/A"
    ],
    "autonomous_transfer": 1,
    "is_deep_rl": 1,
    "tags": [
      "2D",
      "Robotics",
      "Simulation",
      "Gym",
      "CartPole",
      "Acrobot",
      "LunarLanderContinuous",
      "Mujoco",
      "Hopper",
      "Ant",
      "InvertedPendulum",
      "ClassicControl"
    ],
    "allowed_learner": [
      "TD"
    ],
    "country": [
      "Germany",
      "Japan"
    ],
    "uni": [
      "Technical University of Munich",
      "OMRON SINIC X"
    ],
    "department": [
      "Industry",
      "Uni",
      "Colab"
    ],
    "source_task_selection": [
      "lib"
    ],
    "was_in_survey": [
      "mag"
    ],
    "in_title": [
      "t",
      "r",
      "l"
    ],
    "in_abs": [
      "t",
      "r",
      "l",
      "k"
    ],
    "in_contet": [
      "t",
      "r",
      "l",
      "k"
    ],
    "rejected_or_unpublished": null,
    "duplication_status": null,
    "paper_for_thesis": 0,
    "paper_available": 1,
    "behind_paywall": 0,
    "paywall_circumventable": 0,
    "author_names": [
      "Ryo Yonetani",
      "Mohammadamin Barekatain",
      "Masashi Hamaya"
    ]
  },
  {
    "id": 2562252866,
    "title": "Effective Transfer via Demonstrations in Reinforcement Learning: A Preliminary Study",
    "abst": "There are many successful methods for transferring information from one agent to another. One approach, taken in this work, is to have one (source) agent demonstrate a policy to a second (target) agent, and then have that second agent improve upon the policy. By allowing the target agent to observe the source agent&#039;s demonstrations, rather than relying on other types of direct knowledge transfer like Q-values, rules, or shared representations, we remove the need for the agents to know anything about each other&#039;s internal representation or have a shared language. In this work, we introduce a refinement to HAT, an existing transfer learning method, by integrating the target agent&#039;s confidence in its representation of the source agent&#039;s policy. Results show that a target agent can effectively 1) improve its initial performance relative to learning without transfer (jumpstart) and 2) improve its performance relative to the source agent (total reward). Furthermore, both the jumpstart and total reward are improved with this new refinement, relative to learning without transfer and relative to learning with HAT.",
    "url": "https://dblp.uni-trier.de/db/conf/aaaiss/aaaiss2016.html#WangT16",
    "lang": null,
    "authors": [
      2148762994,
      2561227233
    ],
    "fos": [
      41008148,
      154945302,
      47932503,
      2985789551,
      97541855,
      150899416,
      119857082,
      2776960227
    ],
    "journals": [],
    "conferences": [
      2334729844
    ],
    "conference_series": [
      1184914352
    ],
    "references": [
      1570448133,
      1976115983,
      2097381042,
      2113913482,
      2125055259,
      2137375617,
      2164114810,
      2169659168
    ],
    "filter_matches": [
      "tr",
      "tl",
      "trl",
      "rl"
    ],
    "rank": 21761,
    "citation_count": 1,
    "estimated_citation_count": 1,
    "publication_date": "2016-03-04",
    "found_in": 1,
    "transfer_experiment_type": [],
    "transfer_experiment_subtype": [
      "multi"
    ],
    "transfer_data_type": [
      "rule"
    ],
    "transfer_performance_metrics": [
      "j",
      "tr",
      "tt"
    ],
    "implementation": [
      "Custom",
      "CS",
      "Pseudo",
      "Figures",
      "Formulas",
      "Tables"
    ],
    "policy_type": [
      "Q"
    ],
    "task_mappings": [
      "N/A"
    ],
    "autonomous_transfer": 0,
    "is_deep_rl": 0,
    "tags": [
      "2D",
      "Simulation",
      "Navigation",
      "KeepAway",
      "MultiAgent",
      "RoboCup"
    ],
    "allowed_learner": [
      "TD"
    ],
    "country": [
      "USA"
    ],
    "uni": [
      "Washington State University"
    ],
    "department": [
      "EECS"
    ],
    "source_task_selection": [
      "all"
    ],
    "was_in_survey": [
      "mag"
    ],
    "in_title": [
      "t",
      "r",
      "l"
    ],
    "in_abs": [
      "t",
      "l",
      "k"
    ],
    "in_contet": [
      "t",
      "r",
      "l",
      "k"
    ],
    "rejected_or_unpublished": null,
    "duplication_status": null,
    "paper_for_thesis": 0,
    "paper_available": 1,
    "behind_paywall": 0,
    "paywall_circumventable": 0,
    "author_names": [
      "Matthew D. Taylor",
      "Zhaodong Wang"
    ]
  },
  {
    "id": 2977949361,
    "title": "Manufacturing Dispatching using Reinforcement and Transfer Learning.",
    "abst": "Efficient dispatching rule in manufacturing industry is key to ensure product on-time delivery and minimum past-due and inventory cost. Manufacturing, especially in the developed world, is moving towards on-demand manufacturing meaning a high mix, low volume product mix. This requires efficient dispatching that can work in dynamic and stochastic environments, meaning it allows for quick response to new orders received and can work over a disparate set of shop floor settings. In this paper we address this problem of dispatching in manufacturing. Using reinforcement learning (RL), we propose a new design to formulate the shop floor state as a 2-D matrix, incorporate job slack time into state representation, and design lateness and tardiness rewards function for dispatching purpose. However, maintaining a separate RL model for each production line on a manufacturing shop floor is costly and often infeasible. To address this, we enhance our deep RL model with an approach for dispatching policy transfer. This increases policy generalization and saves time and cost for model training and data collection. Experiments show that: (1) our approach performs the best in terms of total discounted reward and average lateness, tardiness, (2) the proposed policy transfer approach reduces training time and increases policy generalization.",
    "url": "http://arxiv.org/pdf/1910.02035.pdf",
    "lang": null,
    "authors": [
      1882546638,
      2126857182,
      2892795692
    ],
    "fos": [
      132439773,
      2778047078,
      99862985,
      97541855,
      126255220,
      2776731479,
      133462117,
      150899416,
      33923547,
      42475967,
      175700187
    ],
    "journals": [
      2597173376
    ],
    "conferences": [],
    "conference_series": [],
    "references": [
      64698994,
      137809328,
      159252758,
      281016700,
      1585546214,
      1797125234,
      1848094219,
      1875189495,
      1964335576,
      2016327843,
      2025369528,
      2027944401,
      2035057469,
      2069641426,
      2101761886,
      2119717200,
      2121863487,
      2133569115,
      2148922589,
      2155027007,
      2157846217,
      2164271762,
      2165698076,
      2170692285,
      2275596639,
      2532523702,
      2546571074,
      2744067593,
      2784831250,
      2789509513,
      2796121673,
      2796900070,
      2913175752,
      2959613672,
      2964090591
    ],
    "filter_matches": [
      "tr",
      "tl",
      "trl",
      "rl"
    ],
    "rank": 21771,
    "citation_count": 0,
    "estimated_citation_count": 0,
    "publication_date": "2019-10-04",
    "found_in": 1,
    "transfer_experiment_type": [
      "#",
      "t",
      "s",
      "s_i"
    ],
    "transfer_experiment_subtype": [
      "same_all"
    ],
    "transfer_data_type": [
      "pi",
      "manifold"
    ],
    "transfer_performance_metrics": [
      "j",
      "tr",
      "tt"
    ],
    "implementation": [
      "Custom",
      "CS",
      "Pseudo",
      "Formulas",
      "Figures",
      "Tables"
    ],
    "policy_type": [
      "DMD"
    ],
    "task_mappings": [
      "N/A"
    ],
    "autonomous_transfer": 0,
    "is_deep_rl": 1,
    "tags": [
      "Scheduling",
      "Dispatching",
      "Simulation"
    ],
    "allowed_learner": [
      "TD"
    ],
    "country": [
      "USA"
    ],
    "uni": [
      "Hitachi Industrial AI Lab"
    ],
    "department": [
      "Industry"
    ],
    "source_task_selection": [
      "h"
    ],
    "was_in_survey": [
      "mag"
    ],
    "in_title": [
      "t",
      "r",
      "l"
    ],
    "in_abs": [
      "t",
      "r",
      "l"
    ],
    "in_contet": [
      "t",
      "r",
      "l"
    ],
    "rejected_or_unpublished": null,
    "duplication_status": null,
    "paper_for_thesis": 0,
    "paper_available": 1,
    "behind_paywall": 0,
    "paywall_circumventable": 0,
    "author_names": [
      "Susumu Serita",
      "Chetan Gupta",
      "Shuai Zheng"
    ]
  },
  {
    "id": 2524747780,
    "title": "Learning a transfer function for reinforcement learning problems",
    "abst": "The goal of transfer learning algorithms is to utilize knowledge gained in a source task to speed up learning in a different but related target task. Recently, several transfer methods for reinforcement learning have been proposed. A lot of them require a mapping that relates features from the source task to those of the target task, and most of the time it is the task of a domain expert to hand code these mappings. This paper proposes a method to learn such a mapping automatically from interactions with the environment, using a probability tree to represent the probability that the optimal action learned in the source task is useful in the target task. Preliminary experiments show that our approach can learn a meaningful mapping that can be used to speed up learning through the execution of transferred actions during exploration.",
    "url": "https://lirias.kuleuven.be/bitstream/123456789/203706/1/Croonenborghs.pdf",
    "lang": null,
    "authors": [
      1275083776,
      1982051687,
      2016312802
    ],
    "fos": [
      150899416,
      119857082,
      41008148,
      97541855,
      154945302,
      3019959826,
      68339613,
      81299745,
      47932503,
      84780729
    ],
    "journals": [],
    "conferences": [
      2786958024
    ],
    "conference_series": [
      1184914352
    ],
    "references": [
      107556075,
      203338875,
      1506146479,
      1844908357,
      2014512216,
      2031727428,
      2033072307,
      2098723043,
      2121863487,
      2123995443,
      2133040789,
      2133632477,
      2153353285,
      2154328025,
      2154441654
    ],
    "filter_matches": [
      "tr",
      "tl",
      "trl",
      "rl"
    ],
    "rank": 21772,
    "citation_count": 3,
    "estimated_citation_count": 3,
    "publication_date": "2008-01-01",
    "found_in": 1,
    "transfer_experiment_type": [
      "s",
      "levels"
    ],
    "transfer_experiment_subtype": [
      "same_all"
    ],
    "transfer_data_type": [
      "Q"
    ],
    "transfer_performance_metrics": [
      "ap",
      "tr",
      "tt"
    ],
    "implementation": [
      "Custom",
      "CS",
      "Pseudo",
      "Formulas",
      "Figures"
    ],
    "policy_type": [
      "Q",
      "SARSA",
      "TILDE"
    ],
    "task_mappings": [
      "N/A"
    ],
    "autonomous_transfer": 0,
    "is_deep_rl": 0,
    "tags": [
      "2D",
      "Navigation",
      "Grid",
      "Simulation"
    ],
    "allowed_learner": [
      "TD"
    ],
    "country": [
      "Belgium"
    ],
    "uni": [
      "KH Kempen University College",
      "Katholiege Universiteit Leuven"
    ],
    "department": [
      "Biosciences and Technology",
      "Declarative Languages and Artifical Intelligence"
    ],
    "source_task_selection": [
      "h"
    ],
    "was_in_survey": [
      "mag"
    ],
    "in_title": [
      "t",
      "r",
      "l"
    ],
    "in_abs": [
      "t",
      "r",
      "l",
      "k"
    ],
    "in_contet": [
      "t",
      "r",
      "l",
      "k"
    ],
    "rejected_or_unpublished": null,
    "duplication_status": null,
    "paper_for_thesis": 0,
    "paper_available": 1,
    "behind_paywall": 0,
    "paywall_circumventable": 0,
    "author_names": [
      "Maurice Bruynooghe",
      "Kurt Driessens",
      "Tom Croonenborghs"
    ]
  },
  {
    "id": 2070732802,
    "title": "The role of temporal statistics in the transfer of experience in context-dependent reinforcement learning",
    "abst": "Reinforcement learning (RL) is an algorithmic theory for learning by experience optimal action control. Two widely discussed problems within this field are the temporal credit assignment problem and the transfer of experience. The temporal credit assignment problem postulates that deciding whether an action is good or bad may not be done upon right away because of delayed rewards. The problem of transferring experience investigates the question of how experience can be generalized and transferred from a familiar context, where it was acquired, to an unfamiliar context, where it may, nevertheless, prove helpful. We propose a controller for modelling such flexibility in a context-dependent reinforcement learning paradigm. The devised controller combines two alternatives of perfect learner algorithms. In the first alternative, rewards are predicted by individual objects presented in a temporal sequence. In the second alternative, rewards are predicted on the basis of successive pairs of objects. Simulations run on both deterministic and random temporal sequences show that only in case of deterministic sequences, a previously acquired context could be retrieved. This suggests a role of temporal sequence information in the generalization and transfer of experience.",
    "url": "https://ieeexplore.ieee.org/document/7086184/",
    "lang": "en",
    "authors": [
      2809142581
    ],
    "fos": [
      97541855,
      2994071553,
      47932503,
      183322885,
      79699506,
      41008148,
      196340769,
      154945302,
      2988340862,
      2993349903
    ],
    "journals": [],
    "conferences": [],
    "conference_series": [
      2898576462
    ],
    "references": [
      169931978,
      1550112914,
      1987626674,
      2007267229,
      2047125104,
      2121863487,
      2124285588,
      2127469248,
      2128084896,
      2167362547,
      2888909284
    ],
    "filter_matches": [
      "tr",
      "tl",
      "trl",
      "rl"
    ],
    "rank": 21790,
    "citation_count": 6,
    "estimated_citation_count": 6,
    "publication_date": "2014-12-01",
    "found_in": 1,
    "transfer_experiment_type": [],
    "transfer_experiment_subtype": [],
    "transfer_data_type": [],
    "transfer_performance_metrics": [],
    "implementation": [
      "Custom",
      "CS"
    ],
    "policy_type": [],
    "task_mappings": [],
    "autonomous_transfer": 2,
    "is_deep_rl": 2,
    "tags": [
      "ANN",
      "HIS",
      "TemporalCredit",
      "Simulation"
    ],
    "allowed_learner": [],
    "country": [
      "Kuwait"
    ],
    "uni": [
      "Arab Open University"
    ],
    "department": [
      "Computer Studies"
    ],
    "source_task_selection": [],
    "was_in_survey": [
      "mag"
    ],
    "in_title": [
      "t",
      "r",
      "l"
    ],
    "in_abs": [
      "t",
      "r",
      "l"
    ],
    "in_contet": [
      "t",
      "r",
      "l",
      "k",
      "s"
    ],
    "rejected_or_unpublished": null,
    "duplication_status": null,
    "paper_for_thesis": 0,
    "paper_available": 1,
    "behind_paywall": 1,
    "paywall_circumventable": 1,
    "author_names": [
      "Oussama H. Hamid"
    ]
  },
  {
    "id": 1569756368,
    "title": "Reinforcement learning transfer using a sparse coded inter-task mapping",
    "abst": "Reinforcement learning agents can successfully learn in a variety of difficult tasks. A fundamental problem is that they may learn slowly in complex environments, inspiring the development of speedup methods such as transfer learning. Transfer improves learning by reusing learned behaviors in similar tasks, usually via an inter-task mapping, which defines how a pair of tasks are related. This paper proposes a novel transfer learning technique to autonomously construct an inter-task mapping by using a novel combinations of sparse coding, sparse projection learning, and sparse pseudo-input gaussian processes. Experiments show successful transfer of information between two very different domains: the mountain car and the pole swing-up task. This paper empirically shows that the learned inter-task mapping can be used to successfully (1) improve the performance of a learned policy on a fixed number of samples, (2) reduce the learning times needed by the algorithms to converge to a policy on a fixed number of samples, and (3) converge faster to a near-optimal policy given a large amount of samples.",
    "url": "https://core.ac.uk/display/158764893",
    "lang": "en",
    "authors": [
      2027870957,
      2142969897,
      2148762994,
      2215740056
    ],
    "fos": [
      28006648,
      68339613,
      41008148,
      150899416,
      77075516,
      77637269,
      119857082,
      61326573,
      199190896,
      97541855,
      154945302
    ],
    "journals": [],
    "conferences": [
      90044701
    ],
    "conference_series": [
      1168823384
    ],
    "references": [
      110451278,
      203338875,
      1515851193,
      1626155273,
      1746819321,
      2027197817,
      2079247031,
      2097381042,
      2098723043,
      2099768828,
      2113606819,
      2114537044,
      2121863487,
      2123995443,
      2130005627,
      2153353285,
      2154328025,
      2158150115,
      2164114810,
      2492778706,
      2834852173,
      2911283634
    ],
    "filter_matches": [
      "tr",
      "tl",
      "trl",
      "rl"
    ],
    "rank": 21794,
    "citation_count": 9,
    "estimated_citation_count": 9,
    "publication_date": "2011-11-14",
    "found_in": 1,
    "transfer_experiment_type": [
      "t",
      "a",
      "v"
    ],
    "transfer_experiment_subtype": [
      "diff-no",
      "lit"
    ],
    "transfer_data_type": [
      "Q",
      "I"
    ],
    "transfer_performance_metrics": [
      "tt",
      "tr"
    ],
    "implementation": [
      "Custom",
      "CS",
      "Formulas",
      "Figures",
      "Pseudo"
    ],
    "policy_type": [
      "LSPI",
      "FQI"
    ],
    "task_mappings": [
      "exp"
    ],
    "autonomous_transfer": 0,
    "is_deep_rl": 0,
    "tags": [
      "2D",
      "Simulation",
      "MountainCar",
      "InvertedPendulum",
      "CartPole",
      "ClassicControl"
    ],
    "allowed_learner": [
      "TD",
      "batch"
    ],
    "country": [
      "Netherlands",
      "USA"
    ],
    "uni": [
      "Maastricht University",
      "Lafayette College"
    ],
    "department": [],
    "source_task_selection": [
      "h"
    ],
    "was_in_survey": [
      "mag"
    ],
    "in_title": [
      "t",
      "r",
      "l"
    ],
    "in_abs": [
      "t",
      "r",
      "l"
    ],
    "in_contet": [
      "t",
      "r",
      "l",
      "k"
    ],
    "rejected_or_unpublished": null,
    "duplication_status": 1,
    "paper_for_thesis": 126751897,
    "paper_available": 1,
    "behind_paywall": 0,
    "paywall_circumventable": 0,
    "author_names": [
      "Karl Tuyls",
      "Gerhard Weiss",
      "Matthew D. Taylor",
      "Haitham Bou Ammar"
    ]
  },
  {
    "id": 2900471431,
    "title": "Feature Learning and Transfer Performance Prediction for Video Reinforcement Learning Tasks via a Siamese Convolutional Neural Network",
    "abst": "In this paper, we handle the negative transfer problem by a deep learning method to predict the transfer performance (positive/negative transfer) between two reinforcement learning tasks. We consider same domain transfer for video reinforcement learning tasks such as video games which can be described as images and perceived by an agent with visual ability. Our method directly trains a neural network from raw task descriptions without other prior knowledge such as models of tasks, target task samples and human experience. The architecture of our neural network consists of two parts: a siamese convolutional neural network to learn the features of each pair of tasks and a softmax layer to predict the binary transfer performance. We conduct extensive experiments in the maze domain and the Ms. PacMan domain to evaluate the performance of our method. The results show the effectiveness and superiority of our method compared with the baseline methods.",
    "url": "https://link.springer.com/chapter/10.1007/978-3-030-04167-0_32",
    "lang": "en",
    "authors": [
      2231309555,
      2594397125,
      2992037913
    ],
    "fos": [
      119857082,
      81363708,
      41008148,
      188441871,
      97541855,
      59404180,
      154945302,
      108583219,
      2779178101,
      150899416,
      50644808
    ],
    "journals": [],
    "conferences": [
      2891292088
    ],
    "conference_series": [
      1183580825
    ],
    "references": [
      158722652,
      1522301498,
      1557517019,
      1806891645,
      1969685488,
      1973627122,
      1990654808,
      2004030284,
      2097381042,
      2097498341,
      2114580749,
      2126565096,
      2132057084,
      2141559023,
      2163605009,
      2271262891,
      2464736835,
      2595845486,
      2893403646,
      2919115771
    ],
    "filter_matches": [
      "tr",
      "tl",
      "trl",
      "rl"
    ],
    "rank": 21799,
    "citation_count": 2,
    "estimated_citation_count": 2,
    "publication_date": "2018-12-13",
    "found_in": 1,
    "transfer_experiment_type": [
      "#",
      "s_i",
      "s_f",
      "s",
      "levels"
    ],
    "transfer_experiment_subtype": [
      "multi"
    ],
    "transfer_data_type": [
      "Q",
      "fea"
    ],
    "transfer_performance_metrics": [
      "j",
      "tt"
    ],
    "implementation": [
      "Custom",
      "CS",
      "Figures",
      "Pseudo",
      "Formulas",
      "Tables"
    ],
    "policy_type": [
      "SARSA",
      "Q"
    ],
    "task_mappings": [
      "exp"
    ],
    "autonomous_transfer": 0,
    "is_deep_rl": 1,
    "tags": [
      "2D",
      "Navigation",
      "Games",
      "VideoGames",
      "CNN",
      "Siamese",
      "Atari",
      "PacMan",
      "Maze",
      "Simulation"
    ],
    "allowed_learner": [
      "TD"
    ],
    "country": [
      "China"
    ],
    "uni": [
      "Nanjing University"
    ],
    "department": [
      "Key Laboratory for Novel Software Technology",
      "Collaborative Innofation of Novel Software Technology and Industrialization"
    ],
    "source_task_selection": [
      "all"
    ],
    "was_in_survey": [
      "mag"
    ],
    "in_title": [
      "t",
      "r",
      "l"
    ],
    "in_abs": [
      "t",
      "r",
      "l",
      "k"
    ],
    "in_contet": [
      "t",
      "r",
      "l",
      "k"
    ],
    "rejected_or_unpublished": null,
    "duplication_status": null,
    "paper_for_thesis": 0,
    "paper_available": 1,
    "behind_paywall": 1,
    "paywall_circumventable": 1,
    "author_names": [
      "Jinhua Song",
      "Hao Wang",
      "Yang Gao"
    ]
  },
  {
    "id": 2980820015,
    "title": "Modelling Generalized Forces with Reinforcement Learning for Sim-to-Real Transfer.",
    "abst": "Learning robotic control policies in the real world gives rise to challenges in data efficiency, safety, and controlling the initial condition of the system. On the other hand, simulations are a useful alternative as they provide an abundant source of data without the restrictions of the real world. Unfortunately, simulations often fail to accurately model complex real-world phenomena. Traditional system identification techniques are limited in expressiveness by the analytical model parameters, and usually are not sufficient to capture such phenomena. In this paper we propose a general framework for improving the analytical model by optimizing state dependent generalized forces. State dependent generalized forces are expressive enough to model constraints in the equations of motion, while maintaining a clear physical meaning and intuition. We use reinforcement learning to efficiently optimize the mapping from states to generalized forces over a discounted infinite horizon. We show that using only minutes of real world data improves the sim-to-real control policy transfer. We demonstrate the feasibility of our approach by validating it on a nonprehensile manipulation task on the Sawyer robot.",
    "url": "https://www.arxiv-vanity.com/papers/1910.09471/",
    "lang": "en",
    "authors": [
      158011298,
      1971811636,
      2086424281,
      2206974089,
      2237408843,
      2908955082,
      2923522702,
      2951123884,
      2952076939
    ],
    "fos": [
      97541855,
      90509273,
      26955809,
      119247159,
      2984679396,
      159759114,
      14037181,
      113614550,
      133731056,
      127413603
    ],
    "journals": [
      2596519289
    ],
    "conferences": [],
    "conference_series": [],
    "references": [
      1495727713,
      2121863487,
      2155007355,
      2158782408,
      2595845486,
      2726187156,
      2767050701,
      2785738552,
      2785962646,
      2786036274,
      2788781499,
      2795341696,
      2885596080,
      2892230114,
      2911087563,
      2962872206,
      2962899390,
      2962957005,
      2963184939,
      2963411833,
      2963674921,
      2963884015,
      2964118020,
      3005347330
    ],
    "filter_matches": [
      "tr",
      "tl",
      "trl",
      "rl"
    ],
    "rank": 21804,
    "citation_count": 2,
    "estimated_citation_count": 2,
    "publication_date": "2019-10-21",
    "found_in": 1,
    "transfer_experiment_type": [
      "t"
    ],
    "transfer_experiment_subtype": [
      "multi",
      "sim2real"
    ],
    "transfer_data_type": [
      "pi"
    ],
    "transfer_performance_metrics": [
      "tr",
      "tt",
      "ap"
    ],
    "implementation": [
      "Custom",
      "CS",
      "Formulas",
      "Figures",
      "Mujoco",
      "SciPy",
      "Tables",
      "Videos"
    ],
    "policy_type": [
      "MPO"
    ],
    "task_mappings": [
      "N/A"
    ],
    "autonomous_transfer": 0,
    "is_deep_rl": 1,
    "tags": [
      "Robotics",
      "Simulation",
      "RealWorld",
      "Arm",
      "Grab"
    ],
    "allowed_learner": [
      "TD"
    ],
    "country": [
      "UK"
    ],
    "uni": [
      "DeepMind"
    ],
    "department": [
      "Industry"
    ],
    "source_task_selection": [
      "all"
    ],
    "was_in_survey": [
      "mag"
    ],
    "in_title": [
      "t",
      "r",
      "l"
    ],
    "in_abs": [
      "t",
      "r",
      "l"
    ],
    "in_contet": [
      "t",
      "r",
      "l",
      "k"
    ],
    "rejected_or_unpublished": 2,
    "duplication_status": null,
    "paper_for_thesis": 0,
    "paper_available": 1,
    "behind_paywall": 0,
    "paywall_circumventable": 0,
    "author_names": [
      "Yuval Tassa",
      "Thomas Lampe",
      "Tom Erez",
      "Abbas Abdolmaleki",
      "Francesco Romano",
      "Francesco Nori",
      "Jackie Kay",
      "Thomas Roth&#x00F6;rl",
      "Rae Jeong"
    ]
  },
  {
    "id": 2967582614,
    "title": "Skill Transfer in Deep Reinforcement Learning under Morphological Heterogeneity.",
    "abst": "Transfer learning methods for reinforcement learning (RL) domains facilitate the acquisition of new skills using previously acquired knowledge. The vast majority of existing approaches assume that the agents have the same design, e.g. same shape and action spaces. In this paper we address the problem of transferring previously acquired skills amongst morphologically different agents (MDAs). For instance, assuming that a bipedal agent has been trained to move forward, could this skill be transferred on to a one-leg hopper so as to make its training process for the same task more sample efficient? We frame this problem as one of subspace learning whereby we aim to infer latent factors representing the control mechanism that is common between MDAs. We propose a novel paired variational encoder-decoder model, PVED, that disentangles the control of MDAs into shared and agent-specific factors. The shared factors are then leveraged for skill transfer using RL. Theoretically, we derive a theorem indicating how the performance of PVED depends on the shared factors and agent morphologies. Experimentally, PVED has been extensively validated on four MuJoCo environments. We demonstrate its performance compared to a state-of-the-art approach and several ablation cases, visualize and interpret the hidden factors, and identify avenues for future improvements.",
    "url": "https://arxiv.org/pdf/1908.05265v2",
    "lang": "en",
    "authors": [
      2103014545,
      2968802471
    ],
    "fos": [
      33923547,
      154945302,
      2986635905,
      150899416,
      97541855,
      119857082,
      32834561
    ],
    "journals": [
      2597173376
    ],
    "conferences": [],
    "conference_series": [],
    "references": [
      1959608418,
      2049273221,
      2091714857,
      2097381042,
      2109910161,
      2145339207,
      2147168033,
      2173248099,
      2187089797,
      2257979135,
      2426267443,
      2527569769,
      2553882142,
      2559823555,
      2585083595,
      2594829461,
      2604636228,
      2604763608,
      2605368761,
      2620290674,
      2736601468,
      2753738274,
      2767050701,
      2785342287,
      2785948534,
      2787501667,
      2798434327,
      2803281228,
      2805481182,
      2889970038,
      2890208753,
      2890260778,
      2903447353,
      2921528247,
      2945322269,
      2946734487,
      2962717849,
      2962817122,
      2962897886,
      2962957005,
      2962974944,
      2963262099,
      2963430173,
      2963547174,
      2963577679,
      2963594498,
      2963661429,
      2963790827,
      2963859851,
      2963875819,
      2963891243,
      2964121744,
      2964121937,
      2964161785,
      2964173023,
      2964262254,
      2964335674,
      2964342357
    ],
    "filter_matches": [
      "tr",
      "tl",
      "trl",
      "rl"
    ],
    "rank": 21810,
    "citation_count": 1,
    "estimated_citation_count": 1,
    "publication_date": "2019-08-14",
    "found_in": 1,
    "transfer_experiment_type": [
      "v",
      "a"
    ],
    "transfer_experiment_subtype": [
      "lit"
    ],
    "transfer_data_type": [
      "VAE",
      "latent"
    ],
    "transfer_performance_metrics": [
      "j",
      "tr",
      "ap",
      "tt"
    ],
    "implementation": [
      "Custom",
      "CS",
      "Theorem",
      "Figures",
      "Formulas",
      "Gym",
      "Tables"
    ],
    "policy_type": [
      "PPO",
      "PVED"
    ],
    "task_mappings": [
      "N/A"
    ],
    "autonomous_transfer": 0,
    "is_deep_rl": 1,
    "tags": [
      "2D",
      "Robotics",
      "Mujoco",
      "Hopper",
      "Bipedal",
      "Walker2d",
      "Jaco3",
      "Simulation"
    ],
    "allowed_learner": [
      "TD"
    ],
    "country": [
      "UK"
    ],
    "uni": [
      "University of Warwick"
    ],
    "department": [
      "Warwick Manufacturing Group"
    ],
    "source_task_selection": [
      "h"
    ],
    "was_in_survey": [
      "mag"
    ],
    "in_title": [
      "t",
      "r",
      "l",
      "s"
    ],
    "in_abs": [
      "t",
      "r",
      "l",
      "s",
      "k"
    ],
    "in_contet": [
      "t",
      "r",
      "l",
      "s",
      "k"
    ],
    "rejected_or_unpublished": 2,
    "duplication_status": null,
    "paper_for_thesis": 0,
    "paper_available": 1,
    "behind_paywall": 0,
    "paywall_circumventable": 0,
    "author_names": [
      "Giovanni Montana",
      "Yang Hu"
    ]
  },
  {
    "id": 2123178778,
    "title": "Transfer of knowledge for a climbing Virtual Human: A reinforcement learning approach",
    "abst": "In the reinforcement learning literature, transfer is the capability to reuse on a new problem what has been learnt from previous experiences on similar problems. Adapting transfer properties for robotics is a useful challenge because it can reduce the time spent in the first exploration phase on a new problem. In this paper we present a transfer framework adapted to the case of a climbing Virtual Human (VH). We show that our VH learns faster to climb a wall after having learnt on a different previous wall.",
    "url": "https://ieeexplore.ieee.org/document/5152553/",
    "lang": "en",
    "authors": [
      29249005,
      1236455448,
      2228169762
    ],
    "fos": [
      97541855,
      127413603,
      90509273,
      34413123,
      60692881,
      150303390,
      95038775,
      2776960227,
      136389625,
      154945302
    ],
    "journals": [],
    "conferences": [
      56912375
    ],
    "conference_series": [
      1163902177
    ],
    "references": [
      32403112,
      1483845297,
      1491843047,
      1515851193,
      1520348301,
      1573240230,
      1796544916,
      1974939912,
      2057226972,
      2079247031,
      2111237478,
      2121863487,
      2122364547,
      2128270866,
      2152083940,
      2156504236,
      2160863137,
      2164040945,
      2247191580,
      2402652894,
      2545425704
    ],
    "filter_matches": [
      "tr",
      "tl",
      "trl",
      "rl"
    ],
    "rank": 21811,
    "citation_count": 3,
    "estimated_citation_count": 3,
    "publication_date": "2009-05-12",
    "found_in": 1,
    "transfer_experiment_type": [
      "s_i",
      "s_f"
    ],
    "transfer_experiment_subtype": [
      "same_all"
    ],
    "transfer_data_type": [
      "Q",
      "fea"
    ],
    "transfer_performance_metrics": [
      "j",
      "tt"
    ],
    "implementation": [
      "Custom",
      "CS",
      "Formulas",
      "Pseudo",
      "Tables",
      "Figures"
    ],
    "policy_type": [
      "Dyna-Q"
    ],
    "task_mappings": [
      "N/A"
    ],
    "autonomous_transfer": 0,
    "is_deep_rl": 0,
    "tags": [
      "Simulation",
      "Robotics",
      "Climbing",
      "Human",
      "Joints"
    ],
    "allowed_learner": [
      "TD"
    ],
    "country": [
      "France"
    ],
    "uni": [
      "Universit Paris 6",
      "Laboratoire dIntgration des Systmes et des Technologies in Commissariat  lnergie Atomique"
    ],
    "department": [
      "Intelligent Systems and Robotics",
      "Industry",
      "Uni",
      "Colab"
    ],
    "source_task_selection": [
      "h"
    ],
    "was_in_survey": [
      "mag"
    ],
    "in_title": [
      "t",
      "r",
      "l",
      "k"
    ],
    "in_abs": [
      "t",
      "r",
      "l"
    ],
    "in_contet": [
      "t",
      "r",
      "l",
      "k"
    ],
    "rejected_or_unpublished": null,
    "duplication_status": null,
    "paper_for_thesis": 0,
    "paper_available": 1,
    "behind_paywall": 0,
    "paywall_circumventable": 0,
    "author_names": [
      "Olivier Sigaud",
      "Alain Micaelli",
      "Benoit Libeau"
    ]
  },
  {
    "id": 2056107474,
    "title": "Efficient Knowledge Transfer in Shaping Reinforcement Learning",
    "abst": "Abstract Reinforcement learning is an attractive solution for deriving an optimal control policy by on-line exploration of the control task. Shaping aims to accelerate reinforcement learning by starting from easy tasks and gradually increasing the complexity, until the original task is solved. In this paper, we consider the essential decision on when to transfer learning from an easier task to a more difficult one, so that the total learning time is reduced. We propose two transfer criteria for making this decision, based on the agent&#039;s performance. The first criterion measures the agent&#039;s performance by the distance between its current solution and the optimal one, and the second by the empirical return obtained. We investigate the learning time gains achieved by using these criteria in a classical gridworld navigation benchmark. This numerical study also serves to compare several major shaping techniques.",
    "url": "http://www.sciencedirect.com/science/article/pii/S1474667016450536",
    "lang": "en",
    "authors": [
      1988337178,
      2628456558,
      2892966320
    ],
    "fos": [
      150899416,
      97541855,
      188888258,
      77967617,
      199190896,
      154945302,
      119857082,
      188116033,
      41008148,
      77075516,
      28006648
    ],
    "journals": [
      2898246533
    ],
    "conferences": [],
    "conference_series": [],
    "references": [
      32403112,
      1507591516,
      1557517019,
      1592853747,
      1777239053,
      1996847178,
      2012036715,
      2037260614,
      2071311198,
      2093349877,
      2097381042,
      2121863487,
      2125074935,
      2171578145
    ],
    "filter_matches": [
      "tr",
      "tl",
      "trl",
      "rl"
    ],
    "rank": 21852,
    "citation_count": 2,
    "estimated_citation_count": 2,
    "publication_date": "2011-01-01",
    "found_in": 1,
    "transfer_experiment_type": [
      "t",
      "r",
      "s",
      "a"
    ],
    "transfer_experiment_subtype": [
      "multi"
    ],
    "transfer_data_type": [
      "Q"
    ],
    "transfer_performance_metrics": [
      "j",
      "tt"
    ],
    "implementation": [
      "Custom",
      "CS",
      "Formulas",
      "Figures",
      "Tables"
    ],
    "policy_type": [
      "Q"
    ],
    "task_mappings": [
      "N/A"
    ],
    "autonomous_transfer": 0,
    "is_deep_rl": 0,
    "tags": [
      "2D",
      "Navigation",
      "Grid",
      "Simulation",
      "Obstacle"
    ],
    "allowed_learner": [
      "TD"
    ],
    "country": [
      "Netherlands"
    ],
    "uni": [
      "Delft University of Technology"
    ],
    "department": [
      "Center for Systems and Control"
    ],
    "source_task_selection": [
      "h"
    ],
    "was_in_survey": [
      "mag"
    ],
    "in_title": [
      "t",
      "r",
      "l",
      "k"
    ],
    "in_abs": [
      "t",
      "r",
      "l"
    ],
    "in_contet": [
      "t",
      "r",
      "l",
      "k"
    ],
    "rejected_or_unpublished": null,
    "duplication_status": null,
    "paper_for_thesis": 0,
    "paper_available": 1,
    "behind_paywall": 0,
    "paywall_circumventable": 0,
    "author_names": [
      "Robert Babuska",
      "Sholeh Norouzzadeh",
      "Lucian Busoniu"
    ]
  },
  {
    "id": 2751021526,
    "title": "Towards Knowledge Transfer in Deep Reinforcement Learning",
    "abst": null,
    "url": "http://jglobal.jst.go.jp/en/public/20090422/201702272924443642",
    "lang": "ja",
    "authors": [
      2746361332,
      2748219145,
      2749633163
    ],
    "fos": [
      56739046,
      97541855,
      2776960227,
      127413603
    ],
    "journals": [],
    "conferences": [],
    "conference_series": [],
    "references": [],
    "filter_matches": [
      "tr",
      "tl",
      "trl",
      "rl"
    ],
    "rank": 21878,
    "citation_count": 0,
    "estimated_citation_count": 0,
    "publication_date": "2016-01-01",
    "found_in": 1,
    "transfer_experiment_type": [
      "s",
      "levels",
      "t"
    ],
    "transfer_experiment_subtype": [
      "diff-no"
    ],
    "transfer_data_type": [
      "pi"
    ],
    "transfer_performance_metrics": [
      "j",
      "tt"
    ],
    "implementation": [
      "Custom",
      "CS",
      "Formulas",
      "Figures",
      "Pseudo"
    ],
    "policy_type": [
      "DQN"
    ],
    "task_mappings": [
      "N/A"
    ],
    "autonomous_transfer": 0,
    "is_deep_rl": 1,
    "tags": [
      "2D",
      "Games",
      "VideoGames",
      "Simulation",
      "Atari",
      "Breakout",
      "Atlantis",
      "Boxing"
    ],
    "allowed_learner": [
      "TD"
    ],
    "country": [
      "Brazil"
    ],
    "uni": [
      "Escola Politcnica da Universidade de Sao Paulo"
    ],
    "department": [],
    "source_task_selection": [
      "h"
    ],
    "was_in_survey": [
      "mag",
      "silva"
    ],
    "in_title": [
      "t",
      "r",
      "l",
      "k"
    ],
    "in_abs": [
      "t",
      "r",
      "l",
      "k"
    ],
    "in_contet": [
      "t",
      "r",
      "l",
      "k",
      "s"
    ],
    "rejected_or_unpublished": null,
    "duplication_status": null,
    "paper_for_thesis": 0,
    "paper_available": 1,
    "behind_paywall": 0,
    "paywall_circumventable": 0,
    "author_names": [
      "Silva Felipe Leno da",
      "Costa Anna Helena Reali",
      "Glatt Ruben"
    ]
  },
  {
    "id": 3010496493,
    "title": "Transferring Human Manipulation Knowledge to Robots with Inverse Reinforcement Learning",
    "abst": null,
    "url": "https://vbn.aau.dk/da/publications/transferring-human-manipulation-knowledge-to-robots-with-inverse-",
    "lang": "en",
    "authors": [
      2071487764,
      2754951293,
      2755566491,
      2755784475
    ],
    "fos": [
      127413603,
      90509273,
      145460709,
      2985814175,
      107457646,
      34413123,
      154945302,
      133731056
    ],
    "journals": [],
    "conferences": [],
    "conference_series": [
      2624205572
    ],
    "references": [],
    "filter_matches": [
      "tr",
      "tl",
      "trl",
      "rl"
    ],
    "rank": 21898,
    "citation_count": 0,
    "estimated_citation_count": 0,
    "publication_date": "2020-03-08",
    "found_in": 1,
    "transfer_experiment_type": [
      "t"
    ],
    "transfer_experiment_subtype": [
      "same_all"
    ],
    "transfer_data_type": [
      "I"
    ],
    "transfer_performance_metrics": [],
    "implementation": [
      "Custom",
      "CS",
      "Figures",
      "Formulas"
    ],
    "policy_type": [
      "Q"
    ],
    "task_mappings": [],
    "autonomous_transfer": 0,
    "is_deep_rl": 0,
    "tags": [
      "Robotics",
      "Simulation",
      "RealWorld",
      "Imitation",
      "UniversalRobot"
    ],
    "allowed_learner": [
      "TD"
    ],
    "country": [
      "Denmark"
    ],
    "uni": [
      "Aalborg University"
    ],
    "department": [
      "Robotics and Automation"
    ],
    "source_task_selection": [
      "h"
    ],
    "was_in_survey": [
      "mag"
    ],
    "in_title": [
      "t",
      "r",
      "l",
      "k"
    ],
    "in_abs": [
      "t",
      "r",
      "l"
    ],
    "in_contet": [
      "t",
      "r",
      "l",
      "k",
      "s"
    ],
    "rejected_or_unpublished": null,
    "duplication_status": null,
    "paper_for_thesis": 0,
    "paper_available": 1,
    "behind_paywall": 0,
    "paywall_circumventable": 0,
    "author_names": [
      "Simon B&#x00F8;gh",
      "Rasmus Eckholdt Andersen",
      "Emil Blixt Hansen",
      "Steffen Madsen"
    ]
  },
  {
    "id": 2945719686,
    "title": "Improving Deep Reinforcement Learning via Transfer",
    "abst": "While achieving the state-of-the-art performance in complex sequential tasks, deep reinforcement learning (deep RL) remains extremely data inefficient. Many approaches have been studied to improve the data efficiency of deep RL algorithms. This dissertation focuses on leveraging various transfer learning techniques to tackle this problem. We first show that positive transfer can be achieved cross-domains via direct weight transfer if the two agents share a certain amount of similarities. Then we look into how could the similarity between cross-domain tasks be quantified, such that we only transfer useful information from one task to another while blocking information that might have a negative effect. The third direction we studied is the human-agent transfer mechanism, which we integrate human knowledge via supervised pre-training on a set of demonstration data collected from a human then transfer to an agent. Lastly, several future directions are proposed for the remainder of this dissertation.",
    "url": "https://dl.acm.org/citation.cfm?id=3332128",
    "lang": null,
    "authors": [
      2755878461
    ],
    "fos": [
      39613435,
      41008148,
      108583219,
      159759114,
      154945302,
      2993826840,
      2992740192,
      119857082,
      114173007,
      97541855,
      150899416
    ],
    "journals": [],
    "conferences": [
      2942726028
    ],
    "conference_series": [
      1168671587
    ],
    "references": [],
    "filter_matches": [
      "tr",
      "tl",
      "trl",
      "rl"
    ],
    "rank": 21911,
    "citation_count": 1,
    "estimated_citation_count": 1,
    "publication_date": "2019-05-08",
    "found_in": 1,
    "transfer_experiment_type": [],
    "transfer_experiment_subtype": [
      "theory"
    ],
    "transfer_data_type": [],
    "transfer_performance_metrics": [],
    "implementation": [],
    "policy_type": [],
    "task_mappings": [],
    "autonomous_transfer": 2,
    "is_deep_rl": 2,
    "tags": [
      "CNN"
    ],
    "allowed_learner": [],
    "country": [
      "USA"
    ],
    "uni": [
      "Washington State University"
    ],
    "department": [],
    "source_task_selection": [],
    "was_in_survey": [
      "mag"
    ],
    "in_title": [
      "t",
      "r",
      "l"
    ],
    "in_abs": [
      "t",
      "r",
      "l",
      "k"
    ],
    "in_contet": [
      "t",
      "r",
      "l",
      "k"
    ],
    "rejected_or_unpublished": null,
    "duplication_status": null,
    "paper_for_thesis": 0,
    "paper_available": 1,
    "behind_paywall": 0,
    "paywall_circumventable": 0,
    "author_names": [
      "Yunshu Du"
    ]
  },
  {
    "id": 3024199149,
    "title": "Efficient Deep Reinforcement Learning via Adaptive Policy Transfer",
    "abst": "Transfer Learning (TL) has shown great potential to accelerate Reinforcement Learning (RL) by leveraging prior knowledge from past learned policies of relevant tasks. Existing transfer approaches either explicitly computes the similarity between tasks or select appropriate source policies to provide guided explorations for the target task. However, how to directly optimize the target policy by alternatively utilizing knowledge from appropriate source policies without explicitly measuring the similarity is currently missing. In this paper, we propose a novel Policy Transfer Framework (PTF) to accelerate RL by taking advantage of this idea. Our framework learns when and which source policy is the best to reuse for the target policy and when to terminate it by modeling multi-policy transfer as the option learning problem. PTF can be easily combined with existing deep RL approaches. Experimental results show it significantly accelerates the learning process and surpasses state-of-the-art policy transfer methods in terms of learning efficiency and final performance in both discrete and continuous action spaces.",
    "url": "http://arxiv.org/pdf/2002.08037.pdf",
    "lang": null,
    "authors": [
      2106362459,
      2225722044,
      2574093644,
      2583591803,
      2792122007,
      2886484655,
      2890294771,
      2964471977,
      3006722961,
      3007369931,
      3027827360
    ],
    "fos": [
      119857082,
      206588197,
      2776731479,
      97541855,
      150899416,
      33923547,
      3020332338,
      154945302
    ],
    "journals": [
      2597173376
    ],
    "conferences": [],
    "conference_series": [],
    "references": [
      199552065,
      1757796397,
      2031727428,
      2097381042,
      2108535023,
      2109910161,
      2113953866,
      2119717200,
      2133458291,
      2145339207,
      2153874061,
      2158782408,
      2312609093,
      2584377191,
      2604618034,
      2604636228,
      2605369401,
      2736601468,
      2771734675,
      2775536965,
      2781585732,
      2789517807,
      2945438069,
      2952012721,
      2963065769,
      2963142324,
      2963221965,
      2963864421,
      2963946410,
      2964043796
    ],
    "filter_matches": [
      "tr",
      "tl",
      "trl",
      "rl"
    ],
    "rank": 21916,
    "citation_count": 0,
    "estimated_citation_count": 0,
    "publication_date": "2020-02-19",
    "found_in": 1,
    "transfer_experiment_type": [
      "s_i",
      "s_f",
      "s",
      "levels",
      "t"
    ],
    "transfer_experiment_subtype": [
      "multi"
    ],
    "transfer_data_type": [
      "pi",
      "pi_dyn",
      "options"
    ],
    "transfer_performance_metrics": [
      "j",
      "tr",
      "tt",
      "ap"
    ],
    "implementation": [
      "Custom",
      "CS",
      "fakeOSS",
      "Figures",
      "Formulas",
      "Pseudo"
    ],
    "policy_type": [
      "PTF-A3C",
      "PTF-PPO",
      "A3C",
      "PPO"
    ],
    "task_mappings": [
      "N/A"
    ],
    "autonomous_transfer": 0,
    "is_deep_rl": 1,
    "tags": [
      "2D",
      "Navigation",
      "Grid",
      "Simulation",
      "Games",
      "Pinball",
      "Reacher",
      "Robotics"
    ],
    "allowed_learner": [
      "TD",
      "H"
    ],
    "country": [
      "China"
    ],
    "uni": [
      "Tianjin University",
      "Huawei",
      "Nanjing University",
      "Netease",
      "JD Digits"
    ],
    "department": [
      "Intelligence and Computing",
      "Noahs Ark Lab",
      "Machine learning"
    ],
    "source_task_selection": [],
    "was_in_survey": [
      "mag"
    ],
    "in_title": [
      "t",
      "r",
      "l"
    ],
    "in_abs": [
      "t",
      "r",
      "l",
      "k"
    ],
    "in_contet": [
      "t",
      "r",
      "l",
      "k"
    ],
    "rejected_or_unpublished": null,
    "duplication_status": null,
    "paper_for_thesis": 0,
    "paper_available": 1,
    "behind_paywall": 0,
    "paywall_circumventable": 0,
    "author_names": [
      "Zongzhang Zhang",
      "Wulong Liu",
      "Tianpei Yang",
      "Jianye Hao",
      "Weixun Wang",
      "Zhaopeng Meng",
      "Changjie Fan",
      "Yujing Hu",
      "Yingfeng Cheng",
      "Zhaodong Wang",
      "Jiajie Peng"
    ]
  },
  {
    "id": 2910862680,
    "title": "Transfer Value or Policy? A Value-centric Framework Towards Transferrable Continuous Reinforcement Learning",
    "abst": null,
    "url": "https://openreview.net/pdf?id=H1gZV30qKQ",
    "lang": null,
    "authors": [
      2123144340,
      2909085531,
      2910345997
    ],
    "fos": [
      97541855,
      107457646,
      41008148
    ],
    "journals": [],
    "conferences": [],
    "conference_series": [],
    "references": [],
    "filter_matches": [
      "tr",
      "tl",
      "trl",
      "rl"
    ],
    "rank": 21943,
    "citation_count": 0,
    "estimated_citation_count": 0,
    "publication_date": "2018-09-27",
    "found_in": 1,
    "transfer_experiment_type": [
      "t"
    ],
    "transfer_experiment_subtype": [
      "same_all"
    ],
    "transfer_data_type": [
      "pi"
    ],
    "transfer_performance_metrics": [
      "j",
      "tt",
      "ap"
    ],
    "implementation": [
      "Custom",
      "CS",
      "Gym",
      "Mujoco",
      "baselines",
      "PyTorch",
      "Figures",
      "Formulas",
      "Pseudo",
      "Theorem"
    ],
    "policy_type": [
      "MVC",
      "TRPO",
      "DDPG"
    ],
    "task_mappings": [
      "N/A"
    ],
    "autonomous_transfer": 0,
    "is_deep_rl": 1,
    "tags": [
      "2D",
      "3D",
      "Robotics",
      "Model",
      "ValueCentric",
      "Mujoco",
      "HalfCheetah",
      "InvertedPendulum",
      "Pendulum",
      "Reacher"
    ],
    "allowed_learner": [
      "TD",
      "MB"
    ],
    "country": [
      "USA"
    ],
    "uni": [
      "University of California"
    ],
    "department": [],
    "source_task_selection": [
      "h"
    ],
    "was_in_survey": [
      "mag"
    ],
    "in_title": [
      "t",
      "r",
      "l"
    ],
    "in_abs": [
      "t",
      "r",
      "l",
      "k"
    ],
    "in_contet": [
      "t",
      "r",
      "l",
      "k",
      "s"
    ],
    "rejected_or_unpublished": 1,
    "duplication_status": null,
    "paper_for_thesis": 0,
    "paper_available": 1,
    "behind_paywall": 0,
    "paywall_circumventable": 0,
    "author_names": [
      "Hao Su",
      "Xingchao Liu",
      "Tongzhou Mu"
    ]
  },
  {
    "id": 2945090640,
    "title": "Options in Multi-task Reinforcement Learning - Transfer via Reflection",
    "abst": "Temporally extended actions such as options are known to lead to improvements in reinforcement learning (RL). At the same time, transfer learning across different RL tasks is an increasingly active area of research. Following Baxter&#x2019;s formalism for transfer, the corresponding RL question considers the benefit that an RL agent can achieve on new tasks based on experience from previous tasks in a common &#x201C;learning environment&#x201D;. We address this in the specific context of goal-based multi-task RL, where the different tasks correspond to different goal states within a common state space, and we introduce Landmark Options Via Reflection (LOVR), a flexible framework that uses options to transfer domain knowledge. As an explicit analog of principles in transfer learning, we provide theoretical and empirical results demonstrating that when a set of landmark states covers the state space suitably, then a LOVR agent that learns optimal value functions for these in an initial phase and deploys the associated optimal policies as options in the main phase, can achieve a drastic reduction in cumulative regret compared to baseline approaches.",
    "url": "https://dblp.uni-trier.de/db/conf/ai/ai2019.html#DenisF19",
    "lang": "en",
    "authors": [
      2128818117,
      2491217612
    ],
    "fos": [
      41008148,
      2778365744,
      154945302,
      207685749,
      2780297707,
      38706069,
      97541855,
      119857082,
      50817715,
      72434380,
      150899416
    ],
    "journals": [],
    "conferences": [
      2890567818
    ],
    "conference_series": [
      1185151044
    ],
    "references": [
      99485931,
      567721252,
      1037351197,
      1492014007,
      1510402218,
      1541730457,
      1592847719,
      2080039641,
      2109910161,
      2120501001,
      2121863487,
      2124088405,
      2129670787,
      2160371091,
      2162888803,
      2169582662,
      2489939061,
      2606568940,
      2624731731,
      2730850385,
      2765602917,
      2804075420,
      2949267040,
      2951061345,
      2952448454
    ],
    "filter_matches": [
      "tr",
      "tl",
      "trl",
      "rl"
    ],
    "rank": 21973,
    "citation_count": 1,
    "estimated_citation_count": 1,
    "publication_date": "2019-05-28",
    "found_in": 1,
    "transfer_experiment_type": [
      "s_i",
      "s_f"
    ],
    "transfer_experiment_subtype": [
      "same_all"
    ],
    "transfer_data_type": [
      "options"
    ],
    "transfer_performance_metrics": [
      "j",
      "tt",
      "tr"
    ],
    "implementation": [
      "Custom",
      "CS",
      "Formulas",
      "Figures",
      "Tables"
    ],
    "policy_type": [
      "DQN",
      "LOVR"
    ],
    "task_mappings": [
      "N/A"
    ],
    "autonomous_transfer": 0,
    "is_deep_rl": 1,
    "tags": [
      "2D",
      "Navigation",
      "Grid",
      "Lifelong",
      "Landmark",
      "CNN",
      "Simulation"
    ],
    "allowed_learner": [
      "TD",
      "H"
    ],
    "country": [
      "Canada"
    ],
    "uni": [
      "University of Ottawa"
    ],
    "department": [
      "Mathematics and Statistics"
    ],
    "source_task_selection": [
      "h"
    ],
    "was_in_survey": [
      "mag",
      "multi"
    ],
    "in_title": [
      "t",
      "r",
      "l"
    ],
    "in_abs": [
      "t",
      "r",
      "l",
      "k"
    ],
    "in_contet": [
      "t",
      "r",
      "l",
      "k"
    ],
    "rejected_or_unpublished": null,
    "duplication_status": null,
    "paper_for_thesis": 0,
    "paper_available": 0,
    "behind_paywall": 1,
    "paywall_circumventable": 1,
    "author_names": [
      "Nicholas J. Denis",
      "Maia Fraser"
    ]
  },
  {
    "id": 2954881914,
    "title": "On mechanisms for transfer using landmark value functions in multi-task lifelong reinforcement learning.",
    "abst": "Transfer learning across different reinforcement learning (RL) tasks is becoming an increasingly valuable area of research. We consider a goal-based multi-task RL framework and mechanisms by which previously solved tasks can reduce sample complexity and regret when the agent is faced with a new task. Specifically, we introduce two metrics on the state space that encode notions of traversibility of the state space for an agent. Using these metrics a topological covering is constructed by way of a set of landmark states in a fully self-supervised manner. We show that these landmark coverings confer theoretical advantages for transfer learning within the goal-based multi-task RL setting. Specifically, we demonstrate three mechanisms by which landmark coverings can be used for successful transfer learning. First, we extend the Landmark Options Via Reflection (LOVR) framework to this new topological covering; second, we use the landmark-centric value functions themselves as features and define a greedy zombie policy that achieves near oracle performance on a sequence of zero-shot transfer tasks; finally, motivated by the second transfer mechanism, we introduce a learned reward function that provides a more dense reward signal for goal-based RL. Our novel topological landmark covering confers beneficial theoretical results, bounding the Q values at each state-action pair. In doing so, we introduce a mechanism that performs action-pruning at infeasible actions which cannot possibly be part of an optimal policy for the current goal.",
    "url": "https://export.arxiv.org/pdf/1907.00884",
    "lang": "en",
    "authors": [
      2954780076
    ],
    "fos": [
      150899416,
      72434380,
      154945302,
      66746571,
      50817715,
      97541855,
      63584917,
      55166926,
      119857082,
      33923547,
      2780297707
    ],
    "journals": [
      2597173376
    ],
    "conferences": [],
    "conference_series": [],
    "references": [
      1037351197,
      1517383877,
      1541730457,
      1592847719,
      2109910161,
      2120501001,
      2121863487,
      2136504847,
      2162888803,
      2163922914,
      2550039418,
      2566898803,
      2606568940,
      2730850385,
      2765602917
    ],
    "filter_matches": [
      "tr",
      "tl",
      "trl",
      "rl"
    ],
    "rank": 21981,
    "citation_count": 0,
    "estimated_citation_count": 0,
    "publication_date": "2019-07-01",
    "found_in": 1,
    "transfer_experiment_type": [
      "s_f"
    ],
    "transfer_experiment_subtype": [
      "same_all"
    ],
    "transfer_data_type": [
      "r",
      "options"
    ],
    "transfer_performance_metrics": [
      "j",
      "tt"
    ],
    "implementation": [
      "Custom",
      "CS",
      "Formulas",
      "Figures",
      "Tables"
    ],
    "policy_type": [
      "DQN",
      "LOVR",
      "Zombie"
    ],
    "task_mappings": [
      "N/A"
    ],
    "autonomous_transfer": 0,
    "is_deep_rl": 1,
    "tags": [
      "2D",
      "Navigation",
      "Grid",
      "Cliff-Walk",
      "Lifelong",
      "Landmark",
      "Simulation"
    ],
    "allowed_learner": [
      "TD",
      "H"
    ],
    "country": [
      "Canada"
    ],
    "uni": [
      "University of Ottawa"
    ],
    "department": [
      "Mathematics and Statistics"
    ],
    "source_task_selection": [
      "h"
    ],
    "was_in_survey": [
      "mag"
    ],
    "in_title": [
      "t",
      "r",
      "l"
    ],
    "in_abs": [
      "t",
      "r",
      "l"
    ],
    "in_contet": [
      "t",
      "r",
      "l",
      "k"
    ],
    "rejected_or_unpublished": 2,
    "duplication_status": null,
    "paper_for_thesis": 0,
    "paper_available": 1,
    "behind_paywall": 0,
    "paywall_circumventable": 0,
    "author_names": [
      "Nick Denis"
    ]
  },
  {
    "id": 2443629117,
    "title": "A Verification of Reinforcement Learning with Knowledge Transfer and Knowledge Selection in Multitask Learning",
    "abst": null,
    "url": "https://www.jstage.jst.go.jp/article/iscie/29/3/29_152/_article/-char/ja/",
    "lang": "en",
    "authors": [
      2727927361
    ],
    "fos": [
      154945302,
      8038995,
      77075516,
      183759332,
      119857082,
      41008148,
      28006648,
      199190896,
      188888258,
      150899416,
      12298181
    ],
    "journals": [],
    "conferences": [],
    "conference_series": [],
    "references": [
      2097381042
    ],
    "filter_matches": [
      "tr",
      "tl",
      "trl",
      "rl"
    ],
    "rank": 21984,
    "citation_count": 1,
    "estimated_citation_count": 1,
    "publication_date": "2016-01-01",
    "found_in": 1,
    "transfer_experiment_type": [],
    "transfer_experiment_subtype": [],
    "transfer_data_type": [],
    "transfer_performance_metrics": [],
    "implementation": [],
    "policy_type": [],
    "task_mappings": [],
    "autonomous_transfer": 2,
    "is_deep_rl": 2,
    "tags": [
      "DifferentLanguage"
    ],
    "allowed_learner": [],
    "country": [
      "Japan"
    ],
    "uni": [],
    "department": [],
    "source_task_selection": [],
    "was_in_survey": [
      "mag"
    ],
    "in_title": [],
    "in_abs": [],
    "in_contet": [],
    "rejected_or_unpublished": null,
    "duplication_status": null,
    "paper_for_thesis": 0,
    "paper_available": 1,
    "behind_paywall": 0,
    "paywall_circumventable": 0,
    "author_names": [
      "Naoki Kotani"
    ]
  },
  {
    "id": 2994229809,
    "title": "Dynamic pricing of demand response based on elasticity transfer and reinforcement learning",
    "abst": "In this paper, we study the dynamic pricing of electricity service provider in the day-ahead spot market. Since the wholesale electricity price that the electricity service provider obtains from the utility grid is constantly changing, and the user&#039;s response behavior is unknown, setting a suitable retail price is a big challenge for the electricity service provider. In response to this problem, we propose a method based on elasticity transfer and reinforcement learning, which transfers the elasticity of the implemented demand response region to the region where the user elasticity is unknown, as the initial reference for dynamic pricing, and then uses the SARAS learning algorithm for practical exploration and learning. The simulation results show that the elasticity transfer to the new region as the initial reference can significantly improve the learning rate compared to the system without the initial reference. Therefore, the proposed method can maximize the price of the electricity service provider and the user by setting the optimal price at a faster rate without prior knowledge of the user.",
    "url": "http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=8921683",
    "lang": null,
    "authors": [
      2936628630,
      2942525543,
      2980876872,
      2991856742,
      2993137036,
      2993652417
    ],
    "fos": [
      116537,
      133731056,
      2776789725,
      206658404,
      2779438525,
      113027937,
      2779370713,
      127413603,
      97541855,
      2779391423,
      126255220
    ],
    "journals": [],
    "conferences": [],
    "conference_series": [
      2622005410
    ],
    "references": [
      2050495423,
      2095244591,
      2141692617,
      2198104867,
      2329114217,
      2343935207,
      2593952096,
      2682322383,
      2724452159,
      2784193589,
      2795276745
    ],
    "filter_matches": [
      "tr",
      "tl",
      "trl",
      "rl"
    ],
    "rank": 22000,
    "citation_count": 0,
    "estimated_citation_count": 0,
    "publication_date": "2019-08-01",
    "found_in": 1,
    "transfer_experiment_type": [
      "t"
    ],
    "transfer_experiment_subtype": [
      "same_all"
    ],
    "transfer_data_type": [
      "Q"
    ],
    "transfer_performance_metrics": [
      "j"
    ],
    "implementation": [
      "Custom",
      "CS",
      "Pseudo",
      "Formulas",
      "Figures"
    ],
    "policy_type": [
      "SARSA"
    ],
    "task_mappings": [
      "N/A"
    ],
    "autonomous_transfer": 0,
    "is_deep_rl": 0,
    "tags": [
      "Power",
      "ElectricityMarket",
      "Electricity",
      "Simulation",
      "DynamicPricing",
      "Dynamic",
      "Demand"
    ],
    "allowed_learner": [
      "TD"
    ],
    "country": [
      "China"
    ],
    "uni": [
      "Tianjin University",
      "State Grid Tianjin Electric Power",
      "Beijing Fibrlink Corperation Company"
    ],
    "department": [
      "Key Laboratory of Smart Grid",
      "Industry",
      "Uni",
      "Colab"
    ],
    "source_task_selection": [
      "h"
    ],
    "was_in_survey": [
      "mag"
    ],
    "in_title": [
      "t",
      "r",
      "l"
    ],
    "in_abs": [
      "t",
      "r",
      "l",
      "k"
    ],
    "in_contet": [
      "t",
      "r",
      "l",
      "k"
    ],
    "rejected_or_unpublished": null,
    "duplication_status": null,
    "paper_for_thesis": 0,
    "paper_available": 1,
    "behind_paywall": 1,
    "paywall_circumventable": 1,
    "author_names": [
      "Jie Xiao",
      "Deqian Kong",
      "Xiangyu Kong",
      "Liang Yue",
      "Jian Zhang",
      "Siwei Li"
    ]
  },
  {
    "id": 2963248502,
    "title": "Unsupervised Discovery of Decision States for Transfer in Reinforcement Learning.",
    "abst": "We present a hierarchical reinforcement learning (HRL) or options framework for identifying decision states. Informally speaking, these are states considered important by the agent&#039;s policy e.g. , for navigation, decision states would be crossroads or doors where an agent needs to make strategic decisions. While previous work (most notably Goyal et. al., 2019) discovers decision states in a task/goal specific (or &#039;supervised&#039;) manner, we do so in a goal-independent (or &#039;unsupervised&#039;) manner, i.e. entirely without any goal or extrinsic rewards. Our approach combines two hitherto disparate ideas - 1) emph{intrinsic control} (Gregor et. al., 2016, Eysenbach et. al., 2018): learning a set of options that allow an agent to reliably reach a diverse set of states, and 2) emph{information bottleneck} (Tishby et. al., 2000): penalizing mutual information between the option $Omega$ and the states $s_t$ visited in the trajectory. The former encourages an agent to reliably explore the environment; the latter allows identification of decision states as the ones with high mutual information $I(Omega; a_t | s_t)$ despite the bottleneck. Our results demonstrate that 1) our model learns interpretable decision states in an unsupervised manner, and 2) these learned decision states transfer to goal-driven tasks in new environments, effectively guide exploration, and improve performance.",
    "url": "https://arxiv.org/abs/1907.10580",
    "lang": "en",
    "authors": [
      2032368582,
      2098683697,
      2223275083,
      2554710403,
      2759498471,
      2786888888,
      2962961424
    ],
    "fos": [
      60008888,
      13662910,
      185994504,
      154945302,
      152139883,
      119857082,
      2780513914,
      97541855,
      33923547
    ],
    "journals": [
      2597173376
    ],
    "conferences": [],
    "conference_series": [],
    "references": [
      1968768508,
      2064675550,
      2099111195,
      2115807451,
      2141124923,
      2143435603,
      2148989240,
      2170503197,
      2398870399,
      2417786368,
      2555129178,
      2556477470,
      2557579533,
      2735995851,
      2753738274,
      2788741142,
      2883433335,
      2899205164,
      2913854057,
      2950040888,
      2950152428,
      2952581030,
      2963523627,
      2979454998
    ],
    "filter_matches": [
      "tr",
      "tl",
      "trl",
      "rl"
    ],
    "rank": 22008,
    "citation_count": 2,
    "estimated_citation_count": 2,
    "publication_date": "2019-07-24",
    "found_in": 1,
    "transfer_experiment_type": [
      "s_i",
      "s_f",
      "s"
    ],
    "transfer_experiment_subtype": [
      "same_all",
      "multi"
    ],
    "transfer_data_type": [
      "options",
      "sub"
    ],
    "transfer_performance_metrics": [
      "j",
      "tr"
    ],
    "implementation": [
      "fakeOSS",
      "Custom",
      "PyTorch",
      "A2C-Kostrikov",
      "Gym",
      "Tables",
      "Figures",
      "Formulas",
      "Pseudo"
    ],
    "policy_type": [
      "A2C",
      "IR-VIC"
    ],
    "task_mappings": [
      "N/A"
    ],
    "autonomous_transfer": 0,
    "is_deep_rl": 1,
    "tags": [
      "2D",
      "Navigation",
      "Grid",
      "Maze",
      "Room",
      "MultiRoom",
      "Simulation",
      "CNN"
    ],
    "allowed_learner": [
      "TD"
    ],
    "country": [
      "USA"
    ],
    "uni": [
      "Georgia Institute of Technology",
      "Facebook AI Research"
    ],
    "department": [
      "Industry",
      "Uni",
      "Colab"
    ],
    "source_task_selection": [],
    "was_in_survey": [
      "mag"
    ],
    "in_title": [
      "t",
      "r",
      "l"
    ],
    "in_abs": [],
    "in_contet": [
      "t",
      "r",
      "l",
      "k"
    ],
    "rejected_or_unpublished": null,
    "duplication_status": null,
    "paper_for_thesis": 0,
    "paper_available": 1,
    "behind_paywall": 0,
    "paywall_circumventable": 0,
    "author_names": [
      "Ramakrishna Vedantam",
      "Dhruv Batra",
      "Devi Parikh",
      "Prithvijit Chattopadhyay",
      "Mohit Sharma",
      "Abhishek Das",
      "Nirbhay Modhe"
    ]
  },
  {
    "id": 2979305570,
    "title": "A Weight Transfer Mechanism for Kernel Reinforcement Learning Decoding in Brain-Machine Interfaces",
    "abst": "Brain-Machine Interfaces (BMIs) aim to help disabled people brain control the external devices to finish a variety of movement tasks. The neural signals are decoded into the execution commands of the apparatus. However, most of the existing decoding algorithms in BMI are only trained for a single task. When facing a new task, even if it is similar to the previous one, the decoder needs to be re-trained from scratch, which is not efficient. Among the different types of decoders, reinforcement learning (RL) based algorithm has the advantage of adaptive training through trial-and-error over the recalibration used in supervised learning. But most of the RL algorithms in BMI do not actively leverage the acquired knowledge in the old task. In this paper, we propose a kernel RL algorithm with a weight transfer mechanism for new task learning. The existing neural patterns are clustered according to their similarities. A new pattern will be assigned with the weights that are transferred from the closest cluster. In this way, the most similar experiences from the previous task could be re-utilized in the new task to fasten the learning speed. The proposed algorithm is tested on synthetic neural data. Compared with the policy of re-training from scratch, the proposed weight transfer mechanism could maintain a significantly higher performance and achieve a faster learning speed on the new task.",
    "url": "https://www.ncbi.nlm.nih.gov/pubmed/31946644",
    "lang": null,
    "authors": [
      2159126239,
      2898998461
    ],
    "fos": [
      136389625,
      154945302,
      97541855,
      175154964,
      31972630,
      41008148,
      73555534,
      57273362,
      99018454,
      2781235140,
      114173007
    ],
    "journals": [],
    "conferences": [
      2942753307
    ],
    "conference_series": [
      2232857946
    ],
    "references": [
      1540155273,
      2063702730,
      2106505783,
      2116532829,
      2128434591,
      2131191620,
      2165698076,
      2167932108,
      2462780241,
      2756387736,
      2899432656
    ],
    "filter_matches": [
      "tr",
      "tl",
      "trl",
      "rl"
    ],
    "rank": 22009,
    "citation_count": 0,
    "estimated_citation_count": 0,
    "publication_date": "2019-07-01",
    "found_in": 1,
    "transfer_experiment_type": [
      "a"
    ],
    "transfer_experiment_subtype": [
      "same_all"
    ],
    "transfer_data_type": [
      "WTF"
    ],
    "transfer_performance_metrics": [
      "j"
    ],
    "implementation": [
      "Custom",
      "CS",
      "Formulas",
      "Figures"
    ],
    "policy_type": [
      "Softmax"
    ],
    "task_mappings": [
      "N/A"
    ],
    "autonomous_transfer": 0,
    "is_deep_rl": 0,
    "tags": [
      "BMI",
      "KernelRL",
      "Mice",
      "Simulation"
    ],
    "allowed_learner": [
      "TD"
    ],
    "country": [
      "China"
    ],
    "uni": [
      "Hong Kong University of Science and Technology"
    ],
    "department": [
      "Electronic and Computer Engineering"
    ],
    "source_task_selection": [
      "h"
    ],
    "was_in_survey": [
      "mag"
    ],
    "in_title": [
      "t",
      "r",
      "l"
    ],
    "in_abs": [
      "t",
      "r",
      "l",
      "k"
    ],
    "in_contet": [
      "t",
      "r",
      "l",
      "k"
    ],
    "rejected_or_unpublished": null,
    "duplication_status": null,
    "paper_for_thesis": 0,
    "paper_available": 1,
    "behind_paywall": 1,
    "paywall_circumventable": 1,
    "author_names": [
      "Yiwen Wang",
      "Xiang Zhang"
    ]
  },
  {
    "id": 2786530308,
    "title": "Shaping in reinforcement learning by knowledge transferred from human-demonstrations of a simple similar task",
    "abst": null,
    "url": "https://dblp.uni-trier.de/db/journals/jifs/jifs34.html#WangFL18",
    "lang": "en",
    "authors": [
      2126098870,
      2480995612,
      2628511581
    ],
    "fos": [
      119857082,
      97541855,
      154945302,
      33923547,
      145420912
    ],
    "journals": [
      179157397
    ],
    "conferences": [],
    "conference_series": [],
    "references": [
      1453801241,
      1506146479,
      1563109146,
      1777239053,
      1845972764,
      1977655452,
      1980620643,
      1986014385,
      2004030284,
      2031727428,
      2046376809,
      2069300867,
      2071302132,
      2097381042,
      2097498341,
      2109910161,
      2113921460,
      2129427976,
      2130750514,
      2137375617,
      2165698076,
      2172968643,
      2257979135,
      2262174858,
      2397581010,
      2515676083
    ],
    "filter_matches": [
      "tr",
      "tl",
      "trl",
      "rl"
    ],
    "rank": 22012,
    "citation_count": 0,
    "estimated_citation_count": 0,
    "publication_date": "2018-01-01",
    "found_in": 1,
    "transfer_experiment_type": [
      "t",
      "v",
      "a"
    ],
    "transfer_experiment_subtype": [
      "diff-it"
    ],
    "transfer_data_type": [
      "r"
    ],
    "transfer_performance_metrics": [
      "tt",
      "j",
      "ap",
      "tr"
    ],
    "implementation": [
      "Custom",
      "CS",
      "Formulas",
      "Figures",
      "Tables"
    ],
    "policy_type": [
      "Q"
    ],
    "task_mappings": [
      "sup"
    ],
    "autonomous_transfer": 0,
    "is_deep_rl": 0,
    "tags": [
      "2D",
      "3D",
      "2Dto3D",
      "MountainCar3D",
      "Simulation",
      "MountainCar",
      "Human",
      "Demonstration",
      "ClassicControl"
    ],
    "allowed_learner": [
      "TD"
    ],
    "country": [
      "China"
    ],
    "uni": [
      "Zhejiang University"
    ],
    "department": [
      "Aeronatuics and Astronautics",
      "Schoolf of Control Science and Engineering"
    ],
    "source_task_selection": [
      "h"
    ],
    "was_in_survey": [
      "mag"
    ],
    "in_title": [
      "t",
      "r",
      "l",
      "k"
    ],
    "in_abs": [
      "t",
      "r",
      "l",
      "k"
    ],
    "in_contet": [
      "t",
      "r",
      "l",
      "k",
      "s"
    ],
    "rejected_or_unpublished": null,
    "duplication_status": null,
    "paper_for_thesis": 0,
    "paper_available": 1,
    "behind_paywall": 1,
    "paywall_circumventable": 1,
    "author_names": [
      "Zhou Fang",
      "Guo-fang Wang",
      "Ping Li"
    ]
  },
  {
    "id": 2399136500,
    "title": "Reducing Sample Complexity in Reinforcement Learning by Transferring Transition and Reward Probabilities",
    "abst": "Most existing reinforcement learning algorithms require many trials until they obtain optimal policies. In this study, we apply transfer learning to reinforcement learning to realize greater efficiency. We propose a new algorithm called TR-MAX, based on the R-MAX algorithm. TR-MAX transfers the transition and reward probabilities from a source task to a target task as prior knowledge. We theoretically analyze the sample complexity of TR-MAX. Moreover, we show that TR-MAX performs much better in practice than R-MAX in maze tasks.",
    "url": "http://dx.doi.org/10.5220/0004915606320638",
    "lang": "en",
    "authors": [
      2149841904,
      2526453992,
      2952857424
    ],
    "fos": [
      77967617,
      199190896,
      97541855,
      150899416,
      28006648,
      41008148,
      8038995,
      196340769,
      119857082,
      154945302,
      2778445095
    ],
    "journals": [],
    "conferences": [
      593016019
    ],
    "conference_series": [
      1200979010
    ],
    "references": [],
    "filter_matches": [
      "tr",
      "tl",
      "trl",
      "rl",
      "rlreward",
      "trreward",
      "tlrreward"
    ],
    "rank": 22032,
    "citation_count": 0,
    "estimated_citation_count": 0,
    "publication_date": "2014-03-06",
    "found_in": 1,
    "transfer_experiment_type": [
      "s",
      "levels"
    ],
    "transfer_experiment_subtype": [
      "same_all",
      "multi"
    ],
    "transfer_data_type": [
      "r",
      "model",
      "transitiondynamics"
    ],
    "transfer_performance_metrics": [
      "j",
      "tr",
      "tt",
      "ap"
    ],
    "implementation": [
      "Custom",
      "CS",
      "Formulas",
      "Pseudo",
      "Theorem",
      "Tables",
      "Figures"
    ],
    "policy_type": [
      "TR-MAX",
      "R-MAX",
      "Q"
    ],
    "task_mappings": [
      "N/A"
    ],
    "autonomous_transfer": 0,
    "is_deep_rl": 0,
    "tags": [
      "2D",
      "Navigation",
      "Grid",
      "Maze",
      "Room",
      "MultiRoom",
      "Simulation"
    ],
    "allowed_learner": [
      "TD",
      "MB"
    ],
    "country": [
      "Japan"
    ],
    "uni": [
      "Tohoku University"
    ],
    "department": [
      "Information Sciences"
    ],
    "source_task_selection": [
      "all"
    ],
    "was_in_survey": [
      "mag"
    ],
    "in_title": [
      "t",
      "r",
      "l"
    ],
    "in_abs": [
      "t",
      "r",
      "l",
      "k"
    ],
    "in_contet": [
      "t",
      "r",
      "l",
      "k"
    ],
    "rejected_or_unpublished": null,
    "duplication_status": null,
    "paper_for_thesis": 0,
    "paper_available": 1,
    "behind_paywall": 0,
    "paywall_circumventable": 0,
    "author_names": [
      "Ayumi Shinohara",
      "Kouta Oguni",
      "Kazuyuki Narisawa"
    ]
  },
  {
    "id": 2889943195,
    "title": "Zero-Shot Transfer with Deictic Object-Oriented Representation in Reinforcement Learning",
    "abst": "Object-oriented representations in reinforcement learning have shown promise in transfer learning, with previous research introducing a propositional object-oriented framework that has provably efficient learning bounds with respect to sample complexity. However, this framework has limitations in terms of the classes of tasks it can efficiently learn. In this paper we introduce a novel deictic object-oriented framework that has provably efficient learning bounds and can solve a broader range of tasks. Additionally, we show that this framework is capable of zero-shot transfer of transition dynamics across tasks and demonstrate this empirically for the Taxi and Sokoban domains.",
    "url": "https://nips.cc/Conferences/2018/Schedule?showEvent=11239",
    "lang": "en",
    "authors": [
      2037717932,
      2788008008
    ],
    "fos": [
      41008148,
      2778445095,
      13077596,
      119857082,
      150899416,
      2989424784,
      154945302,
      97541855
    ],
    "journals": [],
    "conferences": [
      2783316545
    ],
    "conference_series": [
      1127325140
    ],
    "references": [],
    "filter_matches": [
      "tr",
      "tl",
      "trl",
      "rl"
    ],
    "rank": 22059,
    "citation_count": 0,
    "estimated_citation_count": 0,
    "publication_date": "2018-12-03",
    "found_in": 1,
    "transfer_experiment_type": [
      "#",
      "s",
      "levels",
      "s_i",
      "s_f"
    ],
    "transfer_experiment_subtype": [
      "multi"
    ],
    "transfer_data_type": [
      "transitiondynamics"
    ],
    "transfer_performance_metrics": [
      "j",
      "tt"
    ],
    "implementation": [
      "Custom",
      "CS",
      "Pseudo",
      "Formulas",
      "Theorem",
      "Figures"
    ],
    "policy_type": [
      "DOORMAX",
      "OO-MDP"
    ],
    "task_mappings": [
      "N/A"
    ],
    "autonomous_transfer": 0,
    "is_deep_rl": 0,
    "tags": [
      "2D",
      "Navigation",
      "Simulation",
      "Taxi",
      "Sokoban"
    ],
    "allowed_learner": [
      "TD",
      "RRL",
      "MB"
    ],
    "country": [
      "South Africa"
    ],
    "uni": [
      "University of the Witwaterstrand",
      "Council for Scientific and Industrial Research"
    ],
    "department": [
      "Industry",
      "Uni",
      "Colab"
    ],
    "source_task_selection": [
      "h"
    ],
    "was_in_survey": [
      "mag"
    ],
    "in_title": [
      "t",
      "r",
      "l"
    ],
    "in_abs": [
      "t",
      "r",
      "l"
    ],
    "in_contet": [
      "t",
      "r",
      "l",
      "k"
    ],
    "rejected_or_unpublished": null,
    "duplication_status": null,
    "paper_for_thesis": 0,
    "paper_available": 1,
    "behind_paywall": 0,
    "paywall_circumventable": 0,
    "author_names": [
      "Benjamin Rosman",
      "Ofir Marom"
    ]
  },
  {
    "id": 1997664188,
    "title": "Transfer Learning Method Using Ontology for Heterogeneous Multi-agent Reinforcement Learning",
    "abst": "This paper presents a framework, called the knowledge co-creation framework (KCF), for heterogeneous multiagent robot systems that use a transfer learning method. A multiagent robot system (MARS) that utilizes reinforcement learning and a transfer learning method has recently been studied in realworld situations. In MARS, autonomous agents obtain behavior autonomously through multi-agent reinforcement learning and the transfer learning method enables the reuse of the knowledge of other robots&#x2019; behavior, such as for cooperative behavior. Those methods, however, have not been fully and systematically discussed. To address this, KCF leverages the transfer learning method and cloud-computing resources. In prior research, we developed ontology-based inter-task mapping as a core technology for hierarchical transfer learning (HTL) method and investigated its effectiveness in a dynamic multi-agent environment. The HTL method hierarchically abstracts obtained knowledge by ontological methods. Here, we evaluate the effectiveness of HTL with a basic experimental setup that considers two types of ontology: action and state.",
    "url": "https://thesai.org/Publications/ViewPaper?Volume=5&amp;Issue=10&amp;Code=IJACSA&amp;SerialNo=22",
    "lang": "en",
    "authors": [
      2039144714,
      2120703638,
      2125718030,
      2266033036,
      2296688416
    ],
    "fos": [
      150899416,
      25810664,
      90509273,
      154945302,
      97541855,
      28006648,
      41008148,
      77967617,
      188888258,
      13687954,
      119857082
    ],
    "journals": [
      23629721
    ],
    "conferences": [],
    "conference_series": [],
    "references": [
      112971866,
      1507591516,
      1601063228,
      1641379095,
      1744965757,
      1745487673,
      1973367484,
      2045447221,
      2105774558,
      2121696237,
      2137079713,
      2167731360
    ],
    "filter_matches": [
      "tr",
      "tl",
      "trl",
      "rl"
    ],
    "rank": 22068,
    "citation_count": 2,
    "estimated_citation_count": 2,
    "publication_date": "2014-01-01",
    "found_in": 1,
    "transfer_experiment_type": [
      "a",
      "v",
      "s_i",
      "s_f",
      "s"
    ],
    "transfer_experiment_subtype": [
      "diff-no",
      "lit"
    ],
    "transfer_data_type": [
      "Q",
      "federated"
    ],
    "transfer_performance_metrics": [
      "j",
      "tr",
      "tt",
      "ap"
    ],
    "implementation": [
      "Custom",
      "CS",
      "Formulas",
      "Figures",
      "Tables"
    ],
    "policy_type": [
      "Q"
    ],
    "task_mappings": [
      "exp"
    ],
    "autonomous_transfer": 0,
    "is_deep_rl": 0,
    "tags": [
      "2D",
      "Navigation",
      "Grid",
      "PredatorPrey",
      "MultiAgent",
      "Simulation"
    ],
    "allowed_learner": [
      "TD",
      "H"
    ],
    "country": [
      "Japan"
    ],
    "uni": [
      "Tokyo Denki University",
      "National Institute of Advanced Industrial Science and Technology (AIST)"
    ],
    "department": [
      "Advanced Science and Technology",
      "Information and Communication Engineering",
      "Intelligent Systems Research Institute"
    ],
    "source_task_selection": [
      "h"
    ],
    "was_in_survey": [
      "mag",
      "silva"
    ],
    "in_title": [
      "t",
      "r",
      "l"
    ],
    "in_abs": [
      "t",
      "r",
      "l",
      "k"
    ],
    "in_contet": [
      "t",
      "r",
      "l",
      "k"
    ],
    "rejected_or_unpublished": null,
    "duplication_status": null,
    "paper_for_thesis": 0,
    "paper_available": 1,
    "behind_paywall": 0,
    "paywall_circumventable": 0,
    "author_names": [
      "Akiya Kamimura",
      "Hitoshi Kono",
      "Kohji Tomita",
      "Yuta Murata",
      "Tsuyoshi Suzuki"
    ]
  },
  {
    "id": 1974126445,
    "title": "Implementation of Reinforcement Learning by transfering sub-goal policies in robot navigation",
    "abst": "Although Reinforcement Learning (RL) is one of the most popular learning methods, it suffers from the curse of dimensionality. If the state and action domains of the problem are immense, the learning rate of the agent decreases dramatically and eventually the agent loses the ability to learn. In order to eliminate the effects of the curse of the dimensionality, researchers typically concentrate on the methods that reduce the complexity of the problems. While some of them model the problem in a hierarchical manner, the others try to transfer the knowledge obtained during the learning process of simpler tasks. While learning from scratch ignores the previous experiences, transferring full knowledge may mislead the agent because of the conflicting requirements. The main goal of this study is to improve the learning rate of the agent by transferring the relevant parts of the knowledge acquired as a result of previous experiences. The main contribution of this study is to merge these two approaches to transfer only the relevant knowledge in a setting. The proposed method is tested on a robot navigation task in a simulated roombased environment.",
    "url": "https://dblp.uni-trier.de/db/conf/siu/siu2013.html#GokceA13",
    "lang": "en",
    "authors": [
      2583765040,
      2721348337
    ],
    "fos": [
      77967617,
      154945302,
      97541855,
      77075516,
      188888258,
      41008148,
      28006648,
      24138899,
      199190896,
      12298181
    ],
    "journals": [],
    "conferences": [
      2734539129
    ],
    "conference_series": [
      2735422400
    ],
    "references": [
      60146956,
      1536990779,
      1544120923,
      1586944634,
      1598052524,
      1608267696,
      1647196294,
      1968768508,
      1980452903,
      2041141995,
      2090170171,
      2124695578,
      2133040789,
      2143435603,
      2156493855,
      2164114810
    ],
    "filter_matches": [
      "tr",
      "tl",
      "trl",
      "rl"
    ],
    "rank": 22068,
    "citation_count": 1,
    "estimated_citation_count": 1,
    "publication_date": "2013-04-24",
    "found_in": 1,
    "transfer_experiment_type": [
      "s_i",
      "s_f",
      "s"
    ],
    "transfer_experiment_subtype": [
      "same_all"
    ],
    "transfer_data_type": [
      "pi"
    ],
    "transfer_performance_metrics": [
      "tr"
    ],
    "implementation": [
      "Custom",
      "CS",
      "Figures"
    ],
    "policy_type": [
      "Q"
    ],
    "task_mappings": [
      "N/A"
    ],
    "autonomous_transfer": 0,
    "is_deep_rl": 0,
    "tags": [
      "2D",
      "Navigation",
      "Grid",
      "Robotics",
      "Rooms",
      "Simulation"
    ],
    "allowed_learner": [
      "TD"
    ],
    "country": [
      "Turkey"
    ],
    "uni": [
      "Bogazici University"
    ],
    "department": [],
    "source_task_selection": [
      "h"
    ],
    "was_in_survey": [
      "mag"
    ],
    "in_title": [
      "t",
      "r",
      "l"
    ],
    "in_abs": [
      "t",
      "r",
      "l",
      "k"
    ],
    "in_contet": [],
    "rejected_or_unpublished": null,
    "duplication_status": null,
    "paper_for_thesis": 0,
    "paper_available": 1,
    "behind_paywall": 0,
    "paywall_circumventable": 0,
    "author_names": [
      "H. L. Akin",
      "B. Gokce"
    ]
  },
  {
    "id": 2972522277,
    "title": "Learning Transferable Domain Priors for Safe Exploration in Reinforcement Learning",
    "abst": "Prior access to domain knowledge could significantly improve the performance of a reinforcement learning agent. In particular, it could help agents avoid potentially catastrophic exploratory actions, which would otherwise have to be experienced during learning. In this work, we identify consistently undesirable actions in a set of previously learned tasks, and use pseudo-rewards associated with them to learn a prior policy. In addition to enabling safer exploratory behaviors in subsequent tasks in the domain, we show that these priors are transferable to similar environments, and can be learned off-policy and in parallel with the learning of other tasks in the domain. We compare our approach to established, state-of-the-art algorithms in both discrete as well as continuous environments, and demonstrate that it exhibits a safer exploratory behavior while learning to perform arbitrary tasks in the domain. We also present a theoretical analysis to support these results, and briefly discuss the implications and some alternative formulations of this approach, which could also be useful in certain scenarios.",
    "url": "https://export.arxiv.org/abs/1909.04307",
    "lang": "en",
    "authors": [
      2119406083,
      2142238370,
      2146461601,
      2157392948,
      2614491958
    ],
    "fos": [
      41008148,
      154945302,
      97541855,
      119857082,
      207685749,
      177769412,
      2776654903
    ],
    "journals": [
      2596500785
    ],
    "conferences": [],
    "conference_series": [],
    "references": [
      1522301498,
      1757796397,
      1840625103,
      1845972764,
      1949804828,
      2031727428,
      2097381042,
      2114901408,
      2121863487,
      2131600418,
      2145339207,
      2159459871,
      2173248099,
      2257979135,
      2736601468,
      2782656435,
      2789517807,
      2797734773,
      2804673281,
      2926028278,
      2962717849,
      2962803570,
      2963065769,
      2963400359,
      2963575966,
      2963771109,
      2964043796
    ],
    "filter_matches": [
      "tr",
      "tl",
      "trl",
      "rl"
    ],
    "rank": 22075,
    "citation_count": 0,
    "estimated_citation_count": 0,
    "publication_date": "2019-09-10",
    "found_in": 1,
    "transfer_experiment_type": [
      "s_i",
      "s_f",
      "s",
      "levels"
    ],
    "transfer_experiment_subtype": [
      "diff-no"
    ],
    "transfer_data_type": [
      "pri"
    ],
    "transfer_performance_metrics": [
      "tr"
    ],
    "implementation": [
      "Custom",
      "CS",
      "Formulas",
      "Pseudo",
      "Gym",
      "Figures",
      "Theorem"
    ],
    "policy_type": [
      "Q",
      "A2C",
      "SARSA",
      "DQN"
    ],
    "task_mappings": [
      "N/A"
    ],
    "autonomous_transfer": 0,
    "is_deep_rl": 1,
    "tags": [
      "2D",
      "Navigation",
      "Grid",
      "Continuous",
      "Maze",
      "Simulation",
      "SafetyGym"
    ],
    "allowed_learner": [
      "TD"
    ],
    "country": [
      "Australia"
    ],
    "uni": [
      "Deakin University"
    ],
    "department": [
      "Applied Artificial Intelligence"
    ],
    "source_task_selection": [],
    "was_in_survey": [
      "mag"
    ],
    "in_title": [
      "t",
      "r",
      "l"
    ],
    "in_abs": [
      "t",
      "r",
      "l",
      "k"
    ],
    "in_contet": [
      "t",
      "r",
      "l",
      "k"
    ],
    "rejected_or_unpublished": 2,
    "duplication_status": null,
    "paper_for_thesis": 0,
    "paper_available": 1,
    "behind_paywall": 0,
    "paywall_circumventable": 0,
    "author_names": [
      "Sunil Gupta",
      "Santu Rana",
      "Svetha Venkatesh",
      "Truyen Tran",
      "Thommen George Karimpanal"
    ]
  },
  {
    "id": 2991344719,
    "title": "Attention Privileged Reinforcement Learning for Domain Transfer",
    "abst": "Applying reinforcement learning (RL) to physical systems presents notable challenges, given requirements regarding sample efficiency, safety, and physical constraints compared to simulated environments. To enable transfer of policies trained in simulation, randomising simulation parameters leads to more robust policies, but also significantly extends training time. In this paper, we exploit access to privileged information (such as environment states) often available in simulation, in order to improve and accelerate learning over randomised environments. We introduce Attention Privileged Reinforcement Learning (APRiL), which equips the agent with an attention mechanism and makes use of state information in simulation, learning to align attention between state- and image-based policies while additionally sharing generated data. During deployment we can apply the image-based policy to remove the requirement of access to additional information. We experimentally demonstrate accelerated and more robust learning on a number of diverse domains, leading to improved final performance for environments both within and outside the training distribution.",
    "url": "https://export.arxiv.org/pdf/1911.08363",
    "lang": "en",
    "authors": [
      2019646861,
      2154781858,
      2200442376,
      2631794095,
      2791209807
    ],
    "fos": [
      154945302,
      2985545028,
      105339364,
      116672817,
      119857082,
      3018071011,
      165696696,
      41008148,
      97541855,
      2985963534
    ],
    "journals": [
      2596500785
    ],
    "conferences": [],
    "conference_series": [],
    "references": [
      1522301498,
      1731081199,
      2145339207,
      2155541015,
      2156737235,
      2158782408,
      2161381512,
      2173248099,
      2195446438,
      2529477964,
      2577645110,
      2623491082,
      2626073992,
      2733961795,
      2737215781,
      2738129230,
      2766447205,
      2767050701,
      2767621168,
      2781585732,
      2783470107,
      2914688076,
      2948609886,
      2951670162,
      2952531487,
      2952606116,
      2952629144,
      2953127297,
      2962787403,
      2962957005,
      2963175324,
      2963390419,
      2963430173,
      2964198579,
      2981030070,
      2990747716,
      3011732969
    ],
    "filter_matches": [
      "tr",
      "tl",
      "trl",
      "rl"
    ],
    "rank": 22082,
    "citation_count": 2,
    "estimated_citation_count": 2,
    "publication_date": "2019-09-25",
    "found_in": 1,
    "transfer_experiment_type": [
      "#",
      "t",
      "s",
      "s_i",
      "s_f"
    ],
    "transfer_experiment_subtype": [
      "same_all"
    ],
    "transfer_data_type": [
      "pi"
    ],
    "transfer_performance_metrics": [
      "tt",
      "ap",
      "tr"
    ],
    "implementation": [
      "Custom",
      "CS",
      "TensorFlow",
      "Formulas",
      "Pseudo",
      "Figures",
      "Web",
      "Videos"
    ],
    "policy_type": [
      "aDDPG",
      "APRiL"
    ],
    "task_mappings": [
      "N/A"
    ],
    "autonomous_transfer": 0,
    "is_deep_rl": 1,
    "tags": [
      "Simulation",
      "Robotics",
      "2D",
      "Navigation",
      "Arm",
      "Legs",
      "Walker2d",
      "JacoReach",
      "Simulation"
    ],
    "allowed_learner": [
      "TD"
    ],
    "country": [
      "UK"
    ],
    "uni": [
      "University of Oxford",
      "DeepMind"
    ],
    "department": [
      "Applied AI Lab",
      "Industry",
      "Uni",
      "Colab"
    ],
    "source_task_selection": [
      "h"
    ],
    "was_in_survey": [
      "mag"
    ],
    "in_title": [
      "t",
      "r",
      "l"
    ],
    "in_abs": [
      "t",
      "r",
      "l",
      "k"
    ],
    "in_contet": [
      "t",
      "r",
      "l",
      "k"
    ],
    "rejected_or_unpublished": null,
    "duplication_status": null,
    "paper_for_thesis": 0,
    "paper_available": 1,
    "behind_paywall": 0,
    "paywall_circumventable": 0,
    "author_names": [
      "Ingmar Posner",
      "Raia Hadsell",
      "Markus Wulfmeier",
      "Dushyant Rao",
      "Sasha Salter"
    ]
  },
  {
    "id": 2323475154,
    "title": "A Hybrid Transfer Algorithm for Reinforcement Learning Based on Spectral Method",
    "abst": "For scaling up state space transfer underlying the proto-value function framework, only some basis functions corresponding to smaller eigenvalues are transferred effectively, which will result in wrong approximation of value function in the target task. In order to solve the problem, according to the fact that Laplacian eigenmap can preserve the local topology structure of state space, an improved hierarchical decomposition algorithm based on the spectral graph theory is proposed and a hybrid transfer method integrating basis function transfer with subtask optimal polices transfer is designed. At first, the basis functions of the source task are constructed using spectral method. The basis functions of target task are produced through linearly interpolating basis functions of the source task. Secondly, the produced second basis function of the target task (approximating Fiedler eigenvector) is used to decompose the target task. Then the optimal polices of subtasks are obtained using the improved hierarchical decomposition algorithm. At last, the obtained basis functions and optimal subtask polices are transferred to the target task. The proposed hybrid transfer method can directly get optimal policies of some states, reduce the number of iterations and the minimum number of basis functions needed to approximate the value function. The method is suitable for scaling up state space transfer task with hierarchical control structure. Simulation results of grid world have verified the validity of the proposed hybrid transfer method.",
    "url": "http://pub.chinasciencejournal.com/article/getArticleRedirect.action?doiCode=10.3724/SP.J.1004.2012.01765",
    "lang": null,
    "authors": [
      2157260211,
      2423230865,
      2661262389,
      2793436240,
      2947499044
    ],
    "fos": [
      5917680,
      150899416,
      72434380,
      2777965961,
      33923547,
      11413529,
      97541855,
      158693339,
      74003402,
      126255220,
      23463724
    ],
    "journals": [
      23425967
    ],
    "conferences": [],
    "conference_series": [],
    "references": [],
    "filter_matches": [
      "tr",
      "tl",
      "trl",
      "rl"
    ],
    "rank": 22082,
    "citation_count": 3,
    "estimated_citation_count": 3,
    "publication_date": "2012-01-01",
    "found_in": 1,
    "transfer_experiment_type": [],
    "transfer_experiment_subtype": [],
    "transfer_data_type": [],
    "transfer_performance_metrics": [],
    "implementation": [
      "Custom",
      "CS"
    ],
    "policy_type": [],
    "task_mappings": [],
    "autonomous_transfer": 2,
    "is_deep_rl": 2,
    "tags": [
      "2D",
      "Simulation",
      "Navigation",
      "Grid",
      "Spectral",
      "Simulation"
    ],
    "allowed_learner": [],
    "country": [
      "China"
    ],
    "uni": [
      "University of Mining and Technology"
    ],
    "department": [
      "Information and Electrical Engineering"
    ],
    "source_task_selection": [],
    "was_in_survey": [
      "mag"
    ],
    "in_title": [
      "t",
      "r",
      "l"
    ],
    "in_abs": [
      "t",
      "r",
      "l"
    ],
    "in_contet": [],
    "rejected_or_unpublished": null,
    "duplication_status": null,
    "paper_for_thesis": 0,
    "paper_available": 0,
    "behind_paywall": 1,
    "paywall_circumventable": 0,
    "author_names": [
      "Huan-Ting Feng",
      "Ming Li",
      "Mei-Qiang Zhu",
      "Xuesong Wang",
      "Yuhu Cheng"
    ]
  },
  {
    "id": 1178202497,
    "title": "A model-based markovian context-dependent reinforcement learning approach for neurobiologically plausible transfer of experience",
    "abst": "Reinforcement learning (RL) is an algorithmic theory for learning by experience optimal action control. Two widely discussed problems within this field are the temporal credit assignment problem and the transfer of experience. The temporal credit assignment problem postulates that deciding whether an action is good or bad may not be done upon right away because of delayed rewards. The problem of transferring experience investigates the question of how experience can be generalised and transferred from a familiar context, where it was acquired, to an unfamiliar context, where it may, nevertheless, prove helpful. We propose a controller for modelling flexible transfer of experience in a context-dependent reinforcement learning paradigm. The devised controller combines two alternatives of perfect learner algorithms. In the first alternative, rewards are predicted by individual objects presented in a temporal sequence. In the second alternative, rewards are predicted on the basis of successive pairs of objects. Simulations run on both deterministic and random temporal sequences show that only in case of deterministic sequences, a previously acquired context could be retrieved. This suggests a role of temporal sequence information in the generalisation and transfer of experience.",
    "url": "https://dblp.uni-trier.de/db/journals/ijhis/ijhis12.html#Hamid15",
    "lang": "en",
    "authors": [
      2809142581
    ],
    "fos": [
      97541855,
      159886148,
      119857082,
      41008148,
      2988340862,
      154945302,
      2993349903,
      79699506,
      177148314
    ],
    "journals": [
      50927259
    ],
    "conferences": [],
    "conference_series": [],
    "references": [
      158722652,
      169931978,
      1510391338,
      1512746852,
      1550112914,
      1567729634,
      1573190166,
      1759147280,
      1779940166,
      1797759599,
      1800397623,
      1955461630,
      1975401347,
      1987626674,
      1993129834,
      1995875735,
      2002288980,
      2007267229,
      2007651260,
      2011166600,
      2013140236,
      2020935680,
      2024365408,
      2027600189,
      2047125104,
      2066473871,
      2070732802,
      2074642121,
      2096240659,
      2105350006,
      2107726111,
      2121863487,
      2122340689,
      2124285588,
      2124674171,
      2127469248,
      2128084896,
      2129408021,
      2132994929,
      2133993682,
      2138353778,
      2143594200,
      2145461202,
      2145934551,
      2167362547,
      2339009915,
      2487350604,
      2499073429,
      2888909284,
      2914656440
    ],
    "filter_matches": [
      "tr",
      "tl",
      "trl",
      "rl"
    ],
    "rank": 22127,
    "citation_count": 6,
    "estimated_citation_count": 6,
    "publication_date": "2015-01-01",
    "found_in": 1,
    "transfer_experiment_type": [],
    "transfer_experiment_subtype": [],
    "transfer_data_type": [],
    "transfer_performance_metrics": [],
    "implementation": [
      "Custom",
      "CS"
    ],
    "policy_type": [],
    "task_mappings": [],
    "autonomous_transfer": 2,
    "is_deep_rl": 2,
    "tags": [
      "ANN",
      "HIS",
      "TemporalCredit",
      "Simulation"
    ],
    "allowed_learner": [],
    "country": [
      "Kuwait"
    ],
    "uni": [
      "Arab Open University"
    ],
    "department": [
      "Computer Studies"
    ],
    "source_task_selection": [],
    "was_in_survey": [
      "mag"
    ],
    "in_title": [
      "t",
      "r",
      "l"
    ],
    "in_abs": [
      "t",
      "r",
      "l"
    ],
    "in_contet": [
      "t",
      "r",
      "l",
      "k"
    ],
    "rejected_or_unpublished": null,
    "duplication_status": null,
    "paper_for_thesis": 0,
    "paper_available": 1,
    "behind_paywall": 1,
    "paywall_circumventable": 1,
    "author_names": [
      "Oussama H. Hamid"
    ]
  },
  {
    "id": 2611612482,
    "title": "Bacteria Foraging Reinforcement Learning for Risk-Based Economic Dispatch via Knowledge Transfer",
    "abst": "This paper proposes a novel bacteria foraging reinforcement learning with knowledge transfer method for risk-based economic dispatch, in which the economic dispatch is integrated with risk assessment theory to represent the uncertainties of active power demand and contingencies during power system operations. Moreover, a multi-agent collaboration is employed to accelerate the convergence of knowledge matrix, which is decomposed into several lower dimension sub-matrices via a knowledge extension, thus the curse of dimension can be effectively avoided. Besides, the convergence rate of bacteria foraging reinforcement learning is increased dramatically through a knowledge transfer after obtaining the optimal knowledge matrices of source tasks in pre-learning. The performance of bacteria foraging reinforcement learning has been thoroughly evaluated on IEEE RTS-79 system. Simulation results demonstrate that it can outperform conventional artificial intelligence algorithms in terms of global convergence and convergence rate.",
    "url": "https://ideas.repec.org/a/gam/jeners/v10y2017i5p638-d97735.html",
    "lang": "en",
    "authors": [
      2174227369,
      2285060655,
      2318951973,
      2610391700,
      2717243488
    ],
    "fos": [
      108755667,
      71923881,
      133731056,
      97541855,
      89227174,
      2776960227,
      2985028170,
      187633118,
      127413603,
      126255220,
      154945302,
      57869625,
      119857082
    ],
    "journals": [
      198098182
    ],
    "conferences": [],
    "conference_series": [],
    "references": [
      32403112,
      1541979755,
      1967250398,
      1971545597,
      1975774546,
      1975833767,
      1979244036,
      1990904614,
      1991662752,
      1996303439,
      1997176143,
      2008338640,
      2014867030,
      2040396929,
      2065004515,
      2092002374,
      2095553613,
      2097381042,
      2103145033,
      2111974359,
      2126125555,
      2136964221,
      2138514475,
      2155539900,
      2163232949,
      2165182598,
      2165698076,
      2202767830,
      2208683959,
      2312340764,
      2339356805,
      2413273146,
      2514031077,
      2547858766,
      2547937424,
      2564759013
    ],
    "filter_matches": [
      "tr",
      "tl",
      "trl",
      "rl"
    ],
    "rank": 22147,
    "citation_count": 3,
    "estimated_citation_count": 3,
    "publication_date": "2017-05-06",
    "found_in": 1,
    "transfer_experiment_type": [
      "s"
    ],
    "transfer_experiment_subtype": [
      "same_all"
    ],
    "transfer_data_type": [
      "Q"
    ],
    "transfer_performance_metrics": [
      "j"
    ],
    "implementation": [
      "Custom",
      "CS",
      "Formulas",
      "Figures",
      "Tables"
    ],
    "policy_type": [
      "BFRL"
    ],
    "task_mappings": [],
    "autonomous_transfer": 0,
    "is_deep_rl": 0,
    "tags": [
      "Power",
      "Efficiency",
      "SmartEnergy",
      "Effectiveness",
      "Simulation",
      "IEEE RTS-19",
      "Bacteria",
      "MultiAgent"
    ],
    "allowed_learner": [
      "TD"
    ],
    "country": [
      "China"
    ],
    "uni": [
      "South China University of Technology",
      "Kunming University of Science and Technology"
    ],
    "department": [
      "Electric Power Engineering"
    ],
    "source_task_selection": [],
    "was_in_survey": [
      "mag"
    ],
    "in_title": [
      "t",
      "r",
      "l",
      "k"
    ],
    "in_abs": [
      "t",
      "r",
      "l",
      "k"
    ],
    "in_contet": [
      "t",
      "r",
      "l",
      "k"
    ],
    "rejected_or_unpublished": null,
    "duplication_status": null,
    "paper_for_thesis": 0,
    "paper_available": 1,
    "behind_paywall": 0,
    "paywall_circumventable": 0,
    "author_names": [
      "Chuanjia Han",
      "Tao Yu",
      "Xiaoshun Zhang",
      "Tao Bao",
      "Bo Yang"
    ]
  },
  {
    "id": 3013348603,
    "title": "Parallel Knowledge Transfer in Multi-Agent Reinforcement Learning.",
    "abst": "Multi-agent reinforcement learning is a standard framework for modeling multi-agent interactions applied in real-world scenarios. Inspired by experience sharing in human groups, learning knowledge parallel reusing between agents can potentially promote team learning performance, especially in multi-task environments. When all agents interact with the environment and learn simultaneously, how each independent agent selectively learns from other agents&#039; behavior knowledge is a problem that we need to solve. This paper proposes a novel knowledge transfer framework in MARL, PAT (Parallel Attentional Transfer). We design two acting modes in PAT, student mode and self-learning mode. Each agent in our approach trains a decentralized student actor-critic to determine its acting mode at each time step. When agents are unfamiliar with the environment, the shared attention mechanism in student mode effectively selects learning knowledge from other agents to decide agents&#039; actions. PAT outperforms state-of-the-art empirical evaluation results against the prior advising approaches. Our approach not only significantly improves team learning rate and global performance, but also is flexible and transferable to be applied in various multi-agent systems.",
    "url": "http://arxiv.org/pdf/2003.13085.pdf",
    "lang": "en",
    "authors": [
      3013007978,
      3013318901
    ],
    "fos": [
      97541855,
      190839683,
      3017396079,
      206588197,
      119857082,
      107457646,
      2776960227,
      41008148,
      154945302,
      2982742679,
      920782
    ],
    "journals": [
      2596500785
    ],
    "conferences": [],
    "conference_series": [],
    "references": [
      206679605,
      1529399279,
      1589064538,
      1641379095,
      2097381042,
      2107726111,
      2145339207,
      2155027007,
      2165150801,
      2173248099,
      2341479450,
      2418628973,
      2563829177,
      2601465345,
      2620645529,
      2739573821,
      2921955147,
      2950527759,
      2962847771,
      2962966033,
      2963000099,
      2963289505,
      2963390684,
      2963403868,
      2963407617,
      2963588154,
      2963717208,
      2963881016,
      2964338167
    ],
    "filter_matches": [
      "tr",
      "tl",
      "trl",
      "rl"
    ],
    "rank": 22158,
    "citation_count": 0,
    "estimated_citation_count": 0,
    "publication_date": "2020-03-29",
    "found_in": 1,
    "transfer_experiment_type": [
      "s_i",
      "s_f"
    ],
    "transfer_experiment_subtype": [
      "multi"
    ],
    "transfer_data_type": [
      "advisor"
    ],
    "transfer_performance_metrics": [
      "tr",
      "tt",
      "ap"
    ],
    "implementation": [
      "Custom",
      "CS",
      "Formulas",
      "Figures",
      "Tables"
    ],
    "policy_type": [
      "RL",
      "DQN",
      "DDPG"
    ],
    "task_mappings": [
      "N/A"
    ],
    "autonomous_transfer": 0,
    "is_deep_rl": 1,
    "tags": [
      "2D",
      "Navigation",
      "Simulation",
      "Grid",
      "Collect",
      "Cooperation",
      "MultiAgent"
    ],
    "allowed_learner": [
      "TD"
    ],
    "country": [
      "USA",
      "China"
    ],
    "uni": [
      "Carnegie Mellon University",
      "Sun Yat-sen University"
    ],
    "department": [
      "Robotics",
      "Mathematics"
    ],
    "source_task_selection": [],
    "was_in_survey": [
      "mag",
      "multi"
    ],
    "in_title": [
      "t",
      "r",
      "l",
      "k"
    ],
    "in_abs": [
      "t",
      "r",
      "l",
      "k"
    ],
    "in_contet": [
      "t",
      "r",
      "l",
      "k"
    ],
    "rejected_or_unpublished": 2,
    "duplication_status": null,
    "paper_for_thesis": 0,
    "paper_available": 1,
    "behind_paywall": 0,
    "paywall_circumventable": 0,
    "author_names": [
      "Bangwei Li",
      "Yongyuan Liang"
    ]
  },
  {
    "id": 1696410204,
    "title": "Experiments with Adaptive Transfer Rate in Reinforcement Learning",
    "abst": "Transfer algorithms allow the use of knowledge previously learned on related tasks to speed-up learning of the current task. Recently, many complex reinforcement learning problems have been successfully solved by efficient transfer learners. However, most of these algorithms suffer from a severe flaw: they are implicitly tuned to transfer knowledge between tasks having a given degree of similarity. In other words, if the previous task is very dissimilar (resp. nearly identical) to the current task, then the transfer process might slow down the learning (resp. might be far from optimal speed-up). In this paper, we address this specific issue by explicitly optimizing the transfer rate between tasks and answer to the question : &quot;can the transfer rate be accurately optimized, and at what cost ?&quot;. We show that this optimization problem is related to the continuum bandit problem. We then propose a generic adaptive transfer method (AdaTran), which allows to extend several existing transfer learning algorithms to optimize the transfer rate. Finally, we run several experiments validating our approach.",
    "url": "https://rd.springer.com/chapter/10.1007/978-3-642-01715-5_1",
    "lang": "en",
    "authors": [
      178871061,
      2072848526,
      2139589971
    ],
    "fos": [
      41008148,
      77075516,
      97541855,
      2987306524,
      150899416,
      28006648,
      162696548,
      137836250,
      199190896,
      119857082,
      154945302
    ],
    "journals": [],
    "conferences": [],
    "conference_series": [
      1152985112
    ],
    "references": [
      107583932,
      143164768,
      305949787,
      1255659923,
      1510402218,
      1521084402,
      1521332421,
      1582256513,
      1974786743,
      1999874108,
      2014512216,
      2031727428,
      2079247031,
      2097113539,
      2097487180,
      2099667751,
      2122451452,
      2122701159,
      2126565096,
      2129670787,
      2133040789,
      2154328025,
      2158823144
    ],
    "filter_matches": [
      "tr",
      "tl",
      "trl",
      "rl"
    ],
    "rank": 22169,
    "citation_count": 1,
    "estimated_citation_count": 1,
    "publication_date": "2009-05-13",
    "found_in": 1,
    "transfer_experiment_type": [
      "s_i",
      "s_f"
    ],
    "transfer_experiment_subtype": [
      "same_all"
    ],
    "transfer_data_type": [
      "pi"
    ],
    "transfer_performance_metrics": [
      "j",
      "tr",
      "tt"
    ],
    "implementation": [
      "Custom",
      "CS",
      "Pseudo",
      "Formulas",
      "Figures",
      "Tables"
    ],
    "policy_type": [
      "Q",
      "PPR",
      "MGE"
    ],
    "task_mappings": [
      "N/A"
    ],
    "autonomous_transfer": 0,
    "is_deep_rl": 0,
    "tags": [
      "2D",
      "Navigation",
      "Simulation",
      "Grid"
    ],
    "allowed_learner": [
      "TD",
      "H"
    ],
    "country": [
      "France"
    ],
    "uni": [
      "Universit Paris-Dauphine",
      "Universit Paris 6",
      "Centre IRD de l Ile de France"
    ],
    "department": [],
    "source_task_selection": [
      "h"
    ],
    "was_in_survey": [
      "mag"
    ],
    "in_title": [
      "t",
      "r",
      "l"
    ],
    "in_abs": [
      "t",
      "r",
      "l",
      "k"
    ],
    "in_contet": [
      "t",
      "r",
      "l",
      "k"
    ],
    "rejected_or_unpublished": null,
    "duplication_status": null,
    "paper_for_thesis": 0,
    "paper_available": 1,
    "behind_paywall": 1,
    "paywall_circumventable": 1,
    "author_names": [
      "Yann Chevaleyre",
      "Aydano Machado Pamponet",
      "Jean-Daniel Zucker"
    ]
  },
  {
    "id": 2896823681,
    "title": "Episodic memory transfer for multi-task reinforcement learning",
    "abst": "Abstract Episodic memory plays important role in animal behavior. It allows to reuse general skills for solution of specific tasks in changing environment. This beneficial feature of biological cognitive systems is still not incorporated successfully in an artificial neural architectures. In this paper we propose a neural architecture with shared episodic memory for multi-task reinforcement learning (SEM-PAAC). This architecture extends Parallel Advantage Actor Critic (PAAC) with two recurrent sub-networks for separate tracking of environment and task states. The first subnetwork store episodic memory and the second one allows task specific execution of policy. Experiments in the Taxi domain demonstrated that SEM-PAAC has the same performance as PAAC when subtasks are solved separately. On the other hand when subtasks are solved jointly for completing full Taxi task SEM-PAAC is significantly better due to reuse of episodic memory. Proposed architecture also successfully learned to predict task completion. This is a step towards more autonomous agents for multitask problems.",
    "url": "https://www.sciencedirect.com/science/article/abs/pii/S2212683X18300902",
    "lang": "en",
    "authors": [
      1965633265,
      2900997806
    ],
    "fos": [
      2988419192,
      123657996,
      206588197,
      2987198337,
      154945302,
      97541855,
      2780186347,
      88576662,
      13687954,
      41008148
    ],
    "journals": [],
    "conferences": [
      2891121575
    ],
    "conference_series": [
      1122647587
    ],
    "references": [
      1542791059,
      2047057213,
      2064675550,
      2083153956,
      2109910161,
      2114580749,
      2121517924,
      2424347275,
      2530887700,
      2565989828,
      2767050701,
      2885825670
    ],
    "filter_matches": [
      "tr",
      "tl",
      "trl",
      "rl"
    ],
    "rank": 22192,
    "citation_count": 1,
    "estimated_citation_count": 1,
    "publication_date": "2018-10-01",
    "found_in": 1,
    "transfer_experiment_type": [
      "v"
    ],
    "transfer_experiment_subtype": [
      "multi"
    ],
    "transfer_data_type": [
      "LSTM"
    ],
    "transfer_performance_metrics": [
      "ap"
    ],
    "implementation": [
      "Custom",
      "CS",
      "Formulas"
    ],
    "policy_type": [
      "SEM-PAAC",
      "PAAC",
      "A3C-LSTM"
    ],
    "task_mappings": [
      "N/A"
    ],
    "autonomous_transfer": 0,
    "is_deep_rl": 1,
    "tags": [
      "2D",
      "Navigation",
      "Taxi",
      "Simulation",
      "Multitask",
      "Memory",
      "EpisodicMemory"
    ],
    "allowed_learner": [
      "TD",
      "H"
    ],
    "country": [
      "Russia"
    ],
    "uni": [],
    "department": [],
    "source_task_selection": [
      "lib"
    ],
    "was_in_survey": [
      "mag"
    ],
    "in_title": [
      "t",
      "r",
      "l"
    ],
    "in_abs": [
      "t",
      "r",
      "l",
      "s"
    ],
    "in_contet": [
      "t",
      "r",
      "l",
      "k",
      "s"
    ],
    "rejected_or_unpublished": null,
    "duplication_status": null,
    "paper_for_thesis": 0,
    "paper_available": 1,
    "behind_paywall": 1,
    "paywall_circumventable": 1,
    "author_names": [
      "Mikhail Burtsev",
      "Artyom Y. Sorokin"
    ]
  },
  {
    "id": 2984781810,
    "title": "Unsupervised Reinforcement Learning of Transferable Meta-Skills for Embodied Navigation",
    "abst": "Visual navigation is a task of training an embodied agent by intelligently navigating to a target object (e.g., television) using only visual observations. A key challenge for current deep reinforcement learning models lies in the requirements for a large amount of training data. It is exceedingly expensive to construct sufficient 3D synthetic environments annotated with the target object information. In this paper, we focus on visual navigation in the low-resource setting, where we have only a few training environments annotated with object information. We propose a novel unsupervised reinforcement learning approach to learn transferable meta-skills (e.g., bypass obstacles, go straight) from unannotated environments without any supervisory signals. The agent can then fast adapt to visual navigation through learning a high-level master policy to combine these meta-skills, when the visual-navigation-specified reward is provided. Evaluation in the AI2-THOR environments shows that our method significantly outperforms the baseline by 53.34% relatively on SPL, and further qualitative analysis demonstrates that our method learns transferable motor primitives for visual navigation.",
    "url": "https://export.arxiv.org/pdf/1911.07450",
    "lang": "en",
    "authors": [
      2110072997,
      2110566271,
      2127506760,
      2266924332,
      2775392882,
      2956043400,
      2965830330
    ],
    "fos": [
      2987820722,
      100609095,
      119857082,
      51632099,
      41008148,
      103683099,
      154945302,
      3018587665,
      97541855,
      2988863489
    ],
    "journals": [
      2597175965
    ],
    "conferences": [],
    "conference_series": [],
    "references": [
      1516027685,
      1585381382,
      2006501475,
      2030021468,
      2109910161,
      2113560487,
      2123417757,
      2128990851,
      2137825550,
      2395579298,
      2472819217,
      2601450892,
      2603088459,
      2606433045,
      2734377693,
      2749928749,
      2753160622,
      2763323349,
      2765602917,
      2772390515,
      2774005037,
      2776202271,
      2783375473,
      2787501667,
      2788741142,
      2795900505,
      2803325309,
      2804117224,
      2808682055,
      2884565639,
      2948974578,
      2951660448,
      2952791429,
      2962723986,
      2962732398,
      2962887844,
      2963313316,
      2963341924,
      2963523627,
      2963800628,
      2963846044,
      2964043796,
      2964105864,
      2964935470,
      2980730703,
      2982053164
    ],
    "filter_matches": [
      "tr",
      "tl",
      "trl",
      "rl"
    ],
    "rank": 22219,
    "citation_count": 0,
    "estimated_citation_count": 0,
    "publication_date": "2019-11-18",
    "found_in": 1,
    "transfer_experiment_type": [
      "s_i",
      "s_f",
      "s",
      "levels"
    ],
    "transfer_experiment_subtype": [
      "multi"
    ],
    "transfer_data_type": [
      "Curriculum"
    ],
    "transfer_performance_metrics": [
      "tt",
      "tr",
      "ap"
    ],
    "implementation": [
      "Custom",
      "CS",
      "Formulas",
      "Pseudo",
      "Figures",
      "Tables"
    ],
    "policy_type": [
      "A3C",
      "LSTM"
    ],
    "task_mappings": [
      "N/A"
    ],
    "autonomous_transfer": 0,
    "is_deep_rl": 1,
    "tags": [
      "3D",
      "Navigation",
      "Simulation",
      "Hypernetwork",
      "AI2-THOR"
    ],
    "allowed_learner": [
      "TD",
      "H",
      "MB"
    ],
    "country": [
      "China",
      "USA"
    ],
    "uni": [
      "Zheijang University",
      "University of California"
    ],
    "department": [],
    "source_task_selection": [],
    "was_in_survey": [
      "mag"
    ],
    "in_title": [
      "t",
      "r",
      "l",
      "s"
    ],
    "in_abs": [
      "t",
      "r",
      "l",
      "s"
    ],
    "in_contet": [
      "t",
      "r",
      "l",
      "s",
      "k"
    ],
    "rejected_or_unpublished": 2,
    "duplication_status": null,
    "paper_for_thesis": 0,
    "paper_available": 1,
    "behind_paywall": 0,
    "paywall_circumventable": 0,
    "author_names": [
      "Yueting Zhuang",
      "William Yang Wang",
      "Siliang Tang",
      "Fei Wu",
      "Xin Wang",
      "Haizhou Shi",
      "Juncheng Li"
    ]
  },
  {
    "id": 3027086341,
    "title": "Human Instruction-Following with Deep Reinforcement Learning via Transfer-Learning from Text",
    "abst": "Recent work has described neural-network-based agents that are trained with reinforcement learning (RL) to execute language-like commands in simulated worlds, as a step towards an intelligent agent or robot that can be instructed by human users. However, the optimisation of multi-goal motor policies via deep RL from scratch requires many episodes of experience. Consequently, instruction-following with deep RL typically involves language generated from templates (by an environment simulator), which does not reflect the varied or ambiguous expressions of real users. Here, we propose a conceptually simple method for training instruction-following agents with deep RL that are robust to natural human instructions. By applying our method with a state-of-the-art pre-trained text-based language model (BERT), on tasks requiring agents to identify and position everyday objects relative to other objects in a naturalistic 3D simulated room, we demonstrate substantially-above-chance zero-shot transfer from synthetic template commands to natural instructions given by humans. Our approach is a general recipe for training any deep RL-based system to interface with human users, and bridges the gap between two research directions of notable recent success: agent-centric motor behavior and text-based representation learning.",
    "url": "http://arxiv.org/pdf/2005.09382.pdf",
    "lang": null,
    "authors": [
      2188096649,
      3003253308,
      3004223286,
      3004394582
    ],
    "fos": [
      137293760,
      59404180,
      90509273,
      76482347,
      107457646,
      74072328,
      119857082,
      97541855,
      154945302,
      41008148,
      2781235140,
      150899416
    ],
    "journals": [
      2596401190
    ],
    "conferences": [],
    "conference_series": [],
    "references": [
      46490633,
      1522301498,
      1821462560,
      1854884267,
      1949907236,
      2005814556,
      2060088703,
      2117130368,
      2118781169,
      2121879602,
      2190691619,
      2410540304,
      2611884151,
      2620290674,
      2627585944,
      2783375473,
      2785955089,
      2786036274,
      2803325309,
      2895560838,
      2896457183,
      2897513296,
      2899336413,
      2911109671,
      2918241733,
      2947476638,
      2950494312,
      2951420334,
      2951725892,
      2952872637,
      2963403868,
      2963726321,
      2963871073,
      2964043796,
      2964084698,
      2964935470,
      2965373594,
      2967727187,
      2970608575,
      2980853364,
      2990021239,
      2994943647,
      2995264919
    ],
    "filter_matches": [
      "tr",
      "tl",
      "trl",
      "rl"
    ],
    "rank": 22224,
    "citation_count": 0,
    "estimated_citation_count": 0,
    "publication_date": "2020-05-19",
    "found_in": 1,
    "transfer_experiment_type": [],
    "transfer_experiment_subtype": [
      "same_all"
    ],
    "transfer_data_type": [
      "WT"
    ],
    "transfer_performance_metrics": [
      "ap"
    ],
    "implementation": [
      "Custom",
      "CS",
      "BERT",
      "TensorFlow",
      "Figures",
      "Tables"
    ],
    "policy_type": [
      "BERT"
    ],
    "task_mappings": [
      "N/A"
    ],
    "autonomous_transfer": 0,
    "is_deep_rl": 1,
    "tags": [
      "NLP",
      "TextInstructions",
      "3D",
      "Unity",
      "Simulation"
    ],
    "allowed_learner": [
      "TD"
    ],
    "country": [
      "USA"
    ],
    "uni": [
      "DeepMind"
    ],
    "department": [
      "Industry"
    ],
    "source_task_selection": [
      "h"
    ],
    "was_in_survey": [
      "mag"
    ],
    "in_title": [
      "t",
      "r",
      "l"
    ],
    "in_abs": [
      "t",
      "r",
      "l"
    ],
    "in_contet": [
      "t",
      "r",
      "l",
      "k"
    ],
    "rejected_or_unpublished": 2,
    "duplication_status": null,
    "paper_for_thesis": 0,
    "paper_available": 1,
    "behind_paywall": 0,
    "paywall_circumventable": 0,
    "author_names": [
      "Felix Hill",
      "Nathaniel Wong",
      "Tim Harley",
      "Sona Mokra"
    ]
  },
  {
    "id": 2962744619,
    "title": "An improved reinforcement learning algorithm based on knowledge transfer and applications in autonomous vehicles",
    "abst": "Abstract Autonomous learning is a crucially important capability of intelligent robots. As one of the most fashionable machine learning techniques, the reinforcement learning (RL) enables an agent taking an optimized action by interacting with the environment so as to maximize some notion of cumulative reward. In this paper, an improved RL algorithm, named as the KT-HA-Q(&#x03BB;) algorithm, is proposed by resorting to the knowledge transfer of source domain. First, a BP neural network and a liner sensor network are skillfully constructed to perform the knowledge transfer of source task for weight initialization in target task, and the knowledge transfer on actions of case base obtained by source domain, respectively. Then, the novel case base expansion and progressive forgetting criterion, which realize the balance between new experience via online learning and historical experience in the case base, are developed to enhance the learning efficiency and the learning rate. Furthermore, an improved heuristic function is proposed by replacing the action traditionally obtained via a selection strategy by the experience action. This function acts as a crucial role for both the best action selection and its Q value calculation. Finally, the proposed algorithm is utilized in the hill-climbing experiment of unmanned vehicles under a complex 3D scene by transferring the knowledge obtained in a 2D scene. The results of contrast experiments verified the advantages and effectiveness of the proposed algorithm.",
    "url": "https://dblp.uni-trier.de/db/journals/ijon/ijon361.html#DingDWH19",
    "lang": null,
    "authors": [
      2131365882,
      2149480936,
      2889480060,
      2963429441
    ],
    "fos": [
      2776960227,
      50644808,
      2984650650,
      7149132,
      166109690,
      114466953,
      154945302,
      119857082,
      97541855,
      24590314,
      33923547
    ],
    "journals": [
      45693802
    ],
    "conferences": [],
    "conference_series": [],
    "references": [
      193076044,
      605348272,
      851745253,
      1530174427,
      1665214252,
      1799762961,
      1963487302,
      1973627122,
      1986014385,
      2002334697,
      2020573190,
      2021247827,
      2031677098,
      2036103676,
      2056584142,
      2073320424,
      2079206596,
      2096600060,
      2097381042,
      2111478121,
      2113913482,
      2123178778,
      2126385963,
      2130935956,
      2133040789,
      2144366468,
      2145339207,
      2156387975,
      2165698076,
      2259258048,
      2634239194,
      2780033408,
      2789561271,
      2790702239,
      2799479426,
      2803459852,
      2885108836,
      2905630037,
      2922520136,
      2962938168
    ],
    "filter_matches": [
      "tr",
      "tl",
      "trl",
      "rl"
    ],
    "rank": 22225,
    "citation_count": 1,
    "estimated_citation_count": 1,
    "publication_date": "2019-10-07",
    "found_in": 1,
    "transfer_experiment_type": [
      "v",
      "t"
    ],
    "transfer_experiment_subtype": [
      "lit"
    ],
    "transfer_data_type": [
      "cases",
      "Q",
      "pi_gen"
    ],
    "transfer_performance_metrics": [
      "j"
    ],
    "implementation": [
      "Custom",
      "CS",
      "Formulas",
      "Pseudo",
      "Figures",
      "Tables"
    ],
    "policy_type": [
      "Q",
      "KT-HA-Q"
    ],
    "task_mappings": [
      "sup"
    ],
    "autonomous_transfer": 0,
    "is_deep_rl": 0,
    "tags": [
      "2D",
      "ClassicControl",
      "Simulation",
      "2Dto3D",
      "3D",
      "MountainCar",
      "MountainCar3D"
    ],
    "allowed_learner": [
      "TD",
      "CBR"
    ],
    "country": [
      "China"
    ],
    "uni": [
      "University of Shanghai",
      "Northeast Petroleum University"
    ],
    "department": [
      "Control Science and Engineering",
      "Complex Systems and Advanced Control"
    ],
    "source_task_selection": [
      "h"
    ],
    "was_in_survey": [
      "mag"
    ],
    "in_title": [
      "t",
      "r",
      "l",
      "k"
    ],
    "in_abs": [
      "t",
      "r",
      "l",
      "k",
      "s"
    ],
    "in_contet": [
      "t",
      "r",
      "l",
      "k",
      "s"
    ],
    "rejected_or_unpublished": null,
    "duplication_status": null,
    "paper_for_thesis": 0,
    "paper_available": 1,
    "behind_paywall": 1,
    "paywall_circumventable": 1,
    "author_names": [
      "Guoliang Wei",
      "Derui Ding",
      "Fei Han",
      "Zifan Ding"
    ]
  },
  {
    "id": 2901461022,
    "title": "Self-organizing maps for storage and transfer of knowledge in reinforcement learning:",
    "abst": "The idea of reusing or transferring information from previously learned tasks (source tasks) for the learning of new tasks (target tasks) has the potential to significantly improve the sample effic...",
    "url": "https://doi.org/10.1177/1059712318818568",
    "lang": "en",
    "authors": [
      2428647411,
      2614491958
    ],
    "fos": [
      188116033,
      2993389844,
      206588197,
      154945302,
      41008148,
      150899416,
      97541855,
      111168008
    ],
    "journals": [
      183337005
    ],
    "conferences": [],
    "conference_series": [],
    "references": [
      158722652,
      567721252,
      1460713219,
      1566538838,
      1582256513,
      1974043469,
      1996476189,
      2002978771,
      2046079134,
      2056049891,
      2097381042,
      2114901408,
      2119567691,
      2121863487,
      2130935956,
      2132622533,
      2141559023,
      2154727892,
      2167518172,
      2168701898,
      2174786457,
      2334782222,
      2464736835,
      2616311403,
      2735506162,
      2949600457,
      2951618017,
      2962829499
    ],
    "filter_matches": [
      "tr",
      "tl",
      "trl",
      "rl"
    ],
    "rank": 22227,
    "citation_count": 1,
    "estimated_citation_count": 1,
    "publication_date": "2018-12-20",
    "found_in": 1,
    "transfer_experiment_type": [
      "s_i",
      "s_f"
    ],
    "transfer_experiment_subtype": [
      "same_all"
    ],
    "transfer_data_type": [
      "Q"
    ],
    "transfer_performance_metrics": [
      "tr"
    ],
    "implementation": [
      "Custom",
      "CS",
      "Pseudo",
      "Formulas",
      "Figures"
    ],
    "policy_type": [
      "SOM",
      "Q"
    ],
    "task_mappings": [
      "SOM"
    ],
    "autonomous_transfer": 0,
    "is_deep_rl": 0,
    "tags": [
      "SOF",
      "SelfOrganizingMap",
      "2D",
      "Navigation",
      "Simulation"
    ],
    "allowed_learner": [
      "TD"
    ],
    "country": [
      "Singapore"
    ],
    "uni": [
      "Singapore University of Technology and Design"
    ],
    "department": [
      "Computer Science"
    ],
    "source_task_selection": [
      "lib"
    ],
    "was_in_survey": [
      "mag"
    ],
    "in_title": [
      "t",
      "r",
      "l"
    ],
    "in_abs": [
      "t",
      "r",
      "l",
      "k"
    ],
    "in_contet": [
      "t",
      "r",
      "l",
      "k"
    ],
    "rejected_or_unpublished": null,
    "duplication_status": null,
    "paper_for_thesis": 0,
    "paper_available": 1,
    "behind_paywall": 0,
    "paywall_circumventable": 0,
    "author_names": [
      "Roland Bouffanais",
      "Thommen George Karimpanal"
    ]
  },
  {
    "id": 3003281790,
    "title": "Multiperspective Light Field Reconstruction Method via Transfer Reinforcement Learning.",
    "abst": "Compared with traditional imaging, the light field contains more comprehensive image information and higher image quality. However, the available data for light field reconstruction are limited, and the repeated calculation of data seriously affects the accuracy and the real-time performance of multiperspective light field reconstruction. To solve the problems, this paper proposes a multiperspective light field reconstruction method based on transfer reinforcement learning. Firstly, the similarity measurement model is established. According to the similarity threshold of the source domain and the target domain, the reinforcement learning model or the feature transfer learning model is autonomously selected. Secondly, the reinforcement learning model is established. The model uses multiagent (i.e., multiperspective) Q-learning to learn the feature set that is most similar to the target domain and the source domain and feeds it back to the source domain. This model increases the capacity of the source-domain samples and improves the accuracy of light field reconstruction. Finally, the feature transfer learning model is established. The model uses PCA to obtain the maximum embedding space of source-domain and target-domain features and maps similar features to a new space for label data migration. This model solves the problems of multiperspective data redundancy and repeated calculations and improves the real-time performance of maneuvering target recognition. Extensive experiments on PASCAL VOC datasets demonstrate the effectiveness of the proposed algorithm against the existing algorithms.",
    "url": "http://downloads.hindawi.com/journals/cin/2020/8989752.pdf",
    "lang": null,
    "authors": [
      2141325285,
      2969213663,
      2969493313,
      2969931023,
      2978116196
    ],
    "fos": [
      97541855,
      48983235,
      154945302,
      41608201,
      55020928,
      7545210,
      146152329,
      178980831,
      150899416,
      3019013110,
      41008148
    ],
    "journals": [
      72372694
    ],
    "conferences": [],
    "conference_series": [],
    "references": [
      639708223,
      1584919825,
      2016202900,
      2085048807,
      2091978542,
      2094286293,
      2115595637,
      2165698076,
      2310937270,
      2346736747,
      2395611524,
      2531327760,
      2534745787,
      2579348194,
      2596402752,
      2604970008,
      2606437410,
      2733625932,
      2744647809,
      2765180720,
      2767121642,
      2782796222,
      2784755946,
      2794426621,
      2800554784,
      2963881378,
      2969218386
    ],
    "filter_matches": [
      "tr",
      "tl",
      "trl",
      "rl"
    ],
    "rank": 22242,
    "citation_count": 0,
    "estimated_citation_count": 0,
    "publication_date": "2020-02-14",
    "found_in": 1,
    "transfer_experiment_type": [
      "s"
    ],
    "transfer_experiment_subtype": [
      "lit"
    ],
    "transfer_data_type": [
      "pi"
    ],
    "transfer_performance_metrics": [
      "ap"
    ],
    "implementation": [
      "Custom",
      "CS",
      "Formulas",
      "Pseudo",
      "TensorFlow",
      "Tables"
    ],
    "policy_type": [
      "Q"
    ],
    "task_mappings": [
      "exp"
    ],
    "autonomous_transfer": 0,
    "is_deep_rl": 1,
    "tags": [
      "Simulation",
      "LightFieldReconstruction",
      "PCA",
      "Q",
      "Image",
      "ImageRecognition",
      "CNN",
      "Vehicle",
      "MultiAgent"
    ],
    "allowed_learner": [
      "TD"
    ],
    "country": [
      "China"
    ],
    "uni": [
      "Henan Institue of Science and Technology",
      "Shandong University"
    ],
    "department": [
      "Artifical Intelligence",
      "Information Engineering",
      "Control Science and Engineering"
    ],
    "source_task_selection": [
      "all"
    ],
    "was_in_survey": [
      "mag"
    ],
    "in_title": [
      "t",
      "r",
      "l"
    ],
    "in_abs": [
      "t",
      "r",
      "l"
    ],
    "in_contet": [
      "t",
      "r",
      "l"
    ],
    "rejected_or_unpublished": null,
    "duplication_status": null,
    "paper_for_thesis": 0,
    "paper_available": 1,
    "behind_paywall": 0,
    "paywall_circumventable": 0,
    "author_names": [
      "Zhenxue Chen",
      "Guangfu Zhou",
      "Lei Cai",
      "Peien Luo",
      "Tao Xu"
    ]
  },
  {
    "id": 2965999293,
    "title": "Training a RoboCup Striker Agent via Transferred Reinforcement Learning.",
    "abst": "Recent developments in reinforcement learning algorithms have made it possible to train agents in highly complex state and action spaces, including action spaces with continuous parameters. Advancements such as the Deep-Q Network and the Deep Deterministic Policy Gradient were a critical step in making reinforcement learning a feasible option for training agents in real world scenarios. The viability of these technologies has previously been demonstrated in training a RoboCup Soccer agent with no prior domain knowledge to successfully score goals; however, this work required an engineered intermediate reward system to direct the agent in its exploration of the environment. We introduce the use of transfer learning rather than engineered rewards. Our results are positive, showing that it is possible to train an agent through a series of increasingly difficult tasks with fewer training iterations than with an engineered reward. However, when the agent&#x2019;s likelihood of success in a task is low, it may be necessary to reintroduce an engineered reward or to provide extended training and exploration using simpler tasks.",
    "url": "https://link.springer.com/chapter/10.1007%2F978-3-030-27544-0_9",
    "lang": null,
    "authors": [
      2164180442,
      2970881702
    ],
    "fos": [
      207685749,
      97541855,
      41008148,
      154945302,
      41550386,
      150899416,
      143661069,
      44154836
    ],
    "journals": [],
    "conferences": [],
    "conference_series": [
      1170686637
    ],
    "references": [
      1584313244,
      1886539647,
      2031748952,
      2097381042,
      2104641222,
      2121863487,
      2127412976,
      2131600418,
      2141754131,
      2145339207,
      2153353285,
      2161968596,
      2165150801,
      2787848939,
      2963616477,
      2963864421
    ],
    "filter_matches": [
      "tr",
      "tl",
      "trl",
      "rl"
    ],
    "rank": 22260,
    "citation_count": 0,
    "estimated_citation_count": 0,
    "publication_date": "2018-06-18",
    "found_in": 1,
    "transfer_experiment_type": [
      "s_i",
      "s_f"
    ],
    "transfer_experiment_subtype": [
      "same_all"
    ],
    "transfer_data_type": [
      "pi"
    ],
    "transfer_performance_metrics": [
      "tt"
    ],
    "implementation": [
      "Custom",
      "CS",
      "Tables",
      "Figures",
      "Formulas"
    ],
    "policy_type": [
      "DDPG"
    ],
    "task_mappings": [
      "N/A"
    ],
    "autonomous_transfer": 0,
    "is_deep_rl": 1,
    "tags": [
      "Simulation",
      "Striker",
      "RoboCup",
      "Curriculum",
      "CNN",
      "MultiAgent",
      "Soccer"
    ],
    "allowed_learner": [
      "TD",
      "MB"
    ],
    "country": [
      "USA"
    ],
    "uni": [
      "Colorado School of Mines"
    ],
    "department": [
      "Computer Science"
    ],
    "source_task_selection": [
      "all"
    ],
    "was_in_survey": [
      "mag"
    ],
    "in_title": [
      "t",
      "r",
      "l"
    ],
    "in_abs": [
      "t",
      "r",
      "l",
      "k"
    ],
    "in_contet": [
      "t",
      "r",
      "l",
      "k",
      "s"
    ],
    "rejected_or_unpublished": null,
    "duplication_status": null,
    "paper_for_thesis": 0,
    "paper_available": 1,
    "behind_paywall": 1,
    "paywall_circumventable": 1,
    "author_names": [
      "Tracy Camp",
      "Warren Blair Watkinson"
    ]
  },
  {
    "id": 2062960870,
    "title": "Preferential exploration method of transfer learning for reinforcement learning in Same Transition Model",
    "abst": "We aim to accelerate learning processes in reinforcement learning by transfer learning. Its concept is that knowledge to solve similar tasks accelerates a learning process of a target task. We have proposed that the basic transfer method based on forbidden rule set that is a set of rules which cause to immediately failure of a target task. However, the basic method works poorly for the &quot;Same Transition Model,&quot; which has same state transition probability and different goal. In this article, we propose an effective transfer learning method in same transition model. In detail, it consists of two strategies: (1) approaching to the goal for the selected source task quickly, and (2) exploring states around the goal preferentially.",
    "url": "https://dblp.uni-trier.de/db/conf/scisisis/scisisis2012.html#TakanoTKT12",
    "lang": null,
    "authors": [
      1895828408,
      1994321547,
      2113915844,
      2158234577
    ],
    "fos": [
      41008148,
      28006648,
      24138899,
      77967617,
      154945302,
      188888258,
      119857082,
      97541855,
      77075516,
      150899416,
      3017401344
    ],
    "journals": [],
    "conferences": [
      590552851
    ],
    "conference_series": [
      1123077274
    ],
    "references": [
      139877375,
      2031727428,
      2097381042,
      2107726111,
      2139160917,
      2396541868
    ],
    "filter_matches": [
      "tr",
      "tl",
      "trl",
      "rl"
    ],
    "rank": 22279,
    "citation_count": 1,
    "estimated_citation_count": 1,
    "publication_date": "2012-11-01",
    "found_in": 1,
    "transfer_experiment_type": [
      "s_i",
      "s_f",
      "s",
      "levels"
    ],
    "transfer_experiment_subtype": [
      "multi"
    ],
    "transfer_data_type": [
      "rule",
      "pi"
    ],
    "transfer_performance_metrics": [
      "tt"
    ],
    "implementation": [
      "Custom",
      "CS",
      "Formulas",
      "Pseudo",
      "Tables",
      "Figures"
    ],
    "policy_type": [
      "Q"
    ],
    "task_mappings": [
      "N/A"
    ],
    "autonomous_transfer": 0,
    "is_deep_rl": 0,
    "tags": [
      "2D",
      "Navigation",
      "Grid",
      "Maze",
      "Simulation"
    ],
    "allowed_learner": [
      "TD"
    ],
    "country": [
      "Japan"
    ],
    "uni": [
      "Mie University"
    ],
    "department": [
      "Engineering",
      "Regional Innovation Studies"
    ],
    "source_task_selection": [
      "lib"
    ],
    "was_in_survey": [
      "mag"
    ],
    "in_title": [
      "t",
      "r",
      "l"
    ],
    "in_abs": [
      "t",
      "r",
      "l",
      "k"
    ],
    "in_contet": [
      "t",
      "r",
      "l",
      "k"
    ],
    "rejected_or_unpublished": null,
    "duplication_status": null,
    "paper_for_thesis": 0,
    "paper_available": 1,
    "behind_paywall": 1,
    "paywall_circumventable": 1,
    "author_names": [
      "Hiroharu Kawanaka",
      "Shinji Tsuruoka",
      "Toshiaki Takano",
      "Haruhiko Takase"
    ]
  },
  {
    "id": 2902788990,
    "title": "Cache-Enabled Adaptive Bit Rate Streaming via Deep Self-Transfer Reinforcement Learning",
    "abst": "Caching and rate allocation are two promising approaches to support video streaming over wireless networks. However, existing rate allocation designs do not fully exploit the advantages of the two approaches. This paper investigates the problem of cache-enabled video rate allocation. We establish a mathematical model for this problem, and point out that it is difficult to solve it with traditional dynamic programming. Then we propose a deep reinforcement learning approach to solve it. Firstly, we model the problem as a Markov decision problem. Then we present a deep Q-learning algorithm with a special knowledge transfer process to find out an effective allocation policy. Finally, numerical results are given to demonstrate that the proposed solution can effectively maintain high-quality of service. We also investigate the impact of critical parameters on the performance of our algorithm.",
    "url": "https://dblp.uni-trier.de/db/conf/wcsp/wcsp2018.html#ZhangZLHY18",
    "lang": null,
    "authors": [
      2100829531,
      2102163052,
      2110710021,
      2795210709,
      2795220113
    ],
    "fos": [
      2776960227,
      115537543,
      31258907,
      165696696,
      37404715,
      97541855,
      108037233,
      120314980,
      41008148,
      3020015351,
      2987812609
    ],
    "journals": [],
    "conferences": [],
    "conference_series": [
      2622754298
    ],
    "references": [
      393153616,
      1569909993,
      1994613437,
      2017146017,
      2024725057,
      2053157787,
      2098432798,
      2121863487,
      2166302491,
      2257979135,
      2291595385,
      2426267443,
      2508712411,
      2590526842,
      2611373201,
      2741576192,
      2963864421,
      2964121744
    ],
    "filter_matches": [
      "tr",
      "tl",
      "trl",
      "rl"
    ],
    "rank": 22316,
    "citation_count": 1,
    "estimated_citation_count": 1,
    "publication_date": "2018-10-01",
    "found_in": 1,
    "transfer_experiment_type": [],
    "transfer_experiment_subtype": [
      "same_all"
    ],
    "transfer_data_type": [
      "r",
      "imitation"
    ],
    "transfer_performance_metrics": [
      "tr"
    ],
    "implementation": [
      "Custom",
      "CS",
      "Formulas",
      "Pseudo",
      "Figures",
      "Tables"
    ],
    "policy_type": [
      "DQN",
      "NAF"
    ],
    "task_mappings": [
      "N/A"
    ],
    "autonomous_transfer": 0,
    "is_deep_rl": 1,
    "tags": [
      "Network",
      "Wireless",
      "Simulation",
      "Adaptive",
      "BitRate",
      "Streaming",
      "SelfTransfer",
      "CacheEnabled"
    ],
    "allowed_learner": [
      "TD"
    ],
    "country": [
      "China"
    ],
    "uni": [
      "Southeast University"
    ],
    "department": [
      "Information Science and Engineering"
    ],
    "source_task_selection": [],
    "was_in_survey": [
      "mag"
    ],
    "in_title": [
      "t",
      "r",
      "l"
    ],
    "in_abs": [
      "t",
      "r",
      "l",
      "k"
    ],
    "in_contet": [
      "t",
      "r",
      "l",
      "k",
      "s"
    ],
    "rejected_or_unpublished": null,
    "duplication_status": null,
    "paper_for_thesis": 0,
    "paper_available": 1,
    "behind_paywall": 1,
    "paywall_circumventable": 1,
    "author_names": [
      "Yongming Huang",
      "Luxi Yang",
      "Chunguo Li",
      "Yaru Zheng",
      "Zhengming Zhang"
    ]
  },
  {
    "id": 2906852733,
    "title": "Reusing Source Task Knowledge via Transfer Approximator in Reinforcement Transfer Learning",
    "abst": "Transfer Learning (TL) has received a great deal of attention because of its ability to speed up Reinforcement Learning (RL) by reusing learned knowledge from other tasks. This paper proposes a new transfer learning framework, referred to as Transfer Learning via Artificial Neural Network Approximator (TL-ANNA). It builds an Artificial Neural Network (ANN) transfer approximator to transfer the related knowledge from the source task into the target task and reuses the transferred knowledge with a Probabilistic Policy Reuse (PPR) scheme. Specifically, the transfer approximator maps the state of the target task symmetrically to states of the source task with a certain mapping rule, and activates the related knowledge (components of the action-value function) of the source task as the input of the ANNs; it then predicts the quality of the actions in the target task with the ANNs. The target learner uses the PPR scheme to bias the RL with the suggested action from the transfer approximator. In this way, the transfer approximator builds a symmetric knowledge path between the target task and the source task. In addition, two mapping rules for the transfer approximator are designed, namely, Full Mapping Rule and Group Mapping Rule. Experiments performed on the RoboCup soccer Keepaway task verified that the proposed transfer learning methods outperform two other transfer learning methods in both jumpstart and time to threshold metrics and are more robust to the quality of source knowledge. In addition, the TL-ANNA with the group mapping rule exhibits slightly worse performance than the one with the full mapping rule, but with less computation and space cost when appropriate grouping method is used.",
    "url": "https://www.mdpi.com/2073-8994/11/1/25/pdf",
    "lang": "en",
    "authors": [
      2101925657,
      2244529586,
      2633508326,
      2906870694
    ],
    "fos": [
      206588197,
      114614502,
      154945302,
      33923547,
      67203356,
      45374587,
      68339613,
      150899416,
      50644808,
      49937458,
      97541855
    ],
    "journals": [
      190787756
    ],
    "conferences": [],
    "conference_series": [],
    "references": [
      2020573190,
      2056584142,
      2097381042,
      2133040789,
      2517639096,
      2555706138,
      2789901741,
      2802636049,
      2807652507
    ],
    "filter_matches": [
      "tr",
      "tl",
      "trl",
      "rl"
    ],
    "rank": 22399,
    "citation_count": 0,
    "estimated_citation_count": 0,
    "publication_date": "2018-12-29",
    "found_in": 1,
    "transfer_experiment_type": [
      "s",
      "s_i",
      "s_f",
      "t"
    ],
    "transfer_experiment_subtype": [
      "diff-it",
      "lit"
    ],
    "transfer_data_type": [
      "fea",
      "Q"
    ],
    "transfer_performance_metrics": [
      "j",
      "tt"
    ],
    "implementation": [
      "Custom",
      "CS",
      "Pseudo",
      "Formulas",
      "Figures",
      "Tables"
    ],
    "policy_type": [
      "TL-ANNA",
      "PPR",
      "Q"
    ],
    "task_mappings": [
      "exp"
    ],
    "autonomous_transfer": 1,
    "is_deep_rl": 1,
    "tags": [
      "2D",
      "Navigation",
      "RoboCup",
      "Simulation",
      "KeepAway",
      "ProbabilisticPolicyReuse",
      "MultiAgent",
      "3v2",
      "4v3"
    ],
    "allowed_learner": [
      "TD"
    ],
    "country": [
      "China"
    ],
    "uni": [
      "National University of Defense Technology"
    ],
    "department": [
      "Intelligence Science and Technology"
    ],
    "source_task_selection": [
      "lib"
    ],
    "was_in_survey": [
      "mag"
    ],
    "in_title": [
      "t",
      "r",
      "l",
      "k"
    ],
    "in_abs": [
      "t",
      "r",
      "l",
      "k"
    ],
    "in_contet": [
      "t",
      "r",
      "l",
      "k",
      "s"
    ],
    "rejected_or_unpublished": null,
    "duplication_status": null,
    "paper_for_thesis": 0,
    "paper_available": 1,
    "behind_paywall": 0,
    "paywall_circumventable": 0,
    "author_names": [
      "Xiangke Wang",
      "Qiao Cheng",
      "Yifeng Niu",
      "Lincheng Shen"
    ]
  },
  {
    "id": 2949169393,
    "title": "Continual Reinforcement Learning deployed in Real-life using Policy Distillation and Sim2Real Transfer",
    "abst": "We focus on the problem of teaching a robot to solve tasks presented sequentially, i.e., in a continual learning scenario. The robot should be able to solve all tasks it has encountered, without forgetting past tasks. We provide preliminary work on applying Reinforcement Learning to such setting, on 2D navigation tasks for a 3 wheel omni-directional robot. Our approach takes advantage of state representation learning and policy distillation. Policies are trained using learned features as input, rather than raw observations, allowing better sample efficiency. Policy distillation is used to combine multiple policies into a single one that solves all encountered tasks.",
    "url": "https://openreview.net/pdf?id=BklR5pHhjV",
    "lang": "en",
    "authors": [
      290876669,
      2753125602,
      2890645897,
      2893751346,
      2911998776,
      2951685921
    ],
    "fos": [
      97541855,
      204030448,
      90509273,
      154945302,
      119857082,
      3020157229,
      7149132,
      2989414621,
      2993389844,
      33923547
    ],
    "journals": [
      2597173376
    ],
    "conferences": [],
    "conference_series": [],
    "references": [
      1821462560,
      2144823724,
      2160178500,
      2210483910,
      2410983263,
      2426267443,
      2473930607,
      2530944449,
      2560647685,
      2567455162,
      2583761661,
      2584377191,
      2605102758,
      2737492962,
      2751258126,
      2754695199,
      2765101016,
      2787666871,
      2788781499,
      2804175194,
      2808844346,
      2809425026,
      2892234010,
      2892673593,
      2894485595,
      2897007337,
      2902907165,
      2906272760,
      2916592595,
      2963156511,
      2963199420,
      2963430173,
      2963559848,
      2964001908,
      2964189064,
      2974990249
    ],
    "filter_matches": [
      "tr",
      "tl",
      "trl",
      "rl"
    ],
    "rank": 22431,
    "citation_count": 4,
    "estimated_citation_count": 4,
    "publication_date": "2019-05-05",
    "found_in": 1,
    "transfer_experiment_type": [
      "t",
      "s_i",
      "s_f",
      "s",
      "levels"
    ],
    "transfer_experiment_subtype": [
      "sim2real"
    ],
    "transfer_data_type": [
      "distil",
      "pi_dyn",
      "SRL"
    ],
    "transfer_performance_metrics": [
      "tr"
    ],
    "implementation": [
      "OSS",
      "stable-baselines2",
      "PyBullet",
      "numpy",
      "Figures",
      "Formulas",
      "Videos"
    ],
    "policy_type": [
      "PPO2"
    ],
    "task_mappings": [
      "N/A"
    ],
    "autonomous_transfer": 0,
    "is_deep_rl": 1,
    "tags": [
      "2D",
      "Navigation",
      "Robotics",
      "Simulation",
      "RealWorld"
    ],
    "allowed_learner": [
      "TD"
    ],
    "country": [
      "France"
    ],
    "uni": [
      "Flowers Laboratory",
      "Softbank Robotics Europe",
      "Thales"
    ],
    "department": [
      "AI Lab",
      "Theresis Lab"
    ],
    "source_task_selection": [
      "lib"
    ],
    "was_in_survey": [
      "mag"
    ],
    "in_title": [
      "t",
      "r",
      "l"
    ],
    "in_abs": [
      "r",
      "l"
    ],
    "in_contet": [
      "t",
      "r",
      "l",
      "k"
    ],
    "rejected_or_unpublished": null,
    "duplication_status": null,
    "paper_for_thesis": 0,
    "paper_available": 1,
    "behind_paywall": 0,
    "paywall_circumventable": 0,
    "author_names": [
      "David Filliat",
      "Timoth&#x00E9;e Lesort",
      "Hugo Caselles-Dupr&#x00E9;",
      "Ren&#x00E9; Traor&#x00E9;",
      "Natalia D&#x00ED;az-Rodr&#x00ED;guez",
      "Te Sun"
    ]
  },
  {
    "id": 2973019294,
    "title": "Transferring Human Manipulation Knowledge to Industrial Robots Using Reinforcement Learning",
    "abst": "Abstract Nowadays in the context of Industry 4.0, manufacturing companies are faced by increasing global competition and challenges, which requires them to become more flexible and able to adapt fast to rapid market changes. Advanced robot system is an enabler for achieving greater flexibility and adaptability, however, programming such systems also become increasingly more complex. Thus, new methods for programming robot systems and enabling self-learning capabilities to accommodate the natural variation exhibited in real-world tasks are needed. In this paper, we propose a Reinforcement Learning (RL) enabled robot system, which learns task trajectories from human workers. The presented work demonstrates that with minimal human effort, we can transfer manual manipulation tasks in certain domains to a robot system without the requirement for a complicated hardware system model or tedious and complex programming. Furthermore, the robot is able to build upon the learned concepts from the human expert and improve its performance over time. Initially, Q-learning is applied, which has shown very promising results. Preliminary experiments, from a use case in slaughterhouses, demonstrate the viability of the proposed approach. We conclude that the feasibility and applicability of RL for industrial robots and industrial processes, holds and unseen potential, especially for tasks where natural variation is exhibited in either the product or process.",
    "url": "http://www.sciencedirect.com/science/article/pii/S2351978920301372",
    "lang": "en",
    "authors": [
      1253780012,
      2071487764,
      3006190934,
      3006647974
    ],
    "fos": [
      107457646,
      41008148,
      154945302,
      34413123,
      97541855,
      177606310,
      188888258,
      90509273,
      188116033,
      107645828,
      22607594
    ],
    "journals": [
      2898447089
    ],
    "conferences": [],
    "conference_series": [],
    "references": [
      2086113803,
      2145339207,
      2736057540,
      2792243241
    ],
    "filter_matches": [
      "tr",
      "tl",
      "trl",
      "rl"
    ],
    "rank": 22434,
    "citation_count": 0,
    "estimated_citation_count": 0,
    "publication_date": "2019-01-01",
    "found_in": 1,
    "transfer_experiment_type": [
      "t"
    ],
    "transfer_experiment_subtype": [
      "same_all"
    ],
    "transfer_data_type": [
      "I"
    ],
    "transfer_performance_metrics": [],
    "implementation": [
      "Custom",
      "CS",
      "Gym",
      "Figures",
      "Formulas",
      "Tables",
      "Pseudo"
    ],
    "policy_type": [
      "Q"
    ],
    "task_mappings": [],
    "autonomous_transfer": 0,
    "is_deep_rl": 0,
    "tags": [
      "Robotics",
      "Simulation",
      "RealWorld",
      "Imitation",
      "Gazebo"
    ],
    "allowed_learner": [
      "TD"
    ],
    "country": [
      "Spain",
      "Denmark"
    ],
    "uni": [
      "Mondragon University",
      "Aalborg University"
    ],
    "department": [
      "Robotics and Automation"
    ],
    "source_task_selection": [
      "h"
    ],
    "was_in_survey": [
      "mag"
    ],
    "in_title": [
      "t",
      "r",
      "l",
      "k"
    ],
    "in_abs": [
      "t",
      "r",
      "l"
    ],
    "in_contet": [
      "t",
      "r",
      "l",
      "k",
      "s"
    ],
    "rejected_or_unpublished": null,
    "duplication_status": null,
    "paper_for_thesis": 0,
    "paper_available": 1,
    "behind_paywall": 0,
    "paywall_circumventable": 0,
    "author_names": [
      "Dimitrios Chrysostomou",
      "Simon B&#x00F8;gh",
      "N. Arana-Arexolaleiba",
      "N. Urrestilla-Anguiozar"
    ]
  },
  {
    "id": 2892267807,
    "title": "VPE: Variational Policy Embedding for Transfer Reinforcement Learning",
    "abst": "Reinforcement Learning methods are capable of solving complex problems, but resulting policies might perform poorly in environments that are even slightly different. In robotics especially, training and deployment conditions often vary and data collection is expensive, making retraining undesirable. Simulation training allows for feasible training times, but on the other hand suffers from a reality-gap when applied in real-world settings. This raises the need of efficient adaptation of policies acting in new environments. We consider this as a problem of transferring knowledge within a family of similar Markov decision processes. For this purpose we assume that Q-functions are generated by some low-dimensional latent variable. Given such a Q-function, we can find a master policy that can adapt given different values of this latent variable. Our method learns both the generative mapping and an approximate posterior of the latent variables, enabling identification of policies for new tasks by searching only in the latent space, rather than the space of all policies. The low-dimensional space, and master policy found by our method enables policies to quickly adapt to new environments. We demonstrate the method on both a pendulum swing-up task in simulation, and for simulation-to-real transfer on a pushing task.",
    "url": "http://export.arxiv.org/pdf/1809.03548",
    "lang": "en",
    "authors": [
      326313970,
      2096852010,
      2891618226
    ],
    "fos": [
      119857082,
      2778712577,
      51167844,
      41608201,
      133462117,
      33923547,
      97541855,
      34413123,
      106189395,
      154945302,
      105339364
    ],
    "journals": [
      2597173376
    ],
    "conferences": [],
    "conference_series": [],
    "references": [
      1516111018,
      1522301498,
      1903029394,
      1987034518,
      2046376809,
      2047229728,
      2076337359,
      2097381042,
      2121863487,
      2145339207,
      2158782408,
      2161381512,
      2165698076,
      2173248099,
      2207670488,
      2308045930,
      2399306074,
      2509374375,
      2591697182,
      2618170641,
      2623491082,
      2745868649,
      2785342287,
      2795900505,
      2808577099,
      2951567597,
      2951775809,
      2952606116,
      2962899390,
      2963277051,
      2964161785
    ],
    "filter_matches": [
      "tr",
      "tl",
      "trl",
      "rl"
    ],
    "rank": 22455,
    "citation_count": 0,
    "estimated_citation_count": 0,
    "publication_date": "2018-09-10",
    "found_in": 1,
    "transfer_experiment_type": [
      "t"
    ],
    "transfer_experiment_subtype": [
      "sim2real"
    ],
    "transfer_data_type": [
      "Q",
      "pi_gen"
    ],
    "transfer_performance_metrics": [
      "tr"
    ],
    "implementation": [
      "Custom",
      "CS",
      "Gym",
      "Figures",
      "Formulas",
      "Videos"
    ],
    "policy_type": [
      "DQN"
    ],
    "task_mappings": [],
    "autonomous_transfer": 0,
    "is_deep_rl": 1,
    "tags": [
      "Robotics",
      "Simulation",
      "RealWorld",
      "MasterQ",
      "Pendulum",
      "ClassicControl"
    ],
    "allowed_learner": [
      "TD",
      "B"
    ],
    "country": [
      "Sweden"
    ],
    "uni": [
      "Royal Institute of Technology",
      "rebro University"
    ],
    "department": [
      "Robotics, Perception and Learning Lab",
      "Applied Autonomous Sensor Systems"
    ],
    "source_task_selection": [
      "h"
    ],
    "was_in_survey": [
      "mag"
    ],
    "in_title": [
      "t",
      "r",
      "l"
    ],
    "in_abs": [
      "t",
      "r",
      "l",
      "k"
    ],
    "in_contet": [
      "t",
      "r",
      "l",
      "k"
    ],
    "rejected_or_unpublished": null,
    "duplication_status": null,
    "paper_for_thesis": 0,
    "paper_available": 1,
    "behind_paywall": 0,
    "paywall_circumventable": 0,
    "author_names": [
      "Danica Kragic",
      "Johannes A. Stork",
      "Isac Arnekvist"
    ]
  },
  {
    "id": 2794577523,
    "title": "Cache-Enabled Dynamic Rate Allocation via Deep Self-Transfer Reinforcement Learning.",
    "abst": "Caching and rate allocation are two promising approaches to support video streaming over wireless network. However, existing rate allocation designs do not fully exploit the advantages of the two approaches. This paper investigates the problem of cache-enabled QoE-driven video rate allocation problem. We establish a mathematical model for this problem, and point out that it is difficult to solve the problem with traditional dynamic programming. Then we propose a deep reinforcement learning approaches to solve it. First, we model the problem as a Markov decision problem. Then we present a deep Q-learning algorithm with a special knowledge transfer process to find out effective allocation policy. Finally, numerical results are given to demonstrate that the proposed solution can effectively maintain high-quality user experience of mobile user moving among small cells. We also investigate the impact of configuration of critical parameters on the performance of our algorithm.",
    "url": "https://arxiv.org/pdf/1803.11334",
    "lang": null,
    "authors": [
      2100829531,
      2102163052,
      2772648479,
      2795210709,
      2795220113
    ],
    "fos": [
      201025465,
      108037233,
      3020015351,
      37404715,
      41008148,
      115537543,
      97541855,
      2776960227,
      120314980,
      165696696
    ],
    "journals": [
      2597137192
    ],
    "conferences": [],
    "conference_series": [],
    "references": [
      51508254,
      1569909993,
      1682403713,
      1994613437,
      2004690230,
      2017146017,
      2024725057,
      2053157787,
      2098432798,
      2119567691,
      2124994383,
      2145339207,
      2154851929,
      2156133547,
      2166302491,
      2291595385,
      2381074797,
      2400458653,
      2426267443,
      2477392719,
      2508712411,
      2575705757,
      2590526842,
      2611373201,
      2741576192,
      2949608212,
      2950872548,
      2963094133,
      2963864421,
      2964121744
    ],
    "filter_matches": [
      "tr",
      "tl",
      "trl",
      "rl"
    ],
    "rank": 22459,
    "citation_count": 2,
    "estimated_citation_count": 2,
    "publication_date": "2018-03-30",
    "found_in": 1,
    "transfer_experiment_type": [],
    "transfer_experiment_subtype": [
      "same_all"
    ],
    "transfer_data_type": [
      "r",
      "imitation"
    ],
    "transfer_performance_metrics": [
      "tr"
    ],
    "implementation": [
      "Custom",
      "CS",
      "Formulas",
      "Pseudo",
      "Tables",
      "Figures"
    ],
    "policy_type": [
      "DQN",
      "NAF"
    ],
    "task_mappings": [
      "N/A"
    ],
    "autonomous_transfer": 0,
    "is_deep_rl": 1,
    "tags": [
      "Network",
      "Wireless",
      "Simulation",
      "Adaptive",
      "BitRate",
      "Streaming",
      "SelfTransfer",
      "CacheEnabled"
    ],
    "allowed_learner": [
      "TD"
    ],
    "country": [
      "China"
    ],
    "uni": [
      "Southeast University"
    ],
    "department": [
      "Information Science and Engineering"
    ],
    "source_task_selection": [],
    "was_in_survey": [
      "mag"
    ],
    "in_title": [
      "t",
      "r",
      "l"
    ],
    "in_abs": [
      "t",
      "r",
      "l",
      "k"
    ],
    "in_contet": [
      "t",
      "r",
      "l",
      "k",
      "s"
    ],
    "rejected_or_unpublished": 2,
    "duplication_status": null,
    "paper_for_thesis": 0,
    "paper_available": 1,
    "behind_paywall": 1,
    "paywall_circumventable": 1,
    "author_names": [
      "Yongming Huang",
      "Luxi Yang",
      "Meng Hua",
      "Yaru Zheng",
      "Zhengming Zhang"
    ]
  },
  {
    "id": 1585019787,
    "title": "Grounding Hierarchical Reinforcement Learning Models for Knowledge Transfer",
    "abst": "Methods of deep machine learning enable to to reuse low-level representations efficiently for generating more abstract high-level representations. Originally, deep learning has been applied passively (e.g., for classification purposes). Recently, it has been extended to estimate the value of actions for autonomous agents within the framework of reinforcement learning (RL). Explicit models of the environment can be learned to augment such a value function. Although &quot;flat&quot; connectionist methods have already been used for model-based RL, up to now, only model-free variants of RL have been equipped with methods from deep learning. We propose a variant of deep model-based RL that enables an agent to learn arbitrarily abstract hierarchical representations of its environment. In this paper, we present research on how such hierarchical representations can be grounded in sensorimotor interaction between an agent and its environment.",
    "url": "https://128.84.21.199/abs/1412.6451?context=cs.RO",
    "lang": "en",
    "authors": [
      2019155587,
      2111280716
    ],
    "fos": [
      2776960227,
      14646407,
      119857082,
      168993435,
      13687954,
      8521452,
      154945302,
      97541855,
      206588197,
      108583219,
      41008148
    ],
    "journals": [
      2597173376
    ],
    "conferences": [],
    "conference_series": [],
    "references": [
      192712015,
      1595483645,
      1598052524,
      1680797894,
      1757796397,
      2084920657,
      2097364276,
      2105686649,
      2109910161,
      2120472140,
      2143435603,
      2150339816,
      2151210636,
      2163922914,
      2168359464,
      2951774643,
      3011120880,
      3022292044,
      3022318494
    ],
    "filter_matches": [
      "tr",
      "tl",
      "trl",
      "rl"
    ],
    "rank": 22512,
    "citation_count": 2,
    "estimated_citation_count": 2,
    "publication_date": "2014-12-19",
    "found_in": 1,
    "transfer_experiment_type": [
      "s_f",
      "levels"
    ],
    "transfer_experiment_subtype": [
      "multi"
    ],
    "transfer_data_type": [
      "I"
    ],
    "transfer_performance_metrics": [
      "ap",
      "tr"
    ],
    "implementation": [
      "Custom",
      "CS",
      "Formulas",
      "Figures"
    ],
    "policy_type": [
      "Q",
      "QP"
    ],
    "task_mappings": [
      "N/A"
    ],
    "autonomous_transfer": 0,
    "is_deep_rl": 0,
    "tags": [
      "2D",
      "Navigation",
      "Grid",
      "Maze",
      "Simulation"
    ],
    "allowed_learner": [
      "TD",
      "H",
      "MB"
    ],
    "country": [
      "Germany"
    ],
    "uni": [
      "Otto-Friedrich-Universitt Bamberg"
    ],
    "department": [
      "Information Systems and Applied Computer Sciences"
    ],
    "source_task_selection": [
      "h"
    ],
    "was_in_survey": [
      "mag"
    ],
    "in_title": [
      "t",
      "r",
      "l",
      "k"
    ],
    "in_abs": [
      "r",
      "l"
    ],
    "in_contet": [
      "t",
      "r",
      "l",
      "k"
    ],
    "rejected_or_unpublished": null,
    "duplication_status": null,
    "paper_for_thesis": 0,
    "paper_available": 1,
    "behind_paywall": 0,
    "paywall_circumventable": 0,
    "author_names": [
      "Mark Wernsdorfer",
      "Ute Schmid"
    ]
  },
  {
    "id": 126751897,
    "title": "Automated transfer in reinforcement learning",
    "abst": null,
    "url": "https://cris.maastrichtuniversity.nl/portal/en/publications/automated-transfer-in-reinforcement-learning(0a610cee-18d6-4ffe-a803-e6153fc7dc61).html",
    "lang": "en",
    "authors": [
      2215740056
    ],
    "fos": [
      41008148,
      97541855,
      136764020
    ],
    "journals": [],
    "conferences": [],
    "conference_series": [],
    "references": [
      8222043,
      35083779,
      36691172,
      44815768,
      66838807,
      97016928,
      110451278,
      158722652,
      1130790960,
      1483849531,
      1490256991,
      1504915502,
      1506146479,
      1512746852,
      1536990779,
      1580216809,
      1580425441,
      1598052524,
      1598748993,
      1607318605,
      1612195517,
      1626155273,
      1670925118,
      1746819321,
      1777239053,
      1799762961,
      1813659000,
      1822705290,
      1949804828,
      1986014385,
      1995688924,
      2004030284,
      2012392077,
      2027197817,
      2042492924,
      2072128103,
      2073384958,
      2076878942,
      2079247031,
      2084879996,
      2097381042,
      2099768828,
      2106953752,
      2109910161,
      2110292307,
      2116064496,
      2119567691,
      2122922389,
      2126565096,
      2130005627,
      2136719407,
      2137375617,
      2156718197,
      2156869222,
      2157174816,
      2157639439,
      2158150115,
      2163533082,
      2164114810,
      2165698076,
      2165792602,
      2172968643,
      2181744257,
      2206072005,
      2294422333,
      2491253591,
      2579824207,
      2622604310,
      2834852173,
      2951839326
    ],
    "filter_matches": [
      "tr",
      "tl",
      "trl",
      "rl"
    ],
    "rank": 22528,
    "citation_count": 2,
    "estimated_citation_count": 2,
    "publication_date": "2013-01-01",
    "found_in": 1,
    "transfer_experiment_type": [
      "t",
      "a",
      "v"
    ],
    "transfer_experiment_subtype": [
      "diff-no",
      "lit"
    ],
    "transfer_data_type": [
      "Q",
      "I"
    ],
    "transfer_performance_metrics": [
      "tt",
      "tr"
    ],
    "implementation": [
      "Custom",
      "CS",
      "Formulas",
      "Figures",
      "Pseudo",
      "Tables"
    ],
    "policy_type": [
      "LSPI",
      "FQI"
    ],
    "task_mappings": [
      "exp"
    ],
    "autonomous_transfer": 0,
    "is_deep_rl": 0,
    "tags": [
      "2D",
      "Simulation",
      "MountainCar",
      "InvertedPendulum",
      "CartPole",
      "ClassicControl"
    ],
    "allowed_learner": [
      "TD",
      "H",
      "B",
      "MB",
      "batch"
    ],
    "country": [
      "Netherlands",
      "USA"
    ],
    "uni": [
      "Maastricht University",
      "Lafayette College"
    ],
    "department": [
      "Computer Science"
    ],
    "source_task_selection": [
      "h"
    ],
    "was_in_survey": [
      "mag"
    ],
    "in_title": [
      "t",
      "r",
      "l"
    ],
    "in_abs": [
      "t",
      "r",
      "l"
    ],
    "in_contet": [
      "t",
      "r",
      "l",
      "k"
    ],
    "rejected_or_unpublished": null,
    "duplication_status": null,
    "paper_for_thesis": 1,
    "paper_available": 1,
    "behind_paywall": 0,
    "paywall_circumventable": 0,
    "author_names": [
      "Haitham Bou Ammar"
    ]
  },
  {
    "id": 2952460221,
    "title": "Transfer Reinforcement Learning based Framework for Energy Savings in Cellular Base Station Network",
    "abst": "Last few years have witnessed an exponential upsurge in data intensive applications over the communication networks. Energy saving is one of the major aspects in such networks wherein the increased traffic load entails deployment of a large number of base stations (BSs). In this paper, a BS switching scheme is proposed which exploits reinforcement learning (RL) for dynamic sectorization of BSs to increase the energy efficiency of cellular networks. Furthermore, previously estimated traffic statistics is exploited through the process of transfer learning for further improvement in energy savings and speeding up the learning process. The superiority of the proposed framework is depicted through simulations and relevant mathematical analysis. Compared to conventional ON/OFF scheme, proposed framework offers around 40% lower average energy consumption for cellular networks with low to moderate loads.",
    "url": "https://ieeexplore.ieee.org/document/8738418",
    "lang": "en",
    "authors": [
      2292648433,
      2303951183,
      2309477498
    ],
    "fos": [
      97541855,
      68649174,
      150899416,
      105339364,
      153646914,
      2780165032,
      192126672,
      41008148,
      120314980,
      2742236
    ],
    "journals": [],
    "conferences": [],
    "conference_series": [
      2755934421
    ],
    "references": [
      1998058169,
      2087179032,
      2110519532,
      2130827801,
      2136519475,
      2151062451,
      2625184353,
      2626333357,
      2788355865
    ],
    "filter_matches": [
      "tr",
      "tl",
      "trl",
      "rl"
    ],
    "rank": 22536,
    "citation_count": 0,
    "estimated_citation_count": 0,
    "publication_date": "2019-03-09",
    "found_in": 1,
    "transfer_experiment_type": [
      "s_i"
    ],
    "transfer_experiment_subtype": [
      "same_all"
    ],
    "transfer_data_type": [
      "pi"
    ],
    "transfer_performance_metrics": [
      "ap",
      "tr"
    ],
    "implementation": [
      "Custom",
      "CS",
      "Formulas",
      "Figures",
      "Tables"
    ],
    "policy_type": [
      "Actor-Critic"
    ],
    "task_mappings": [
      "N/A"
    ],
    "autonomous_transfer": 0,
    "is_deep_rl": 0,
    "tags": [
      "Energy Saving",
      "Network",
      "WiFi",
      "RealWorld",
      "Simulation"
    ],
    "allowed_learner": [
      "TD"
    ],
    "country": [
      "India"
    ],
    "uni": [
      "Indraprastha Institute of Information Technology"
    ],
    "department": [
      "Electronics and Communication Engineering"
    ],
    "source_task_selection": [
      "h"
    ],
    "was_in_survey": [
      "mag"
    ],
    "in_title": [
      "t",
      "r",
      "l"
    ],
    "in_abs": [
      "r",
      "l"
    ],
    "in_contet": [
      "t",
      "r",
      "l"
    ],
    "rejected_or_unpublished": null,
    "duplication_status": 1,
    "paper_for_thesis": 2604038904,
    "paper_available": 1,
    "behind_paywall": 0,
    "paywall_circumventable": 0,
    "author_names": [
      "Shreyata Sharma",
      "Anand Srivastava",
      "Sumit J. Darak"
    ]
  },
  {
    "id": 3022965940,
    "title": "5G UDN Proactive Handover with Transferable Reinforcement Learning-based Trajectory Prediction",
    "abst": null,
    "url": "https://boris.unibe.ch/134014/",
    "lang": "en",
    "authors": [
      2070779201,
      2111567888,
      2178118525,
      2499637986,
      2617012031,
      2922780639,
      2974106021
    ],
    "fos": [
      111852164,
      41008148,
      97541855,
      154945302,
      13662910
    ],
    "journals": [],
    "conferences": [],
    "conference_series": [],
    "references": [],
    "filter_matches": [
      "tr",
      "tl",
      "trl",
      "rl"
    ],
    "rank": 22541,
    "citation_count": 0,
    "estimated_citation_count": 0,
    "publication_date": "2020-05-09",
    "found_in": 1,
    "transfer_experiment_type": [],
    "transfer_experiment_subtype": [],
    "transfer_data_type": [],
    "transfer_performance_metrics": [],
    "implementation": [
      "Custom",
      "CS"
    ],
    "policy_type": [],
    "task_mappings": [],
    "autonomous_transfer": 2,
    "is_deep_rl": 2,
    "tags": [
      "5G",
      "ICT",
      "Handover"
    ],
    "allowed_learner": [],
    "country": [],
    "uni": [],
    "department": [],
    "source_task_selection": [],
    "was_in_survey": [
      "mag"
    ],
    "in_title": [
      "t",
      "r",
      "l"
    ],
    "in_abs": [],
    "in_contet": [],
    "rejected_or_unpublished": null,
    "duplication_status": null,
    "paper_for_thesis": 0,
    "paper_available": 0,
    "behind_paywall": 1,
    "paywall_circumventable": 0,
    "author_names": [
      "Eduardo Cerqueira",
      "Zhongliang Zhao",
      "Torsten Braun",
      "Hugo Santos",
      "Mostafa Karimzadeh Motallebiazar",
      "Lucas Pacheco",
      "Denis Ros&#x00E1;rio"
    ]
  },
  {
    "id": 2997386021,
    "title": "Bayesian Inverse Reinforcement Learning for Demonstrations of an Expert in Multiple Dynamics: Toward Estimation of Transferable Reward",
    "abst": null,
    "url": "https://www.jstage.jst.go.jp/article/tjsai/35/1/35_G-J73/_pdf",
    "lang": null,
    "authors": [
      2164140351,
      2648723217
    ],
    "fos": [
      154945302,
      2985814175,
      119857082,
      41008148,
      107673813
    ],
    "journals": [
      68749858
    ],
    "conferences": [],
    "conference_series": [],
    "references": [],
    "filter_matches": [
      "tr",
      "tl",
      "trl",
      "rl",
      "rlreward",
      "trreward",
      "tlrreward"
    ],
    "rank": 22578,
    "citation_count": 0,
    "estimated_citation_count": 0,
    "publication_date": "2020-01-01",
    "found_in": 1,
    "transfer_experiment_type": [],
    "transfer_experiment_subtype": [],
    "transfer_data_type": [
      "r"
    ],
    "transfer_performance_metrics": [],
    "implementation": [
      "Custom",
      "CS",
      "Formulas",
      "Pseudo"
    ],
    "policy_type": [
      "Q",
      "BIRL"
    ],
    "task_mappings": [],
    "autonomous_transfer": 0,
    "is_deep_rl": 2,
    "tags": [
      "2D",
      "Navigation",
      "Grid",
      "Simulation"
    ],
    "allowed_learner": [
      "TD"
    ],
    "country": [
      "Japan"
    ],
    "uni": [
      "Chiba University"
    ],
    "department": [
      "Urban Environment Systems"
    ],
    "source_task_selection": [],
    "was_in_survey": [
      "mag"
    ],
    "in_title": [
      "t",
      "r",
      "l"
    ],
    "in_abs": [
      "t",
      "r",
      "l"
    ],
    "in_contet": [],
    "rejected_or_unpublished": null,
    "duplication_status": null,
    "paper_for_thesis": 0,
    "paper_available": 1,
    "behind_paywall": 0,
    "paywall_circumventable": 0,
    "author_names": [
      "Sachiyo Arai",
      "Yusuke Nakata"
    ]
  },
  {
    "id": 2989943922,
    "title": "Helping an Agent Reach a Different Goal by Action Transfer in Reinforcement Learning",
    "abst": "Reinforcement learning agents can be helped by the knowledge transferred from experienced agents. This paper studies the problem of how an experienced agent helps another agent learn when they have different learning goals by action transfer. This problem is motivated by the widely existing situations where agents have different learning goals and only action transfer is available to agents. To tackle the problem, we propose an approach to facilitate the transfer of actions that are right to a learning agent&#x2019;s goal. Experimental results show the effectiveness of the proposed approach in transferring right actions to an agent and helping the agent learn to reach a different goal.",
    "url": "http://doi.org/10.1007/978-3-030-35288-2_2",
    "lang": "en",
    "authors": [
      2127061868,
      2171854186,
      2769697939
    ],
    "fos": [
      2988486947,
      107457646,
      97541855,
      41008148
    ],
    "journals": [],
    "conferences": [],
    "conference_series": [
      2755498524
    ],
    "references": [
      1529399279,
      1557517019,
      1969685488,
      1971890413,
      2031727428,
      2054339854,
      2097381042,
      2119567691,
      2121863487,
      2165792602,
      2169743339,
      2341479450,
      2563829177,
      2620645529,
      2940805057
    ],
    "filter_matches": [
      "tr",
      "tl",
      "trl",
      "rl"
    ],
    "rank": 22601,
    "citation_count": 0,
    "estimated_citation_count": 0,
    "publication_date": "2019-12-02",
    "found_in": 1,
    "transfer_experiment_type": [
      "s_f"
    ],
    "transfer_experiment_subtype": [
      "multi"
    ],
    "transfer_data_type": [
      "advisor"
    ],
    "transfer_performance_metrics": [
      "j",
      "tt"
    ],
    "implementation": [
      "Custom",
      "CS",
      "Formulas",
      "Theorem",
      "Pseudo",
      "Tables"
    ],
    "policy_type": [
      "Q"
    ],
    "task_mappings": [
      "N/A"
    ],
    "autonomous_transfer": 0,
    "is_deep_rl": 0,
    "tags": [
      "2D",
      "Navigation",
      "Grid",
      "Simulation",
      "PolicySimilarity"
    ],
    "allowed_learner": [
      "TD",
      "H",
      "RRL"
    ],
    "country": [
      "Australia"
    ],
    "uni": [
      "University of Wollongong"
    ],
    "department": [
      "Computing and Information Technology"
    ],
    "source_task_selection": [],
    "was_in_survey": [
      "mag"
    ],
    "in_title": [
      "t",
      "r",
      "l"
    ],
    "in_abs": [
      "t",
      "r",
      "l"
    ],
    "in_contet": [
      "t",
      "r",
      "l",
      "k"
    ],
    "rejected_or_unpublished": null,
    "duplication_status": null,
    "paper_for_thesis": 0,
    "paper_available": 1,
    "behind_paywall": 1,
    "paywall_circumventable": 1,
    "author_names": [
      "Minjie Zhang",
      "Fenghui Ren",
      "Yuchen Wang"
    ]
  },
  {
    "id": 2100493795,
    "title": "Online collaborative multi-agent reinforcement learning by transfer of abstract trajectories",
    "abst": "In this paper we propose a method for multi-agent reinforcement learning by automatic discovery of abstract trajectories. Local details are abstracted from successful trajectories and the resulting generalized, abstract trajectories are exchanged between agents. Each agent learns a policy for its own environment. By abstracting trajectories and sharing the result the agents benefit from each others learning. This reduces the overall learning time compared to individual learning.",
    "url": "http://dare.uva.nl/document/2/64097",
    "lang": "en",
    "authors": [
      2020906035,
      2392085974,
      2612782441
    ],
    "fos": [
      19122763,
      97541855,
      154945302,
      77967617,
      188888258,
      47932503,
      119857082,
      199190896,
      41008148,
      2985688074
    ],
    "journals": [],
    "conferences": [],
    "conference_series": [
      1121181248
    ],
    "references": [
      1557517019,
      1570690983,
      1598052524,
      1612195517,
      1923871101,
      2107726111,
      2109910161,
      2118022839,
      2121517924,
      2169743339
    ],
    "filter_matches": [
      "tr",
      "tl",
      "trl",
      "rl"
    ],
    "rank": 22645,
    "citation_count": 0,
    "estimated_citation_count": 0,
    "publication_date": "2008-01-01",
    "found_in": 1,
    "transfer_experiment_type": [
      "#",
      "levels",
      "s_i",
      "s_f"
    ],
    "transfer_experiment_subtype": [
      "multi"
    ],
    "transfer_data_type": [
      "fea",
      "AbstractTrajectories"
    ],
    "transfer_performance_metrics": [
      "tt"
    ],
    "implementation": [
      "Custom",
      "CS",
      "Multiquest",
      "Figures",
      "Pseudo",
      "Tables"
    ],
    "policy_type": [
      "Q"
    ],
    "task_mappings": [
      "N/A"
    ],
    "autonomous_transfer": 0,
    "is_deep_rl": 0,
    "tags": [
      "2D",
      "Navigation",
      "Grid",
      "Maze",
      "Pickup",
      "Subgoals",
      "MultiAgent",
      "Simulation"
    ],
    "allowed_learner": [
      "TD",
      "H"
    ],
    "country": [
      "Netherlands"
    ],
    "uni": [
      "University of Amsterdam"
    ],
    "department": [
      "Computer Science"
    ],
    "source_task_selection": [
      "h"
    ],
    "was_in_survey": [
      "mag"
    ],
    "in_title": [
      "t",
      "r",
      "l"
    ],
    "in_abs": [
      "r",
      "l"
    ],
    "in_contet": [
      "t",
      "r",
      "l",
      "k",
      "s"
    ],
    "rejected_or_unpublished": null,
    "duplication_status": null,
    "paper_for_thesis": 0,
    "paper_available": 1,
    "behind_paywall": 0,
    "paywall_circumventable": 0,
    "author_names": [
      "S. Korzec",
      "M. Pool",
      "M. van Someren"
    ]
  },
  {
    "id": 2956458483,
    "title": "Global Maximum Power Point Tracking of PV Systems under Partial Shading Condition: A Transfer Reinforcement Learning Approach",
    "abst": null,
    "url": "https://www.mdpi.com/2076-3417/9/13/2769/pdf",
    "lang": null,
    "authors": [
      2318951973,
      2717243488,
      2897109182,
      2959759038,
      2960222876,
      2960334519,
      2963780907
    ],
    "fos": [
      127413603,
      133731056,
      97541855,
      41291067,
      177515723,
      36139824
    ],
    "journals": [
      2736532880
    ],
    "conferences": [],
    "conference_series": [],
    "references": [
      32403112,
      437913477,
      605348272,
      1577421539,
      1964880684,
      1973627122,
      1976463393,
      2028230875,
      2050322675,
      2076217933,
      2077651383,
      2090168554,
      2142196876,
      2145691299,
      2166849740,
      2227050001,
      2257979135,
      2328340680,
      2523502388,
      2534860022,
      2547858766,
      2559202864,
      2591427341,
      2614994247,
      2745338687,
      2765185240,
      2801929437,
      2898299211,
      2907870419,
      2910301662,
      2913792596,
      2920890796,
      2921110717
    ],
    "filter_matches": [
      "tr",
      "tl",
      "trl",
      "rl"
    ],
    "rank": 22716,
    "citation_count": 1,
    "estimated_citation_count": 1,
    "publication_date": "2019-07-09",
    "found_in": 1,
    "transfer_experiment_type": [
      "t",
      "s_i"
    ],
    "transfer_experiment_subtype": [
      "same_all"
    ],
    "transfer_data_type": [
      "KM",
      "Q"
    ],
    "transfer_performance_metrics": [
      "j",
      "tr"
    ],
    "implementation": [
      "Custom",
      "CS",
      "Formulas",
      "Figures",
      "Tables"
    ],
    "policy_type": [
      "Q"
    ],
    "task_mappings": [
      "N/A"
    ],
    "autonomous_transfer": 0,
    "is_deep_rl": 0,
    "tags": [
      "PV",
      "Power",
      "Electric",
      "Efficiency",
      "SmartEnergy",
      "MPPT",
      "Simulation"
    ],
    "allowed_learner": [
      "TD"
    ],
    "country": [
      "China"
    ],
    "uni": [
      "Suzhou Power Supply Company",
      "Kunming University of Science and Technology",
      "Shantou University"
    ],
    "department": [
      "Industry",
      "Uni",
      "Colab"
    ],
    "source_task_selection": [],
    "was_in_survey": [
      "mag"
    ],
    "in_title": [
      "t",
      "r",
      "l"
    ],
    "in_abs": [
      "t",
      "r",
      "l",
      "k"
    ],
    "in_contet": [
      "t",
      "r",
      "l",
      "k"
    ],
    "rejected_or_unpublished": null,
    "duplication_status": null,
    "paper_for_thesis": 0,
    "paper_available": 1,
    "behind_paywall": 0,
    "paywall_circumventable": 0,
    "author_names": [
      "Xiaoshun Zhang",
      "Bo Yang",
      "Chen Yang",
      "Dong Lv",
      "Min Ding",
      "Qi Fang",
      "Shi Li"
    ]
  },
  {
    "id": 2997891625,
    "title": "Transfer Reinforcement Learning using Output-Gated Working Memory",
    "abst": null,
    "url": "https://aaai.org/Conferences/AAAI-20/wp-content/uploads/2020/01/AAAI-20-Accepted-Paper-List.pdf#4442",
    "lang": "en",
    "authors": [
      2308109953,
      2942488380
    ],
    "fos": [
      154945302,
      97541855,
      21963081,
      119857082,
      41008148
    ],
    "journals": [],
    "conferences": [
      2969613132
    ],
    "conference_series": [
      1184914352
    ],
    "references": [],
    "filter_matches": [
      "tr",
      "tl",
      "trl",
      "rl"
    ],
    "rank": 22729,
    "citation_count": 0,
    "estimated_citation_count": 0,
    "publication_date": "2020-02-07",
    "found_in": 1,
    "transfer_experiment_type": [
      "s_i",
      "s_f"
    ],
    "transfer_experiment_subtype": [
      "multi"
    ],
    "transfer_data_type": [
      "output-gated-memory"
    ],
    "transfer_performance_metrics": [
      "j",
      "tt"
    ],
    "implementation": [
      "OSS",
      "Github",
      "pandas",
      "Formulas",
      "Figures",
      "Tables"
    ],
    "policy_type": [
      "DQN",
      "HRR"
    ],
    "task_mappings": [
      "N/A"
    ],
    "autonomous_transfer": 0,
    "is_deep_rl": 1,
    "tags": [
      "2D",
      "Navigation",
      "Grid",
      "HRR",
      "Gates",
      "Simulation"
    ],
    "allowed_learner": [
      "TD"
    ],
    "country": [
      "USA"
    ],
    "uni": [
      "Middle Tennessee State University"
    ],
    "department": [
      "Computer Science"
    ],
    "source_task_selection": [
      "h"
    ],
    "was_in_survey": [
      "mag"
    ],
    "in_title": [
      "t",
      "r",
      "l"
    ],
    "in_abs": [
      "t",
      "r",
      "l",
      "k"
    ],
    "in_contet": [
      "t",
      "r",
      "l",
      "k"
    ],
    "rejected_or_unpublished": null,
    "duplication_status": null,
    "paper_for_thesis": 0,
    "paper_available": 1,
    "behind_paywall": 0,
    "paywall_circumventable": 0,
    "author_names": [
      "Joshua L. Phillips",
      "Arthur Williams"
    ]
  },
  {
    "id": 2987047801,
    "title": "A Deep Reinforcement Learning based Approach to Learning Transferable Proof Guidance Strategies.",
    "abst": "Traditional first-order logic (FOL) reasoning systems usually rely on manual heuristics for proof guidance. We propose TRAIL: a system that learns to perform proof guidance using reinforcement learning. A key design principle of our system is that it is general enough to allow transfer to problems in different domains that do not share the same vocabulary of the training set. To do so, we developed a novel representation of the internal state of a prover in terms of clauses and inference actions. We also propose a novel neural-based attention mechanism to learn interactions between clauses. We demonstrate that this approach enables the system to generalize from training to test data across domains with different vocabularies, suggesting that the neural architecture in TRAIL is well suited for representing and processing of logical formalisms. We also show that TRAIL&#039;s learned strategies provide a comparable performance to an established heuristics-based theorem prover.",
    "url": "https://arxiv.org/abs/1911.02065",
    "lang": "en",
    "authors": [
      77696115,
      149771453,
      339801592,
      765536654,
      2153385700,
      2238314490,
      2285488634,
      2724598099,
      2765825549,
      2987650592,
      3005723362
    ],
    "fos": [
      97541855,
      159718280,
      2777601683,
      119857082,
      2776214188,
      41008148,
      171018156,
      16910744,
      206880738,
      127705205,
      154945302
    ],
    "journals": [
      2596500785
    ],
    "conferences": [],
    "conference_series": [],
    "references": [
      41512512,
      995310127,
      1501039513,
      1597128052,
      1789208025,
      1815556503,
      1870229778,
      1902237438,
      1966647497,
      1982239968,
      2019404692,
      2023035194,
      2091776255,
      2105505307,
      2124125910,
      2156849247,
      2257979135,
      2310495003,
      2428112462,
      2460200316,
      2539209300,
      2618097077,
      2770430340,
      2772709170,
      2883268853,
      2890451807,
      2893518191,
      2899482370,
      2916302273,
      2929954177,
      2932237430,
      2945173409,
      2945576559,
      2946187216,
      2947304943,
      2962905782,
      2963147113,
      2963376030,
      2963403868,
      2963576560,
      2963938535,
      2969934262,
      2991316449
    ],
    "filter_matches": [
      "tr",
      "tl",
      "trl",
      "rl"
    ],
    "rank": 22742,
    "citation_count": 1,
    "estimated_citation_count": 1,
    "publication_date": "2019-11-05",
    "found_in": 1,
    "transfer_experiment_type": [
      "s",
      "t"
    ],
    "transfer_experiment_subtype": [
      "lit"
    ],
    "transfer_data_type": [
      "pi"
    ],
    "transfer_performance_metrics": [
      "j",
      "ap"
    ],
    "implementation": [
      "Custom",
      "CS",
      "deepmath",
      "SciKit-Optimize",
      "TPTP",
      "Formulas",
      "Figures",
      "Tables"
    ],
    "policy_type": [
      "TRAIL"
    ],
    "task_mappings": [
      "N/A"
    ],
    "autonomous_transfer": 1,
    "is_deep_rl": 1,
    "tags": [
      "Simulation",
      "FOL",
      "FirstOrderLogic",
      "TheoremProving",
      "Theorem",
      "TRAIL",
      "Proof",
      "Guidance"
    ],
    "allowed_learner": [
      "TD"
    ],
    "country": [
      "USA"
    ],
    "uni": [
      "Northwestern University",
      "University of Illinois",
      "MIT-IBM AI Lab",
      "IBM Research",
      "The University of Auckland"
    ],
    "department": [
      "Uni",
      "Industry",
      "Colab"
    ],
    "source_task_selection": [
      "h"
    ],
    "was_in_survey": [
      "mag"
    ],
    "in_title": [
      "t",
      "r",
      "l"
    ],
    "in_abs": [
      "r",
      "l"
    ],
    "in_contet": [
      "t",
      "r",
      "l",
      "k"
    ],
    "rejected_or_unpublished": 2,
    "duplication_status": null,
    "paper_for_thesis": 0,
    "paper_available": 1,
    "behind_paywall": 0,
    "paywall_circumventable": 0,
    "author_names": [
      "Achille Fokoue",
      "Veronika Thost",
      "Bassem Makni",
      "Pavan Kapanipathi",
      "Cristina Cornelio",
      "Kavitha Srinivas",
      "Ibrahim Abdelaziz",
      "Michael Witbrock",
      "Spencer Whitehead",
      "Edwin Pell",
      "Maxwell Crouse"
    ]
  },
  {
    "id": 232085181,
    "title": "Transferring Models in Hybrid Reinforcement Learning Agents",
    "abst": "The main objective of transfer learning is to reuse knowledge acquired in a previous learned task, in order to enhance the learning procedure in a new and more complex task. Transfer learning comprises a suitable solution for speeding up the learning procedure in Reinforcement Learning tasks. In this work, we propose a novel method for transferring models to a hybrid reinforcement learning agent. The models of the transition and reward functions of a source task, will be transferred to a relevant but different target task. The learning algorithm of the target task&#x2019;s agent takes a hybrid approach, implementing both model-free and model-based learning, in order to fully exploit the presence of a model. The empirical evaluation, of the proposed approach, demonstrated significant results and performance improvements in the 3D Mountain Car task, by successfully using the models generated from the standard 2D Mountain Car.",
    "url": "https://link.springer.com/content/pdf/10.1007%2F978-3-642-23957-1_19.pdf",
    "lang": "en",
    "authors": [
      127847919,
      222830299,
      695239088,
      2012164496
    ],
    "fos": [
      188888258,
      58973888,
      28006648,
      154945302,
      97541855,
      24138899,
      119857082,
      199190896,
      77967617,
      127413603,
      8038995
    ],
    "journals": [],
    "conferences": [],
    "conference_series": [],
    "references": [
      111328409,
      1487385582,
      1506146479,
      1854866626,
      1902261929,
      1993740947,
      2004030284,
      2031727428,
      2095779990,
      2096712730,
      2097381042,
      2103048296,
      2110292307,
      2113913482,
      2133040789,
      2158150115,
      2164114810
    ],
    "filter_matches": [
      "tr",
      "tl",
      "trl",
      "rl"
    ],
    "rank": 22761,
    "citation_count": 1,
    "estimated_citation_count": 1,
    "publication_date": "2011-09-15",
    "found_in": 1,
    "transfer_experiment_type": [
      "v",
      "t"
    ],
    "transfer_experiment_subtype": [
      "diff-it"
    ],
    "transfer_data_type": [
      "model",
      "r"
    ],
    "transfer_performance_metrics": [
      "tt",
      "tr"
    ],
    "implementation": [
      "Custom",
      "CS",
      "Formulas",
      "Pseudo",
      "Figures",
      "Tables",
      "RL-Glue"
    ],
    "policy_type": [
      "TiMRLA",
      "Q",
      "SARSA"
    ],
    "task_mappings": [
      "sup"
    ],
    "autonomous_transfer": 0,
    "is_deep_rl": 0,
    "tags": [
      "2D",
      "3D",
      "2Dto3D",
      "MountainCar",
      "MountainCar3D",
      "Simulation",
      "ClassicControl"
    ],
    "allowed_learner": [
      "TD",
      "MB"
    ],
    "country": [
      "Greece"
    ],
    "uni": [
      "Aristotle University of Thessaloniki"
    ],
    "department": [
      "Informatics"
    ],
    "source_task_selection": [
      "h"
    ],
    "was_in_survey": [
      "mag"
    ],
    "in_title": [
      "t",
      "r",
      "l"
    ],
    "in_abs": [
      "t",
      "r",
      "l",
      "k"
    ],
    "in_contet": [
      "t",
      "r",
      "l",
      "k"
    ],
    "rejected_or_unpublished": null,
    "duplication_status": 6,
    "paper_for_thesis": 3002235425,
    "paper_available": 1,
    "behind_paywall": 0,
    "paywall_circumventable": 0,
    "author_names": [
      "Ioannis Partalas",
      "Anestis Fachantidis",
      "Ioannis Vlahavas",
      "Grigorios Tsoumakas"
    ]
  },
  {
    "id": 197857547,
    "title": "Qualitative Transfer for Reinforcement Learning with Continuous State and Action Spaces",
    "abst": "In this work we present a novel approach to transfer knowledge between reinforcement learning tasks with continuous states and actions, where the transition and policy functions are approximated by Gaussian Processes GPs. The novelty in the proposed approach lies in the idea of transferring qualitative knowledge between tasks, we do so by using the GPs&#039; hyper-parameters used to represent the state transition function in the source task, which represents qualitative knowledge about the type of transition function that the target task might have. We show that the proposed technique constrains the search space, which accelerates the learning process. We performed experiments varying the relevance of transferring the hyper-parameters from the source task into the target task and show, in general, a clear improvement in the overall performance of the system when compared to a state of the art reinforcement learning algorithm for continuous state and action spaces without transfer.",
    "url": "https://rd.springer.com/chapter/10.1007/978-3-642-41822-8_25",
    "lang": "en",
    "authors": [
      2102534334,
      2151553307,
      2206439997
    ],
    "fos": [
      2778738651,
      61326573,
      8038995,
      28006648,
      150899416,
      154945302,
      199190896,
      41008148,
      60229501,
      97541855,
      119857082
    ],
    "journals": [],
    "conferences": [
      74330182
    ],
    "conference_series": [
      1120869304
    ],
    "references": [
      34092776,
      109727711,
      755046805,
      1480877541,
      1515851193,
      1520597402,
      1746819321,
      1986935961,
      1994005439,
      2004030284,
      2097381042,
      2109102709,
      2110292307,
      2123246446,
      2126025075,
      2128786740,
      2132849848,
      2140135625,
      2151268438,
      2156974606
    ],
    "filter_matches": [
      "tr",
      "tl",
      "trl",
      "rl"
    ],
    "rank": 22762,
    "citation_count": 2,
    "estimated_citation_count": 2,
    "publication_date": "2013-11-20",
    "found_in": 1,
    "transfer_experiment_type": [
      "t"
    ],
    "transfer_experiment_subtype": [
      "same_all"
    ],
    "transfer_data_type": [
      "model",
      "pi"
    ],
    "transfer_performance_metrics": [
      "tt"
    ],
    "implementation": [
      "Custom",
      "CS",
      "Formulas",
      "Pseudo",
      "Figures"
    ],
    "policy_type": [
      "PILCO",
      "QTL-PILCO"
    ],
    "task_mappings": [
      "N/A"
    ],
    "autonomous_transfer": 0,
    "is_deep_rl": 0,
    "tags": [
      "InvertedPendulum",
      "Simulation",
      "GuassianProcess",
      "ClassicControl"
    ],
    "allowed_learner": [
      "TD",
      "PS",
      "MB"
    ],
    "country": [
      "Mexico"
    ],
    "uni": [
      "Instituto Nacional de Astrfisica"
    ],
    "department": [
      "Optics and Electronics"
    ],
    "source_task_selection": [],
    "was_in_survey": [
      "mag"
    ],
    "in_title": [
      "t",
      "r",
      "l"
    ],
    "in_abs": [
      "t",
      "r",
      "l",
      "k"
    ],
    "in_contet": [
      "t",
      "r",
      "l",
      "k"
    ],
    "rejected_or_unpublished": null,
    "duplication_status": null,
    "paper_for_thesis": 0,
    "paper_available": 1,
    "behind_paywall": 0,
    "paywall_circumventable": 0,
    "author_names": [
      "Esteban O. Garcia",
      "Eduardo F. Morales",
      "Enrique Munoz de Cote"
    ]
  },
  {
    "id": 2949344914,
    "title": "Injecting Prior Knowledge for Transfer Learning into Reinforcement Learning Algorithms using Logic Tensor Networks.",
    "abst": "Human ability at solving complex tasks is helped by priors on object and event semantics of their environment. This paper investigates the use of similar prior knowledge for transfer learning in Reinforcement Learning agents. In particular, the paper proposes to use a first-order-logic language grounded in deep neural networks to represent facts about objects and their semantics in the real world. Facts are provided as background knowledge a priori to learning a policy for how to act in the world. The priors are injected with the conventional input in a single agent architecture. As proof-of-concept, the paper tests the system in simple experiments that show the importance of symbolic abstraction and flexible fact derivation. The paper shows that the proposed system can learn to take advantage of both the symbolic layer and the image layer in a single decision selection module.",
    "url": "http://arxiv.org/pdf/1906.06576.pdf",
    "lang": "en",
    "authors": [
      2951972211,
      2952725969
    ],
    "fos": [
      150899416,
      124304363,
      97541855,
      75553542,
      154945302,
      119857082,
      33923547,
      177769412,
      123657996,
      2776095079,
      124246873
    ],
    "journals": [
      2597173376
    ],
    "conferences": [],
    "conference_series": [],
    "references": [
      2000514530,
      2145339207,
      2173248099,
      2342662072,
      2521274174,
      2614839826,
      2619531163,
      2761873684,
      2766447205,
      2785693718,
      2798338638,
      2951799221,
      2952523895,
      2962791438,
      2964043796
    ],
    "filter_matches": [
      "tr",
      "tl",
      "trl",
      "rl"
    ],
    "rank": 22816,
    "citation_count": 2,
    "estimated_citation_count": 2,
    "publication_date": "2019-06-15",
    "found_in": 1,
    "transfer_experiment_type": [
      "s_i",
      "s_f",
      "#"
    ],
    "transfer_experiment_subtype": [
      "multi"
    ],
    "transfer_data_type": [
      "pri"
    ],
    "transfer_performance_metrics": [
      "j",
      "tt"
    ],
    "implementation": [
      "Custom",
      "CS",
      "Formulas",
      "Figures"
    ],
    "policy_type": [
      "DDQN",
      "DQN",
      "LTS"
    ],
    "task_mappings": [
      "N/A"
    ],
    "autonomous_transfer": 0,
    "is_deep_rl": 1,
    "tags": [
      "2D",
      "Simulation",
      "Navigation",
      "Grid"
    ],
    "allowed_learner": [
      "TD"
    ],
    "country": [
      "Belgium",
      "USA"
    ],
    "uni": [
      "Universit Libre de Bruxelles",
      "Sony Computer Science Laboratories"
    ],
    "department": [
      "Industry",
      "Uni",
      "Colab"
    ],
    "source_task_selection": [
      "all"
    ],
    "was_in_survey": [
      "mag"
    ],
    "in_title": [
      "t",
      "r",
      "l",
      "k"
    ],
    "in_abs": [
      "t",
      "r",
      "l",
      "k"
    ],
    "in_contet": [
      "t",
      "r",
      "l",
      "k"
    ],
    "rejected_or_unpublished": 2,
    "duplication_status": null,
    "paper_for_thesis": 0,
    "paper_available": 1,
    "behind_paywall": 0,
    "paywall_circumventable": 0,
    "author_names": [
      "Samy Badreddine",
      "Michael Spranger"
    ]
  },
  {
    "id": 2990376820,
    "title": "Self-Attentional Credit Assignment for Transfer in Reinforcement Learning",
    "abst": "The ability to transfer knowledge to novel environments and tasks is a sensible desiderata for general learning agents. Despite the apparent promises, transfer in RL is still an open and little exploited research area. In this paper, we take a brand-new perspective about transfer: we suggest that the ability to assign credit unveils structural invariants in the tasks that can be transferred to make RL more sample-efficient. Our main contribution is SECRET, a novel approach to transfer learning for RL that uses a backward-view credit assignment mechanism based on a self-attentive architecture. Two aspects are key to its generality: it learns to assign credit as a separate offline supervised process and exclusively modifies the reward function. Consequently, it can be supplemented by transfer methods that do not modify the reward function and it can be plugged on top of any RL algorithm.",
    "url": "https://openreview.net/pdf?id=B1xybgSKwB",
    "lang": null,
    "authors": [
      175821849,
      2792877023,
      2895242216,
      2961707676
    ],
    "fos": [
      2988340862,
      154945302,
      33923547,
      49209780,
      2780767217,
      119857082,
      150899416,
      97541855,
      3019450906,
      123657996
    ],
    "journals": [
      2597173376
    ],
    "conferences": [],
    "conference_series": [],
    "references": [
      81137444,
      1777239053,
      2095705004,
      2097381042,
      2130942839,
      2141559645,
      2145339207,
      2334782222,
      2426267443,
      2550182557,
      2560647685,
      2584377191,
      2604763608,
      2736601468,
      2787501667,
      2789517807,
      2790924949,
      2897023205,
      2913350117,
      2914261249,
      2951032747,
      2962739339,
      2962858248,
      2963026768,
      2963085895,
      2963088995,
      2963184621,
      2963199420,
      2963341956,
      2963403868,
      2964059481,
      2964097858,
      2964185768,
      2964227899,
      2964308564,
      2970705602
    ],
    "filter_matches": [
      "tr",
      "tl",
      "trl",
      "rl"
    ],
    "rank": 22829,
    "citation_count": 0,
    "estimated_citation_count": 0,
    "publication_date": "2019-09-25",
    "found_in": 1,
    "transfer_experiment_type": [
      "v",
      "s_i",
      "s_f",
      "t",
      "r"
    ],
    "transfer_experiment_subtype": [
      "multi"
    ],
    "transfer_data_type": [
      "r",
      "credit"
    ],
    "transfer_performance_metrics": [
      "tt"
    ],
    "implementation": [
      "Custom",
      "CS",
      "Formulas",
      "Figures"
    ],
    "policy_type": [
      "Q",
      "PPO",
      "DQN"
    ],
    "task_mappings": [
      "N/A"
    ],
    "autonomous_transfer": 0,
    "is_deep_rl": 1,
    "tags": [
      "2D",
      "Navigation",
      "Grid",
      "DMLab",
      "Credit",
      "Simulation"
    ],
    "allowed_learner": [
      "TD"
    ],
    "country": [
      "USA"
    ],
    "uni": [
      "Google Research"
    ],
    "department": [
      "Industry"
    ],
    "source_task_selection": [],
    "was_in_survey": [
      "mag"
    ],
    "in_title": [
      "t",
      "r",
      "l"
    ],
    "in_abs": [
      "t",
      "r",
      "l",
      "k"
    ],
    "in_contet": [
      "t",
      "r",
      "l",
      "k",
      "s"
    ],
    "rejected_or_unpublished": 1,
    "duplication_status": null,
    "paper_for_thesis": 0,
    "paper_available": 1,
    "behind_paywall": 0,
    "paywall_circumventable": 0,
    "author_names": [
      "Olivier Pietquin",
      "Matthieu Geist",
      "Rapha&#x00EB;l Marinier",
      "Johan Ferret"
    ]
  },
  {
    "id": 3008492644,
    "title": "Real&#x2013;Sim&#x2013;Real Transfer for Real-World Robot Control Policy Learning with Deep Reinforcement Learning",
    "abst": "Compared to traditional data-driven learning methods, recently developed deep reinforcement learning (DRL) approaches can be employed to train robot agents to obtain control policies with appealing performance. However, learning control policies for real-world robots through DRL is costly and cumbersome. A promising alternative is to train policies in simulated environments and transfer the learned policies to real-world scenarios. Unfortunately, due to the reality gap between simulated and real-world environments, the policies learned in simulated environments often cannot be generalized well to the real world. Bridging the reality gap is still a challenging problem. In this paper, we propose a novel real&#x2013;sim&#x2013;real (RSR) transfer method that includes a real-to-sim training phase and a sim-to-real inference phase. In the real-to-sim training phase, a task-relevant simulated environment is constructed based on semantic information of the real-world scenario and coordinate transformation, and then a policy is trained with the DRL method in the built simulated environment. In the sim-to-real inference phase, the learned policy is directly applied to control the robot in real-world scenarios without any real-world data. Experimental results in two different robot control tasks show that the proposed RSR method can train skill policies with high generalization performance and significantly low training costs.",
    "url": "https://www.mdpi.com/2076-3417/10/5/1555/pdf",
    "lang": null,
    "authors": [
      2590515920,
      2597953933,
      2675416094,
      2721503857,
      2781873003
    ],
    "fos": [
      2776214188,
      127413603,
      80551277,
      2779436431,
      174348530,
      90509273,
      65401140,
      97541855,
      2993861310,
      133731056,
      154945302
    ],
    "journals": [
      2736532880
    ],
    "conferences": [],
    "conference_series": [],
    "references": [
      1977655452,
      2060914855,
      2064355448,
      2088219864,
      2124267516,
      2145339207,
      2154543878,
      2157072005,
      2395611524,
      2745868649,
      2766447205,
      2962736495,
      2963184124,
      2964161785
    ],
    "filter_matches": [
      "tr",
      "tl",
      "trl",
      "rl"
    ],
    "rank": 22851,
    "citation_count": 0,
    "estimated_citation_count": 0,
    "publication_date": "2020-02-25",
    "found_in": 1,
    "transfer_experiment_type": [
      "t",
      "s_i",
      "s_f"
    ],
    "transfer_experiment_subtype": [
      "multi"
    ],
    "transfer_data_type": [
      "RSR",
      "model"
    ],
    "transfer_performance_metrics": [
      "tt"
    ],
    "implementation": [
      "Custom",
      "CS",
      "Gazebo",
      "TensorFlow",
      "Gym",
      "Pseudo",
      "Formulas",
      "Figures",
      "Tables"
    ],
    "policy_type": [
      "PPO"
    ],
    "task_mappings": [
      "N/A"
    ],
    "autonomous_transfer": 0,
    "is_deep_rl": 1,
    "tags": [
      "Robotics",
      "Simulation",
      "RealWorld",
      "Gazebo",
      "Manipulation",
      "Navigation",
      "3D"
    ],
    "allowed_learner": [
      "TD"
    ],
    "country": [
      "China"
    ],
    "uni": [
      "Chinese Academy of Sciences",
      "State Key Laboratory of Management and Control for Complex Systems"
    ],
    "department": [
      "Institute of Automation"
    ],
    "source_task_selection": [
      "h"
    ],
    "was_in_survey": [
      "mag"
    ],
    "in_title": [
      "t",
      "r",
      "l"
    ],
    "in_abs": [
      "t",
      "r",
      "l",
      "s"
    ],
    "in_contet": [
      "t",
      "r",
      "l",
      "s"
    ],
    "rejected_or_unpublished": null,
    "duplication_status": null,
    "paper_for_thesis": 0,
    "paper_available": 1,
    "behind_paywall": 0,
    "paywall_circumventable": 0,
    "author_names": [
      "Shuo Wang",
      "Tao Lu",
      "Rui Wang",
      "Yinghao Cai",
      "Naijun Liu"
    ]
  },
  {
    "id": 1803475911,
    "title": "Hierarchical Functional Concepts for Knowledge Transfer among Reinforcement Learning Agents",
    "abst": "This article introduces the notions of functional space and concept as a way of knowledge representation and abstraction for Reinforcement Learning agents. These definitions are used as a tool of knowledge transfer among agents. The agents are assumed to be heterogeneous; they have different state spaces but share a same dynamic, reward and action space. In other words, the agents are assumed to have different representations of an environment while having similar actions. The learning framework is $Q$-learning. Each dimension of the functional space is the normalized expected value of an action. An unsupervisedclustering approach is used to form the functional concepts as some fuzzy areas in the functional space. The functional concepts are abstracted further in a hierarchy using the clustering approach. The hierarchical concepts are employed for knowledge transfer among agents. Properties of the proposed approach are tested in a set of case studies. The results show that the approach is very effective in transfer learning among heterogeneous agents especially in the beginning episodes of the learning.",
    "url": "http://ijfs.usb.ac.ir/article_2113.html",
    "lang": "en",
    "authors": [
      442130,
      2034701141,
      2253922153,
      2330815663,
      2638190519
    ],
    "fos": [
      2776960227,
      97541855,
      33923547,
      31170391,
      124304363,
      73555534,
      150899416,
      154945302,
      58166,
      161301231
    ],
    "journals": [
      30613177
    ],
    "conferences": [],
    "conference_series": [],
    "references": [
      115717799,
      1572778877,
      1898259180,
      2072493968,
      2079247031,
      2097381042,
      2107298017,
      2120835067,
      2121517924,
      2121863487,
      2134845968,
      2136027818,
      2138497321,
      2145983895,
      2152444902,
      2158150115,
      2169743339,
      2808217720,
      3011120880
    ],
    "filter_matches": [
      "tr",
      "tl",
      "trl",
      "rl"
    ],
    "rank": 22880,
    "citation_count": 0,
    "estimated_citation_count": 0,
    "publication_date": "2015-10-30",
    "found_in": 1,
    "transfer_experiment_type": [
      "s_i",
      "s_f",
      "levels"
    ],
    "transfer_experiment_subtype": [
      "multi"
    ],
    "transfer_data_type": [
      "Q",
      "fea"
    ],
    "transfer_performance_metrics": [
      "j",
      "tt"
    ],
    "implementation": [
      "Custom",
      "CS",
      "Formulas",
      "Figures",
      "Tables"
    ],
    "policy_type": [
      "Q"
    ],
    "task_mappings": [
      "N/A"
    ],
    "autonomous_transfer": 0,
    "is_deep_rl": 0,
    "tags": [
      "2D",
      "Navigation",
      "Grid",
      "Taxi",
      "Simulation"
    ],
    "allowed_learner": [
      "TD",
      "H"
    ],
    "country": [
      "Iran"
    ],
    "uni": [
      "University of Tehran"
    ],
    "department": [
      "Electrical and Computer Engineering"
    ],
    "source_task_selection": [
      "h"
    ],
    "was_in_survey": [
      "mag"
    ],
    "in_title": [
      "t",
      "r",
      "l",
      "k"
    ],
    "in_abs": [
      "t",
      "r",
      "l",
      "k"
    ],
    "in_contet": [
      "t",
      "r",
      "l",
      "k"
    ],
    "rejected_or_unpublished": null,
    "duplication_status": null,
    "paper_for_thesis": 0,
    "paper_available": 1,
    "behind_paywall": 0,
    "paywall_circumventable": 0,
    "author_names": [
      "Babak Nadjar Araabi",
      "M. Nili Ahmadabadi",
      "N. Zaare",
      "Amin Mousavi",
      "H. Vosoughpour"
    ]
  },
  {
    "id": 2980248684,
    "title": "Knowledge Transfer in Reinforcement Learning Agent",
    "abst": "This manuscript is focused on transfer learning methods for reinforcement learning agents. An preview of contemporary papers in area of transfer Leaning and Knowledge transfer. We provided the background and overview of knowledge transfer methods with an emphasis on the topics of reinforcement learning.",
    "url": "http://xplorestaging.ieee.org/ielx7/8850849/8860868/08860881.pdf?arnumber=8860881",
    "lang": null,
    "authors": [
      1505360330,
      2116011919
    ],
    "fos": [
      97541855,
      2776960227,
      41008148,
      150899416,
      107457646
    ],
    "journals": [],
    "conferences": [],
    "conference_series": [],
    "references": [
      158722652,
      158822840,
      1529399279,
      1606056663,
      1663973292,
      1777239053,
      1972108682,
      2004030284,
      2119567691,
      2129427976,
      2138671676,
      2900896126,
      2914656440
    ],
    "filter_matches": [
      "tr",
      "tl",
      "trl",
      "rl"
    ],
    "rank": 22881,
    "citation_count": 0,
    "estimated_citation_count": 0,
    "publication_date": "2019-09-01",
    "found_in": 1,
    "transfer_experiment_type": [
      "s_i",
      "s_f"
    ],
    "transfer_experiment_subtype": [
      "multi"
    ],
    "transfer_data_type": [
      "Q",
      "pi"
    ],
    "transfer_performance_metrics": [
      "j"
    ],
    "implementation": [
      "Custom",
      "CS",
      "Formulas",
      "Figures"
    ],
    "policy_type": [
      "Q"
    ],
    "task_mappings": [
      "N/A"
    ],
    "autonomous_transfer": 0,
    "is_deep_rl": 0,
    "tags": [
      "2D",
      "Navigation",
      "Simulation",
      "Grid"
    ],
    "allowed_learner": [
      "TD",
      "CBR"
    ],
    "country": [
      "Bulgaria"
    ],
    "uni": [
      "Bulgarian Academy of Sciences"
    ],
    "department": [
      "Institute of Robotics"
    ],
    "source_task_selection": [
      "h"
    ],
    "was_in_survey": [
      "mag"
    ],
    "in_title": [
      "t",
      "r",
      "l",
      "k"
    ],
    "in_abs": [
      "t",
      "r",
      "l",
      "k"
    ],
    "in_contet": [
      "t",
      "r",
      "l",
      "k"
    ],
    "rejected_or_unpublished": null,
    "duplication_status": null,
    "paper_for_thesis": 0,
    "paper_available": 1,
    "behind_paywall": 0,
    "paywall_circumventable": 0,
    "author_names": [
      "Ventseslav Shopov",
      "Vanya Markova"
    ]
  },
  {
    "id": 3026075272,
    "title": "Transferable Cost-Aware Security Policy Implementation for Malware Detection Using Deep Reinforcement Learning.",
    "abst": "Malware detection is an ever-present challenge for all organizational gatekeepers, who must maintain high detection rates while minimizing interruptions to the organization&#039;s workflow. To improve detection rates, organizations often deploy an ensemble of detectors. While effective, this approach is computationally expensive, since every file - even clear-cut cases - needs to be analyzed by all detectors. Moreover, with an ever-increasing number of files to process, the use of ensembles may incur unacceptable processing times and costs (e.g., cloud resources). In this study, we propose SPIREL, a reinforcement learning-based method for cost-effective malware detection. Our method enables organizations to directly associate costs to correct/incorrect classification, computing resources and run-time, and then dynamically establishes a security policy. This security policy is then implemented, and for each inspected file, a different set of detectors is assigned and a different detection threshold is set. Our evaluation on two malware domains- Portable Executable (PE) and Android Application Package (APK)files - shows that SPIREL is both accurate and extremely resource-efficient: the proposed method either outperforms the best performing baselines while achieving a modest improvement in efficiency, or reduces the required running time by ~80% while decreasing the accuracy and F1-score by only 0.5%. We also show that our approach is both highly transferable across different datasets and adaptable to changes in individual detector performance.",
    "url": null,
    "lang": null,
    "authors": [
      1514115001,
      2132343420,
      2944981007,
      2946705160
    ],
    "fos": [
      109364899,
      196156399,
      541664917,
      3019361169,
      97541855,
      177212765,
      94915269,
      154908896,
      41008148,
      38652104,
      120314980
    ],
    "journals": [
      2595770078
    ],
    "conferences": [],
    "conference_series": [],
    "references": [
      158722652,
      200681053,
      1482612322,
      1583484179,
      1771410628,
      1851403712,
      1987375341,
      1999728176,
      2016433328,
      2021436318,
      2024170198,
      2042742130,
      2085301524,
      2097746659,
      2119954997,
      2120705432,
      2141559645,
      2148542813,
      2162034246,
      2164463255,
      2174786457,
      2253429366,
      2253728219,
      2291014553,
      2292977173,
      2395579298,
      2476301537,
      2556958149,
      2557513839,
      2575164337,
      2591102410,
      2758375579,
      2766447205,
      2772660489,
      2773028286,
      2774354230,
      2778749116,
      2782999659,
      2783348159,
      2806702723,
      2808927717,
      2889825749,
      2899776223,
      2904816386,
      2963252583,
      2977340257,
      2988335058
    ],
    "filter_matches": [
      "tr",
      "tl",
      "trl",
      "rl"
    ],
    "rank": 22894,
    "citation_count": 0,
    "estimated_citation_count": 0,
    "publication_date": "2020-05-20",
    "found_in": 1,
    "transfer_experiment_type": [
      "t"
    ],
    "transfer_experiment_subtype": [
      "same_all"
    ],
    "transfer_data_type": [
      "WT"
    ],
    "transfer_performance_metrics": [
      "j",
      "tr",
      "tt"
    ],
    "implementation": [
      "Custom",
      "CS",
      "Gym",
      "TensorFlow",
      "Keras",
      "ChainerRL",
      "Figures",
      "Formulas",
      "Tables"
    ],
    "policy_type": [
      "ACER"
    ],
    "task_mappings": [],
    "autonomous_transfer": 0,
    "is_deep_rl": 1,
    "tags": [
      "Malware",
      "Detection",
      "Simulation"
    ],
    "allowed_learner": [
      "TD"
    ],
    "country": [
      "Israel"
    ],
    "uni": [
      "Ben-Gurion University of the Negev"
    ],
    "department": [
      "Software and Information Systems Engineering"
    ],
    "source_task_selection": [
      "h"
    ],
    "was_in_survey": [
      "mag"
    ],
    "in_title": [
      "t",
      "r",
      "l"
    ],
    "in_abs": [
      "t",
      "r",
      "l"
    ],
    "in_contet": [
      "t",
      "r",
      "l",
      "k"
    ],
    "rejected_or_unpublished": 2,
    "duplication_status": null,
    "paper_for_thesis": 0,
    "paper_available": 1,
    "behind_paywall": 0,
    "paywall_circumventable": 0,
    "author_names": [
      "Asaf Shabtai",
      "Gilad Katz",
      "Shaked Hindi",
      "Yoni Birman"
    ]
  },
  {
    "id": 2891191051,
    "title": "Transferring Deep Reinforcement Learning with Adversarial Objective and Augmentation",
    "abst": "In the past few years, deep reinforcement learning has been proven to solve problems which have complex states like video games or board games. The next step of intelligent agents would be able to generalize between tasks, and using prior experience to pick up new skills more quickly. However, most reinforcement learning algorithms for now are often suffering from catastrophic forgetting even when facing a very similar target task. Our approach enables the agents to generalize knowledge from a single source task, and boost the learning progress with a semisupervised learning method when facing a new task. We evaluate this approach on Atari games, which is a popular reinforcement learning benchmark, and show that it outperforms common baselines based on pre-training and fine-tuning.",
    "url": "https://ui.adsabs.harvard.edu/abs/2018arXiv180900770H/abstract",
    "lang": "en",
    "authors": [
      2135800016,
      2139482179,
      2889836378
    ],
    "fos": [
      119857082,
      74072328,
      3020055732,
      37736160,
      7149132,
      2994503672,
      154945302,
      33923547,
      97541855
    ],
    "journals": [
      2597173376
    ],
    "conferences": [],
    "conference_series": [],
    "references": [],
    "filter_matches": [
      "tr",
      "tl",
      "trl",
      "rl"
    ],
    "rank": 22957,
    "citation_count": 0,
    "estimated_citation_count": 0,
    "publication_date": "2018-09-04",
    "found_in": 1,
    "transfer_experiment_type": [
      "#",
      "v",
      "t"
    ],
    "transfer_experiment_subtype": [
      "lit"
    ],
    "transfer_data_type": [
      "GAN"
    ],
    "transfer_performance_metrics": [
      "tr",
      "tt"
    ],
    "implementation": [
      "Custom",
      "CS",
      "Gym",
      "ALE",
      "Formulas",
      "Figures",
      "Tables"
    ],
    "policy_type": [
      "DQN"
    ],
    "task_mappings": [
      "N/A"
    ],
    "autonomous_transfer": 0,
    "is_deep_rl": 1,
    "tags": [
      "2D",
      "Games",
      "VideoGames",
      "Atari",
      "Pong",
      "Simulation"
    ],
    "allowed_learner": [
      "TD"
    ],
    "country": [
      "Taiwan"
    ],
    "uni": [
      "National Taiwan University"
    ],
    "department": [
      "Computer Science and Information Engineering"
    ],
    "source_task_selection": [
      "h"
    ],
    "was_in_survey": [
      "mag"
    ],
    "in_title": [
      "t",
      "r",
      "l"
    ],
    "in_abs": [
      "r",
      "l",
      "k",
      "s"
    ],
    "in_contet": [
      "t",
      "r",
      "l",
      "k"
    ],
    "rejected_or_unpublished": 2,
    "duplication_status": null,
    "paper_for_thesis": 0,
    "paper_available": 1,
    "behind_paywall": 0,
    "paywall_circumventable": 0,
    "author_names": [
      "I-Chao Shen",
      "Bing-Yu Chen",
      "Shu-Hsuan Hsu"
    ]
  },
  {
    "id": 2167778886,
    "title": "Transferring experience in reinforcement learning through task decomposition",
    "abst": "Transfer learning refers to the process of conveying experience from a simple task to another more complex (and related) task in order to reduce the amount of time that is required to learn the latter task. Typically, in a transfer learning procedure the agent learns a behavior in a source task, and it uses the gained knowledge in order to speed up the learning process in a target task. Reinforcement Learning algorithms are time expensive when they learn from scratch, especially in complex domains, and transfer learning comprises a suitable solution to speed up the training process. In this work we propose a method that decomposes the target task in several instances of the source task and uses them to extract an advised action for the target task. We evaluate the efficacy of the proposed approach in the robotic soccer Keepaway domain. The results demonstrate that the proposed method helps to reduce the training time of the target task.",
    "url": "https://dl.acm.org/citation.cfm?id=1558109.1558208",
    "lang": "en",
    "authors": [
      127847919,
      695239088,
      2012164496,
      2410251532
    ],
    "fos": [
      28006648,
      199190896,
      97541855,
      175154964,
      77075516,
      150899416,
      119857082,
      68339613,
      8038995,
      41008148,
      154945302
    ],
    "journals": [],
    "conferences": [
      2785448048
    ],
    "conference_series": [
      1168671587
    ],
    "references": [
      36691172,
      1506146479,
      1517018472,
      1799762961,
      1993277309,
      2012036715,
      2014512216,
      2031727428,
      2068362282,
      2113913482,
      2121863487,
      2133040789,
      2158150115,
      2164114810,
      2169659168,
      2334782222
    ],
    "filter_matches": [
      "tr",
      "tl",
      "trl",
      "rl"
    ],
    "rank": 23054,
    "citation_count": 0,
    "estimated_citation_count": 0,
    "publication_date": "2009-05-10",
    "found_in": 1,
    "transfer_experiment_type": [
      "#"
    ],
    "transfer_experiment_subtype": [
      "same_all"
    ],
    "transfer_data_type": [
      "rule",
      "advisor"
    ],
    "transfer_performance_metrics": [
      "tt"
    ],
    "implementation": [
      "Custom",
      "CS",
      "Formulas",
      "Figures",
      "Pseudo",
      "Tables"
    ],
    "policy_type": [
      "Q"
    ],
    "task_mappings": [
      "sup"
    ],
    "autonomous_transfer": 0,
    "is_deep_rl": 0,
    "tags": [
      "2D",
      "RoboCup",
      "KeepAway",
      "Simulation",
      "MultiAgent",
      "4v3",
      "3v2",
      "5v4",
      "6v5"
    ],
    "allowed_learner": [
      "TD"
    ],
    "country": [
      "Greece"
    ],
    "uni": [
      "Aristotle University of Thessaloniki"
    ],
    "department": [
      "Informatics"
    ],
    "source_task_selection": [
      "h"
    ],
    "was_in_survey": [
      "mag"
    ],
    "in_title": [
      "t",
      "r",
      "l"
    ],
    "in_abs": [
      "t",
      "r",
      "l",
      "k"
    ],
    "in_contet": [
      "t",
      "r",
      "l",
      "k"
    ],
    "rejected_or_unpublished": null,
    "duplication_status": null,
    "paper_for_thesis": 0,
    "paper_available": 1,
    "behind_paywall": 0,
    "paywall_circumventable": 0,
    "author_names": [
      "Ioannis Partalas",
      "Ioannis Vlahavas",
      "Grigorios Tsoumakas",
      "Konstantinos Tzevanidis"
    ]
  },
  {
    "id": 3022749764,
    "title": "Enhancing Transferability of Deep Reinforcement Learning-Based Variable Speed Limitendgraf Control Using Transfer Learning",
    "abst": null,
    "url": "http://xplorestaging.ieee.org/ielx7/6979/4358928/09090297.pdf?arnumber=9090297",
    "lang": null,
    "authors": [
      2151346274,
      2713251555,
      3021667528,
      3022728405
    ],
    "fos": [
      61272859,
      2780210587,
      44154836,
      127413603,
      97541855,
      150899416
    ],
    "journals": [
      144771191
    ],
    "conferences": [],
    "conference_series": [],
    "references": [],
    "filter_matches": [
      "tr",
      "tl",
      "trl",
      "rl"
    ],
    "rank": 23076,
    "citation_count": 0,
    "estimated_citation_count": 0,
    "publication_date": "2020-05-08",
    "found_in": 1,
    "transfer_experiment_type": [
      "t"
    ],
    "transfer_experiment_subtype": [
      "same_all"
    ],
    "transfer_data_type": [
      "nn_weight_links"
    ],
    "transfer_performance_metrics": [
      "tt"
    ],
    "implementation": [
      "Custom",
      "CS",
      "Formulas"
    ],
    "policy_type": [
      "VSL",
      "DDQN"
    ],
    "task_mappings": [
      "N/A"
    ],
    "autonomous_transfer": 0,
    "is_deep_rl": 1,
    "tags": [
      "Simulation",
      "Traffic",
      "SmartTraffic",
      "SpeedLimit",
      "Adaptive",
      "Vehicle"
    ],
    "allowed_learner": [
      "TD"
    ],
    "country": [
      "China",
      "Australia"
    ],
    "uni": [
      "Southeast University",
      "University of Tasmania"
    ],
    "department": [
      "School of Transportation",
      "Information and Communication Technology"
    ],
    "source_task_selection": [
      "h"
    ],
    "was_in_survey": [
      "mag"
    ],
    "in_title": [
      "t",
      "r",
      "l"
    ],
    "in_abs": [
      "t",
      "r",
      "l",
      "k"
    ],
    "in_contet": [
      "t",
      "r",
      "l",
      "k"
    ],
    "rejected_or_unpublished": null,
    "duplication_status": null,
    "paper_for_thesis": 0,
    "paper_available": 1,
    "behind_paywall": 1,
    "paywall_circumventable": 1,
    "author_names": [
      "Zhibin Li",
      "Pan Liu",
      "Zehong Cao",
      "Zemian Ke"
    ]
  },
  {
    "id": 2893105945,
    "title": "Bayesian Transfer Reinforcement Learning with Prior Knowledge Rules.",
    "abst": "We propose a probabilistic framework to directly insert prior knowledge in reinforcement learning (RL) algorithms by defining the behaviour policy as a Bayesian posterior distribution. Such a posterior combines task specific information with prior knowledge, thus allowing to achieve transfer learning across tasks. The resulting method is flexible and it can be easily incorporated to any standard off-policy and on-policy algorithms, such as those based on temporal differences and policy gradients. We develop a specific instance of this Bayesian transfer RL framework by expressing prior knowledge as general deterministic rules that can be useful in a large variety of tasks, such as navigation tasks. Also, we elaborate more on recent probabilistic and entropy-regularised RL by developing a novel temporal learning algorithm and show how to combine it with Bayesian transfer RL. Finally, we demonstrate our method for solving mazes and show that significant speed ups can be obtained.",
    "url": "https://export.arxiv.org/pdf/1810.00468",
    "lang": "en",
    "authors": [
      167087310,
      2893607301
    ],
    "fos": [
      119857082,
      57830394,
      150899416,
      49937458,
      2982788086,
      154945302,
      2780076653,
      107673813,
      2986323316,
      33923547,
      97541855
    ],
    "journals": [
      2597173376
    ],
    "conferences": [],
    "conference_series": [],
    "references": [
      2294241375,
      2949608212
    ],
    "filter_matches": [
      "tr",
      "tl",
      "trl",
      "rl"
    ],
    "rank": 23096,
    "citation_count": 0,
    "estimated_citation_count": 0,
    "publication_date": "2018-09-30",
    "found_in": 1,
    "transfer_experiment_type": [
      "s"
    ],
    "transfer_experiment_subtype": [
      "same_all"
    ],
    "transfer_data_type": [
      "pri"
    ],
    "transfer_performance_metrics": [
      "j",
      "tt"
    ],
    "implementation": [
      "Custom",
      "CS",
      "Formulas",
      "Gym",
      "Figures"
    ],
    "policy_type": [
      "Q"
    ],
    "task_mappings": [
      "N/A"
    ],
    "autonomous_transfer": 0,
    "is_deep_rl": 0,
    "tags": [
      "2D",
      "Navigation",
      "Grid",
      "Maze",
      "Simulation"
    ],
    "allowed_learner": [
      "TD",
      "B"
    ],
    "country": [
      "Greece"
    ],
    "uni": [
      "Athens University"
    ],
    "department": [
      "Economics and Business"
    ],
    "source_task_selection": [],
    "was_in_survey": [
      "mag"
    ],
    "in_title": [
      "t",
      "r",
      "l",
      "k"
    ],
    "in_abs": [
      "t",
      "r",
      "l",
      "k"
    ],
    "in_contet": [
      "t",
      "r",
      "l",
      "k"
    ],
    "rejected_or_unpublished": 2,
    "duplication_status": null,
    "paper_for_thesis": 0,
    "paper_available": 1,
    "behind_paywall": 0,
    "paywall_circumventable": 0,
    "author_names": [
      "Michalis K. Titsias",
      "Sotirios Nikoloutsopoulos"
    ]
  },
  {
    "id": 2989941044,
    "title": "Transfer Reinforcement Learning across Environment Dynamics with Multiple Advisors.",
    "abst": null,
    "url": "http://ceur-ws.org/Vol-2491/paper11.pdf",
    "lang": null,
    "authors": [
      2270192752,
      2554860860,
      2779560189,
      2789104927
    ],
    "fos": [
      107457646,
      97541855,
      41008148
    ],
    "journals": [],
    "conferences": [],
    "conference_series": [],
    "references": [],
    "filter_matches": [
      "tr",
      "tl",
      "trl",
      "rl"
    ],
    "rank": 23104,
    "citation_count": 0,
    "estimated_citation_count": 0,
    "publication_date": "2019-01-01",
    "found_in": 1,
    "transfer_experiment_type": [
      "t"
    ],
    "transfer_experiment_subtype": [
      "multi"
    ],
    "transfer_data_type": [
      "advisor"
    ],
    "transfer_performance_metrics": [
      "j",
      "tr"
    ],
    "implementation": [
      "Custom",
      "CS",
      "Formulas",
      "Figures"
    ],
    "policy_type": [
      "BDPI",
      "ABDPI",
      "DQN"
    ],
    "task_mappings": [
      "N/A"
    ],
    "autonomous_transfer": 0,
    "is_deep_rl": 1,
    "tags": [
      "2D",
      "Navigation",
      "Grid",
      "Rooms",
      "Simulation"
    ],
    "allowed_learner": [
      "TD"
    ],
    "country": [
      "Belgium"
    ],
    "uni": [
      "Vrije Universiteit Brussel"
    ],
    "department": [
      "Computer Science"
    ],
    "source_task_selection": [
      "all"
    ],
    "was_in_survey": [
      "mag"
    ],
    "in_title": [
      "t",
      "r",
      "l"
    ],
    "in_abs": [
      "t",
      "r",
      "l",
      "k"
    ],
    "in_contet": [
      "t",
      "r",
      "l",
      "k",
      "s"
    ],
    "rejected_or_unpublished": null,
    "duplication_status": null,
    "paper_for_thesis": 0,
    "paper_available": 1,
    "behind_paywall": 0,
    "paywall_circumventable": 0,
    "author_names": [
      "Ann Now&#x00E9;",
      "Denis Steckelmacher",
      "Diederik M. Roijers",
      "H&#x00E9;l&#x00E8;ne Plisnier"
    ]
  },
  {
    "id": 1604959332,
    "title": "Relational transfer across reinforcement learning tasks via abstract policies.",
    "abst": "When designing intelligent agents that must solve sequential decision problems, often we do not have enough knowledge to build a complete model for the problems at hand. Reinforcement learning enables an agent to learn behavior by acquiring experience through trial-and-error interactions with the environment. However, knowledge is usually built from scratch and learning the optimal policy may take a long time. In this work, we improve the learning performance by exploring transfer learning; that is, the knowledge acquired in previous source tasks is used to accelerate learning in new target tasks. If the tasks present similarities, then the transferred knowledge guides the agent towards faster learning. We explore the use of a relational representation that allows description of relationships among objects. This representation simplifies the use of abstraction and the extraction of the similarities among tasks, enabling the generalization of solutions that can be used across different, but related, tasks. This work presents two model-free algorithms for online learning of abstract policies: AbsSarsa(&#x03BB;) and AbsProb-RL. The former builds a deterministic abstract policy from value functions, while the latter builds a stochastic abstract policy through direct search on the space of policies. We also propose the S2L-RL agent architecture, containing two levels of learning: an abstract level and a ground level. The agent simultaneously builds a ground policy and an abstract policy; not only the abstract policy can accelerate learning on the current task, but also it can guide the agent in a future task. Experiments in a robotic navigation environment show that these techniques are effective in improving the agent&#x2019;s learning performance, especially during the early stages of the learning process, when the agent is completely unaware of the new task.",
    "url": "http://www.teses.usp.br/teses/disponiveis/3/3141/tde-04112014-103827/pt-br.php",
    "lang": "pt",
    "authors": [
      2694705437
    ],
    "fos": [
      137703981,
      97541855,
      177877439,
      150899416,
      74072328,
      161301231,
      41008148,
      159886148,
      154945302,
      124304363
    ],
    "journals": [],
    "conferences": [],
    "conference_series": [],
    "references": [
      929682,
      6242441,
      24477102,
      121750629,
      132675044,
      142858861,
      193076044,
      249650263,
      610676890,
      1490954610,
      1491843047,
      1510943008,
      1517613402,
      1541084404,
      1595634327,
      1601974704,
      1631187438,
      1687873425,
      1822705290,
      1977970897,
      1991564165,
      1992445254,
      1993277309,
      1997477668,
      2014208555,
      2031727428,
      2033072307,
      2050630772,
      2051203581,
      2055418958,
      2056584142,
      2076337359,
      2083115070,
      2096600060,
      2097381042,
      2097498341,
      2098524868,
      2104641222,
      2107726111,
      2109910161,
      2118022839,
      2119053738,
      2119567691,
      2121517924,
      2121863487,
      2122410182,
      2124175081,
      2125055259,
      2126834960,
      2132057084,
      2133040789,
      2140135625,
      2149126181,
      2151382427,
      2152166054,
      2152669282,
      2154328025,
      2160067530,
      2163533082,
      2163808368,
      2165698076,
      2167840144,
      2169659168,
      2170164558,
      2171084228,
      2183243664,
      2183728818,
      2187770737,
      2200611301,
      2253637894,
      2344013593,
      2397240726,
      2964108826,
      3011120880,
      3020831056
    ],
    "filter_matches": [
      "tr",
      "tl",
      "trl",
      "rl"
    ],
    "rank": 23129,
    "citation_count": 0,
    "estimated_citation_count": 0,
    "publication_date": "2013-11-21",
    "found_in": 1,
    "transfer_experiment_type": [
      "s_i",
      "s_f",
      "levels"
    ],
    "transfer_experiment_subtype": [
      "multi"
    ],
    "transfer_data_type": [
      "Q",
      "pi_dyn"
    ],
    "transfer_performance_metrics": [
      "tt",
      "ap"
    ],
    "implementation": [
      "OSS",
      "Custom",
      "Figures",
      "Pseudo",
      "Formulas",
      "Tables"
    ],
    "policy_type": [
      "SARSA",
      "Q",
      "AbsSarsa",
      "AbsProb-RL",
      "S2L-RL"
    ],
    "task_mappings": [
      "N/A"
    ],
    "autonomous_transfer": 0,
    "is_deep_rl": 0,
    "tags": [
      "2D",
      "Navigation",
      "Grid",
      "Simulation",
      "MonteCarlo"
    ],
    "allowed_learner": [
      "RRL",
      "CBR",
      "H",
      "TD",
      "MB"
    ],
    "country": [
      "Brazil"
    ],
    "uni": [
      "University of Sao Paulo"
    ],
    "department": [
      "Computer Engineering"
    ],
    "source_task_selection": [
      "all",
      "lib"
    ],
    "was_in_survey": [
      "mag"
    ],
    "in_title": [
      "t",
      "r",
      "l"
    ],
    "in_abs": [
      "t",
      "r",
      "l",
      "k"
    ],
    "in_contet": [
      "t",
      "r",
      "l",
      "k",
      "s"
    ],
    "rejected_or_unpublished": null,
    "duplication_status": null,
    "paper_for_thesis": 2,
    "paper_available": 1,
    "behind_paywall": 0,
    "paywall_circumventable": 0,
    "author_names": [
      "Marcelo Li Koga"
    ]
  },
  {
    "id": 3007833995,
    "title": "Multi-AUV Collaborative Target Recognition Based on Transfer-Reinforcement Learning",
    "abst": null,
    "url": "https://dblp.uni-trier.de/db/journals/access/access8.html#CaiSXMC20",
    "lang": null,
    "authors": [
      2141325285,
      2969493313,
      2978116196,
      3008049092,
      3018811543
    ],
    "fos": [
      97541855,
      120314980,
      41008148,
      154945302
    ],
    "journals": [
      2485537415
    ],
    "conferences": [],
    "conference_series": [],
    "references": [],
    "filter_matches": [
      "tr",
      "tl",
      "trl",
      "rl"
    ],
    "rank": 23143,
    "citation_count": 0,
    "estimated_citation_count": 0,
    "publication_date": "2020-03-06",
    "found_in": 1,
    "transfer_experiment_type": [
      "s"
    ],
    "transfer_experiment_subtype": [
      "same_all"
    ],
    "transfer_data_type": [
      "fea",
      "Q"
    ],
    "transfer_performance_metrics": [
      "ap",
      "tr"
    ],
    "implementation": [
      "Custom",
      "CS",
      "Formulas",
      "Figures",
      "Pseudo",
      "Tables"
    ],
    "policy_type": [
      "Q"
    ],
    "task_mappings": [
      "N/A"
    ],
    "autonomous_transfer": 0,
    "is_deep_rl": 1,
    "tags": [
      "Underwater",
      "Image",
      "Feature",
      "Extraction",
      "Target",
      "Recognition",
      "AutonomousUnderwaterVehicle",
      "AUV",
      "CNN",
      "Vehicle",
      "Simulation"
    ],
    "allowed_learner": [
      "TD",
      "B"
    ],
    "country": [
      "China"
    ],
    "uni": [
      "Henan Institute of Science and Technology",
      "Shandong University"
    ],
    "department": [
      "Artifical Intelligence",
      "Information Engineering",
      "Control Science"
    ],
    "source_task_selection": [
      "all"
    ],
    "was_in_survey": [
      "mag"
    ],
    "in_title": [
      "t",
      "r",
      "l"
    ],
    "in_abs": [
      "t",
      "r",
      "l"
    ],
    "in_contet": [
      "t",
      "r",
      "l",
      "k"
    ],
    "rejected_or_unpublished": null,
    "duplication_status": null,
    "paper_for_thesis": 0,
    "paper_available": 1,
    "behind_paywall": 0,
    "paywall_circumventable": 0,
    "author_names": [
      "Zhenxue Chen",
      "Lei Cai",
      "Tao Xu",
      "Qiankun Sun",
      "Yukun Ma"
    ]
  },
  {
    "id": 2573943923,
    "title": "Transfer of Reinforcement Learning Negotiation Policies: from Bilateral to Multilateral Scenarios",
    "abst": null,
    "url": "https://researchportal.hw.ac.uk/en/publications/transfer-of-reinforcement-learning-negotiation-policies-from-bila",
    "lang": "en",
    "authors": [
      275989945,
      2250351219
    ],
    "fos": [
      56739046,
      119857082,
      539667460,
      154945302,
      41008148,
      199776023,
      97541855
    ],
    "journals": [],
    "conferences": [
      2331219974
    ],
    "conference_series": [
      1162831978
    ],
    "references": [],
    "filter_matches": [
      "tr",
      "tl",
      "trl",
      "rl"
    ],
    "rank": 23151,
    "citation_count": 0,
    "estimated_citation_count": 0,
    "publication_date": "2016-06-08",
    "found_in": 1,
    "transfer_experiment_type": [
      "t",
      "a"
    ],
    "transfer_experiment_subtype": [
      "multi"
    ],
    "transfer_data_type": [
      "pi"
    ],
    "transfer_performance_metrics": [
      "tr"
    ],
    "implementation": [
      "Custom",
      "CS"
    ],
    "policy_type": [],
    "task_mappings": [],
    "autonomous_transfer": 0,
    "is_deep_rl": 2,
    "tags": [
      "Simulation",
      "Trading",
      "Settlers"
    ],
    "allowed_learner": [
      "TD",
      "B"
    ],
    "country": [
      "UK"
    ],
    "uni": [
      "Herriot-Watt University"
    ],
    "department": [
      "Interaction Lab"
    ],
    "source_task_selection": [
      "h"
    ],
    "was_in_survey": [
      "mag"
    ],
    "in_title": [
      "t",
      "r",
      "l"
    ],
    "in_abs": [
      "t",
      "r",
      "l"
    ],
    "in_contet": [
      "t",
      "r",
      "l"
    ],
    "rejected_or_unpublished": null,
    "duplication_status": null,
    "paper_for_thesis": 0,
    "paper_available": 1,
    "behind_paywall": 0,
    "paywall_circumventable": 0,
    "author_names": [
      "Oliver Lemon",
      "Ioannis Efstathiou"
    ]
  },
  {
    "id": 2617832762,
    "title": "State Space Decomposition and Subgoal Creation for Transfer in Deep Reinforcement Learning",
    "abst": "Typical reinforcement learning (RL) agents learn to complete tasks specified by reward functions tailored to their domain. As such, the policies they learn do not generalize even to similar domains. To address this issue, we develop a framework through which a deep RL agent learns to generalize policies from smaller, simpler domains to more complex ones using a recurrent attention mechanism. The task is presented to the agent as an image and an instruction specifying the goal. This meta-controller guides the agent towards its goal by designing a sequence of smaller subtasks on the part of the state space within the attention, effectively decomposing it. As a baseline, we consider a setup without attention as well. Our experiments show that the meta-controller learns to create subgoals within the attention.",
    "url": "https://export.arxiv.org/pdf/1705.08997",
    "lang": "en",
    "authors": [
      2106580818,
      2139952810,
      2618361023,
      2619415042,
      2898328886
    ],
    "fos": [
      119857082,
      41008148,
      97541855,
      154945302,
      2993214343,
      72434380
    ],
    "journals": [
      2596500785
    ],
    "conferences": [],
    "conference_series": [],
    "references": [
      567721252,
      1515851193,
      2064675550,
      2119717200,
      2335959470
    ],
    "filter_matches": [
      "tr",
      "tl",
      "trl",
      "rl"
    ],
    "rank": 23167,
    "citation_count": 0,
    "estimated_citation_count": 0,
    "publication_date": "2017-05-24",
    "found_in": 1,
    "transfer_experiment_type": [
      "s_f"
    ],
    "transfer_experiment_subtype": [
      "same_all"
    ],
    "transfer_data_type": [
      "sub"
    ],
    "transfer_performance_metrics": [
      "tr"
    ],
    "implementation": [
      "Custom",
      "CS",
      "Figures",
      "Formulas"
    ],
    "policy_type": [
      "PG",
      "LSTM"
    ],
    "task_mappings": [
      "exp"
    ],
    "autonomous_transfer": 0,
    "is_deep_rl": 1,
    "tags": [
      "Hierarchical",
      "MetaController",
      "Attention",
      "Decomposition",
      "SubgoalCreation",
      "2D",
      "Simulation",
      "Grid",
      "Navigation"
    ],
    "allowed_learner": [
      "TD",
      "H"
    ],
    "country": [
      "USA"
    ],
    "uni": [
      "Georgia Institute of Technology"
    ],
    "department": [
      "College of Computing"
    ],
    "source_task_selection": [
      "h"
    ],
    "was_in_survey": [
      "mag"
    ],
    "in_title": [
      "t",
      "r",
      "l"
    ],
    "in_abs": [
      "r",
      "l"
    ],
    "in_contet": [
      "t",
      "r",
      "l",
      "k"
    ],
    "rejected_or_unpublished": null,
    "duplication_status": null,
    "paper_for_thesis": 0,
    "paper_available": 1,
    "behind_paywall": 0,
    "paywall_circumventable": 0,
    "author_names": [
      "Charles L. Isbell",
      "Himanshu Sahni",
      "Saurabh Kumar",
      "Farhan Tejani",
      "Yannick Schroecker"
    ]
  },
  {
    "id": 2395918031,
    "title": "FCM-Type Co-clustering Transfer Reinforcement Learning for Non-Markov Processes",
    "abst": "In applying reinforcement learning to continuous space problems, discretization or redefinition of the learning space can be a promising approach. Several methods and algorithms have been introduced to learning agents to respond to this problem. In our previous study, we introduced an FCCM clustering technique into Q-learning (called QL-FCCM) and its transfer learning in the Markov process. Since we could not respond to complicated environments like a non-Markov process, in this study, we propose a method in which an agent updates his Q-table by changing the trade-off ratio, Q-learning and QL-FCCM, based on the damping ratio. We conducted numerical experiments of the single pendulum standing problem and our model resulted in a smooth learning process.",
    "url": "https://dblp.uni-trier.de/db/conf/iukm/iukm2015.html#NotsuUHUH15",
    "lang": "en",
    "authors": [
      2013435550,
      2130307561,
      2227448763,
      2233364215,
      2499937781
    ],
    "fos": [
      159886148,
      73555534,
      110639684,
      124139854,
      41008148,
      154945302,
      97541855,
      150899416,
      144817290,
      73000952
    ],
    "journals": [],
    "conferences": [
      180696917
    ],
    "conference_series": [
      2759911507
    ],
    "references": [
      1501162218,
      1557517019,
      1948973323,
      1950259624,
      1968274501,
      2016312520,
      2029849080,
      2041191585,
      2061708588,
      2113076747,
      2160067530,
      2765733863
    ],
    "filter_matches": [
      "tr",
      "tl",
      "trl",
      "rl"
    ],
    "rank": 23232,
    "citation_count": 0,
    "estimated_citation_count": 0,
    "publication_date": "2015-10-15",
    "found_in": 1,
    "transfer_experiment_type": [
      "t"
    ],
    "transfer_experiment_subtype": [
      "same_all"
    ],
    "transfer_data_type": [
      "FCCM"
    ],
    "transfer_performance_metrics": [
      "j",
      "tt"
    ],
    "implementation": [
      "Custom",
      "CS",
      "Formulas",
      "Figures",
      "Pseudo"
    ],
    "policy_type": [
      "Q",
      "Q-FCCM"
    ],
    "task_mappings": [
      "N/A"
    ],
    "autonomous_transfer": 0,
    "is_deep_rl": 0,
    "tags": [
      "2D",
      "ClassicControl",
      "Pendulum",
      "Simulation"
    ],
    "allowed_learner": [
      "TD"
    ],
    "country": [
      "Japan"
    ],
    "uni": [
      "Osaka Prefecture University"
    ],
    "department": [
      "Engineering"
    ],
    "source_task_selection": [
      "h"
    ],
    "was_in_survey": [
      "mag"
    ],
    "in_title": [
      "t",
      "r",
      "l"
    ],
    "in_abs": [
      "t",
      "r",
      "l"
    ],
    "in_contet": [
      "t",
      "r",
      "l",
      "k"
    ],
    "rejected_or_unpublished": null,
    "duplication_status": null,
    "paper_for_thesis": 0,
    "paper_available": 1,
    "behind_paywall": 1,
    "paywall_circumventable": 1,
    "author_names": [
      "Akira Notsu",
      "Katsuhiro Honda",
      "Seiki Ubukata",
      "Yuichi Hattori",
      "Takanori Ueno"
    ]
  },
  {
    "id": 3004450474,
    "title": "Adapt-to-Learn: Policy Transfer in Reinforcement Learning",
    "abst": null,
    "url": "https://openreview.net/pdf?id=ryeT10VKDH",
    "lang": null,
    "authors": [
      2098179893,
      2110746766
    ],
    "fos": [
      2776731479,
      56739046,
      97541855,
      41008148
    ],
    "journals": [],
    "conferences": [],
    "conference_series": [],
    "references": [],
    "filter_matches": [
      "tr",
      "tl",
      "trl",
      "rl"
    ],
    "rank": 23301,
    "citation_count": 0,
    "estimated_citation_count": 0,
    "publication_date": "2019-09-25",
    "found_in": 1,
    "transfer_experiment_type": [
      "t"
    ],
    "transfer_experiment_subtype": [
      "same_all"
    ],
    "transfer_data_type": [
      "r",
      "manifold"
    ],
    "transfer_performance_metrics": [
      "j",
      "tr"
    ],
    "implementation": [
      "Custom",
      "CS",
      "Formulas",
      "Pseudo",
      "Theorem",
      "Tables",
      "Lemma"
    ],
    "policy_type": [
      "PPO",
      "ATL"
    ],
    "task_mappings": [
      "exp"
    ],
    "autonomous_transfer": 0,
    "is_deep_rl": 1,
    "tags": [
      "Simulation",
      "Robotics",
      "AdaptToLearn",
      "Vehicle",
      "Aerial",
      "Drone"
    ],
    "allowed_learner": [
      "TD",
      "B",
      "MB"
    ],
    "country": [
      "USA"
    ],
    "uni": [
      "University of Illinois"
    ],
    "department": [],
    "source_task_selection": [],
    "was_in_survey": [
      "mag"
    ],
    "in_title": [
      "t",
      "r",
      "l"
    ],
    "in_abs": [
      "t",
      "r",
      "l",
      "s"
    ],
    "in_contet": [
      "t",
      "r",
      "l",
      "s"
    ],
    "rejected_or_unpublished": 1,
    "duplication_status": null,
    "paper_for_thesis": 0,
    "paper_available": 1,
    "behind_paywall": 0,
    "paywall_circumventable": 0,
    "author_names": [
      "Girish Joshi",
      "Girish Chowdhary"
    ]
  },
  {
    "id": 166919721,
    "title": "Bayesian methods for knowledge transfer and policy search in reinforcement learning",
    "abst": null,
    "url": "https://ir.library.oregonstate.edu/downloads/1544bs366",
    "lang": "en",
    "authors": [
      2720688162
    ],
    "fos": [
      97541855,
      2776960227,
      119857082,
      41008148,
      154945302,
      107673813,
      199190896
    ],
    "journals": [],
    "conferences": [],
    "conference_series": [],
    "references": [],
    "filter_matches": [
      "tr",
      "tl",
      "trl",
      "rl"
    ],
    "rank": 23398,
    "citation_count": 0,
    "estimated_citation_count": 0,
    "publication_date": "2012-07-28",
    "found_in": 1,
    "transfer_experiment_type": [
      "s_i",
      "s_f",
      "levels",
      "#"
    ],
    "transfer_experiment_subtype": [
      "multi"
    ],
    "transfer_data_type": [
      "advisor",
      "pri"
    ],
    "transfer_performance_metrics": [
      "tt",
      "tr",
      "ap"
    ],
    "implementation": [
      "Custom",
      "CS",
      "Formulas",
      "Figures",
      "Tables",
      "Pseudo"
    ],
    "policy_type": [
      "MBOA",
      "DynaQ",
      "BOA",
      "LSPI",
      "Q",
      "CMAC",
      "BPS"
    ],
    "task_mappings": [
      "N/A"
    ],
    "autonomous_transfer": 0,
    "is_deep_rl": 0,
    "tags": [
      "Simulation",
      "2D",
      "Navigation",
      "Cost",
      "Wargus",
      "MountainCar",
      "Acrobot",
      "CartPole",
      "Bicycle",
      "3LinkPlanarArm",
      "MultiAgent",
      "MonteCarlo",
      "ClassicControl",
      "PredatorPrey"
    ],
    "allowed_learner": [
      "H",
      "TD",
      "PS",
      "B",
      "MB",
      "batch"
    ],
    "country": [
      "USA"
    ],
    "uni": [
      "Oregon State University"
    ],
    "department": [
      "Computer Science"
    ],
    "source_task_selection": [
      "all"
    ],
    "was_in_survey": [
      "mag"
    ],
    "in_title": [
      "t",
      "r",
      "l",
      "k"
    ],
    "in_abs": [
      "t",
      "r",
      "l",
      "k"
    ],
    "in_contet": [
      "t",
      "r",
      "l",
      "k"
    ],
    "rejected_or_unpublished": null,
    "duplication_status": null,
    "paper_for_thesis": 1,
    "paper_available": 1,
    "behind_paywall": 0,
    "paywall_circumventable": 0,
    "author_names": [
      "Aaron Wilson"
    ]
  },
  {
    "id": 3006434787,
    "title": "Transferring knowledge as heuristics in reinforcement learning",
    "abst": "The goal of this paper is to propose and analyse a transfer learning meta-algorithm that allows the implementation of distinct methods using heuristics to accelerate a Reinforcement Learning proced...",
    "url": "https://dl.acm.org/doi/10.1016/j.artint.2015.05.008",
    "lang": "en",
    "authors": [
      3004290310,
      3005576157,
      3005577180,
      3006251290,
      3006298169
    ],
    "fos": [
      150899416,
      154945302,
      127705205,
      119857082,
      97541855,
      20162079,
      33923547
    ],
    "journals": [
      196139623
    ],
    "conferences": [],
    "conference_series": [],
    "references": [],
    "filter_matches": [
      "tr",
      "tl",
      "trl",
      "rl"
    ],
    "rank": 23400,
    "citation_count": 0,
    "estimated_citation_count": 0,
    "publication_date": "2015-09-01",
    "found_in": 1,
    "transfer_experiment_type": [
      "a",
      "v"
    ],
    "transfer_experiment_subtype": [
      "diff-it"
    ],
    "transfer_data_type": [
      "Q"
    ],
    "transfer_performance_metrics": [
      "j"
    ],
    "implementation": [
      "Custom",
      "OSS",
      "Formulas",
      "Figures",
      "Tables",
      "Pseudo"
    ],
    "policy_type": [
      "HA-SARSA",
      "L3-SARSA",
      "HAQL",
      "CB-HAQL"
    ],
    "task_mappings": [
      "sup"
    ],
    "autonomous_transfer": 0,
    "is_deep_rl": 0,
    "tags": [
      "2D",
      "3D",
      "2Dto3D",
      "MountainCar",
      "MountainCar3D",
      "Simulation",
      "CaseBased",
      "ClassicControl"
    ],
    "allowed_learner": [
      "CBR"
    ],
    "country": [
      "Brazil",
      "Spain"
    ],
    "uni": [
      "Centro Universitrio da FE",
      "Universidade Federal do ABC",
      "Universitat Autonoma de Barcelona"
    ],
    "department": [
      "Artifical Intelligence Research Institute"
    ],
    "source_task_selection": [],
    "was_in_survey": [
      "mag",
      "silva"
    ],
    "in_title": [
      "t",
      "r",
      "l"
    ],
    "in_abs": [
      "t",
      "r",
      "l",
      "k"
    ],
    "in_contet": [
      "t",
      "r",
      "l",
      "k"
    ],
    "rejected_or_unpublished": null,
    "duplication_status": null,
    "paper_for_thesis": 0,
    "paper_available": 1,
    "behind_paywall": 0,
    "paywall_circumventable": 0,
    "author_names": [
      "Lopez de MantarasRamon",
      "A CelibertoLuiz",
      "E SantosPaulo",
      "P MatsuuraJackson",
      "A C BianchiReinaldo"
    ]
  },
  {
    "id": 2910166048,
    "title": "Knowledge Transfer between Multi-granularity Models for Reinforcement Learning",
    "abst": "As a widely used machine learning method, reinforcement learning (RL) is a very effective way to solve decision and control problems where learning skills are needed. In this paper, a knowledge transfer method between multi-granularity models is proposed for RL to speed up the learning process and adapt to the dynamic environments. The learning process runs on naturally organized multi-granularity models, e.g., the coarse-grained model and the fine-grained model. This multi-granularity model constitutes a knowledge transfer architecture that bridges the reinforcement learning between different granularity levels. The proposed multi-granularity reinforcement learning (MGRL) approach and related algorithms can scale up very well and speed up learning with other granularity learning process. Several groups of simulation experiments are carried out using a puzzle problem in a gridworld environment. The results demonstrate the effectiveness and efficiency of the proposed approach.",
    "url": "https://dblp.uni-trier.de/db/conf/smc/smc2018.html#XinTWC18",
    "lang": null,
    "authors": [
      2908530451,
      2908874069,
      2909636697,
      2911167310
    ],
    "fos": [
      154945302,
      41008148,
      119857082,
      123657996,
      97541855,
      68339613,
      177774035,
      2776960227,
      2777874358
    ],
    "journals": [],
    "conferences": [
      2317949139
    ],
    "conference_series": [
      1170695740
    ],
    "references": [
      605348272,
      1542941925,
      2002260889,
      2014512216,
      2062472527,
      2100677568,
      2114451917,
      2116157560,
      2119567691,
      2121863487,
      2128814508,
      2150478139,
      2773952570,
      2912681837
    ],
    "filter_matches": [
      "tr",
      "tl",
      "trl",
      "rl"
    ],
    "rank": 23430,
    "citation_count": 0,
    "estimated_citation_count": 0,
    "publication_date": "2018-10-01",
    "found_in": 1,
    "transfer_experiment_type": [
      "v",
      "s_i",
      "s_f"
    ],
    "transfer_experiment_subtype": [
      "same_all"
    ],
    "transfer_data_type": [
      "Q",
      "options"
    ],
    "transfer_performance_metrics": [
      "j",
      "tt"
    ],
    "implementation": [
      "Custom",
      "CS",
      "Formulas",
      "Pseudo",
      "Figures"
    ],
    "policy_type": [
      "Q",
      "MGRL"
    ],
    "task_mappings": [
      "exp"
    ],
    "autonomous_transfer": 0,
    "is_deep_rl": 0,
    "tags": [
      "2D",
      "Navigation",
      "Grid",
      "Maze",
      "Simulation"
    ],
    "allowed_learner": [
      "TD",
      "H"
    ],
    "country": [
      "China"
    ],
    "uni": [
      "Nanjing University"
    ],
    "department": [
      "Control and Systems Engineering",
      "Management and Engineering"
    ],
    "source_task_selection": [
      "h"
    ],
    "was_in_survey": [
      "mag"
    ],
    "in_title": [
      "t",
      "r",
      "l",
      "k"
    ],
    "in_abs": [
      "t",
      "r",
      "l",
      "k",
      "s"
    ],
    "in_contet": [
      "t",
      "r",
      "l",
      "k",
      "s"
    ],
    "rejected_or_unpublished": null,
    "duplication_status": null,
    "paper_for_thesis": 0,
    "paper_available": 1,
    "behind_paywall": 0,
    "paywall_circumventable": 0,
    "author_names": [
      "Chunlin Chen",
      "Kaiqiang Tang",
      "Bo Xin",
      "Lan Wang"
    ]
  },
  {
    "id": 1604516390,
    "title": "Graph Laplacian based transfer learning in reinforcement learning",
    "abst": "The aim of transfer learning is to accelerate learning in related domains. In reinforcement learning, many different features such as a value function and a policy can be transferred from a source domain to a related target domain. Many researches focused on transfer using hand-coded translation functions that are designed by the experts a priori. However, it is not only very costly but also problem dependent. We propose to apply the Graph Laplacian that is based on the spectral graph theory to decompose the value functions of both a source domain and a target domain into a sum of the basis functions respectively. The transfer learning can be carried out by transferring weights on the basis functions of a source domain to a target domain. We investigate two types of domain transfer, scaling and topological. The results demonstrated that the transferred policy is a better prior policy to reduce the learning time.",
    "url": "https://dl.acm.org/citation.cfm?id=1402821.1402869",
    "lang": "en",
    "authors": [
      1974178992,
      2141359790,
      2155552642
    ],
    "fos": [
      41008148,
      97541855,
      119857082,
      150899416,
      77075516,
      58973888,
      154945302,
      77967617,
      28006648,
      115178988,
      8038995
    ],
    "journals": [],
    "conferences": [
      2787693658
    ],
    "conference_series": [
      1168671587
    ],
    "references": [
      90468634,
      1509577866,
      1515851193,
      1578099820,
      1607318605,
      1966949944,
      2143958939,
      2154328025,
      2161795906,
      2164114810
    ],
    "filter_matches": [
      "tr",
      "tl",
      "trl",
      "rl"
    ],
    "rank": 23430,
    "citation_count": 1,
    "estimated_citation_count": 1,
    "publication_date": "2008-05-12",
    "found_in": 1,
    "transfer_experiment_type": [
      "v",
      "s",
      "levels"
    ],
    "transfer_experiment_subtype": [
      "multi"
    ],
    "transfer_data_type": [
      "Q",
      "pvf"
    ],
    "transfer_performance_metrics": [
      "j"
    ],
    "implementation": [
      "Custom",
      "CS",
      "Pseudo",
      "Formulas",
      "Figures"
    ],
    "policy_type": [
      "Q",
      "LaplacianGraph"
    ],
    "task_mappings": [
      "exp"
    ],
    "autonomous_transfer": 0,
    "is_deep_rl": 0,
    "tags": [
      "2D",
      "Navigation",
      "Grid",
      "Maze",
      "Simulation"
    ],
    "allowed_learner": [
      "TD"
    ],
    "country": [
      "Taiwan"
    ],
    "uni": [
      "National Tsing-Hua University"
    ],
    "department": [
      "Computer Science"
    ],
    "source_task_selection": [
      "h"
    ],
    "was_in_survey": [
      "mag"
    ],
    "in_title": [
      "t",
      "r",
      "l"
    ],
    "in_abs": [
      "t",
      "r",
      "l"
    ],
    "in_contet": [
      "t",
      "r",
      "l",
      "k"
    ],
    "rejected_or_unpublished": null,
    "duplication_status": null,
    "paper_for_thesis": 0,
    "paper_available": 1,
    "behind_paywall": 0,
    "paywall_circumventable": 0,
    "author_names": [
      "Ke-Ting Xiao",
      "Von-Wun Soo",
      "Yi-Ting Tsao"
    ]
  },
  {
    "id": 3016369563,
    "title": "Combined Model for Partially-Observable and Non-Observable Task Switching: Solving Hierarchical Reinforcement Learning Problems Statically and Dynamically with Transfer Learning.",
    "abst": "An integral function of fully autonomous robots and humans is the ability to focus attention on a few relevant percepts to reach a certain goal while disregarding irrelevant percepts. Humans and animals rely on the interactions between the Pre-Frontal Cortex (PFC) and the Basal Ganglia (BG) to achieve this focus called Working Memory (WM). The Working Memory Toolkit (WMtk) was developed based on a computational neuroscience model of this phenomenon with Temporal Difference (TD) Learning for autonomous systems. Recent adaptations of the toolkit either utilize Abstract Task Representations (ATRs) to solve Non-Observable (NO) tasks or storage of past input features to solve Partially-Observable (PO) tasks, but not both. We propose a new model, PONOWMtk, which combines both approaches, ATRs and input storage, with a static or dynamic number of ATRs. The results of our experiments show that PONOWMtk performs effectively for tasks that exhibit PO, NO, or both properties.",
    "url": "http://arxiv.org/pdf/2004.06213.pdf",
    "lang": "en",
    "authors": [
      2308109953,
      2990515125
    ],
    "fos": [
      154945302,
      90509273,
      97541855,
      150899416,
      119857082,
      15286952,
      41008148,
      21963081,
      93500599,
      196340769,
      5147268
    ],
    "journals": [
      2596500785
    ],
    "conferences": [],
    "conference_series": [],
    "references": [
      124197801,
      2012536241,
      2099201756,
      2111971565,
      2121863487,
      2127262270,
      2157306293,
      2168145769,
      2767493943,
      2906926620
    ],
    "filter_matches": [
      "tr",
      "tl",
      "trl",
      "rl"
    ],
    "rank": 23467,
    "citation_count": 0,
    "estimated_citation_count": 0,
    "publication_date": "2020-04-13",
    "found_in": 1,
    "transfer_experiment_type": [
      "s_i",
      "s_f",
      "t"
    ],
    "transfer_experiment_subtype": [
      "same_all"
    ],
    "transfer_data_type": [
      "fea",
      "model"
    ],
    "transfer_performance_metrics": [],
    "implementation": [
      "Custom",
      "CS",
      "Formulas",
      "Figures",
      "Tables"
    ],
    "policy_type": [
      "Q"
    ],
    "task_mappings": [
      "exp"
    ],
    "autonomous_transfer": 0,
    "is_deep_rl": 1,
    "tags": [
      "2D",
      "Navigation",
      "Grid",
      "Maze",
      "Simulation",
      "Hierarchical",
      "HolographicReducedRepresentations",
      "WorkingMemory",
      "AbstractTaskRepresentations"
    ],
    "allowed_learner": [
      "TD",
      "H",
      "B"
    ],
    "country": [
      "USA"
    ],
    "uni": [
      "Middle Tennessee State University"
    ],
    "department": [
      "Computer Science"
    ],
    "source_task_selection": [
      "h"
    ],
    "was_in_survey": [
      "mag"
    ],
    "in_title": [
      "t",
      "r",
      "l"
    ],
    "in_abs": [
      "t",
      "r",
      "l"
    ],
    "in_contet": [
      "t",
      "r",
      "l",
      "k"
    ],
    "rejected_or_unpublished": 2,
    "duplication_status": null,
    "paper_for_thesis": 0,
    "paper_available": 1,
    "behind_paywall": 0,
    "paywall_circumventable": 0,
    "author_names": [
      "Joshua L. Phillips",
      "Nibraas Khan"
    ]
  },
  {
    "id": 2997729838,
    "title": "An Optimal Transfer of Knowledge in Reinforcement Learning through Greedy Approach",
    "abst": "Transfer Learning has been used in Reinforcement Learning methods by integrating information from preceding task. Although, these algorithms generally have to be assigned either a complete prototype of the task or direct mapping of information from input task to output task. The agent may not have the knowledge of such complex information. However, it is able to examine the resemblance between two tasks which are similar in taking action or approach. In this paper efforts has been made to solve mountain car problem (a famous problem domain in the area of Reinforcement Learning) using the SARSA ( $&#x0008;oldsymbol{lambda}$ ) [10] algorithm and MASTER [3] method by using greedy approach. The greedy approach has been used in applying parameters for transferring the knowledge within 2-dimensional mountain car to 3-dimensional mountain car. In the existing techniques the best results are obtained with use of parameters like epsilon decay factor, eligibility traces with greater decay factor and positive weight initialization. In the article, greedy approach has been applied with these parameters to get optimal results.",
    "url": "https://dblp.uni-trier.de/db/conf/icccnt/icccnt2019.html#KumariCM19",
    "lang": null,
    "authors": [
      2996882712,
      2998629961,
      2998710854
    ],
    "fos": [
      2993708069,
      2778113609,
      2992808375,
      150899416,
      154945302,
      97541855,
      114466953,
      41008148,
      143377199
    ],
    "journals": [],
    "conferences": [],
    "conference_series": [
      2755009998
    ],
    "references": [
      1533597678,
      1550698229,
      1570448133,
      1600437712,
      2041367235,
      2109910161,
      2113913482,
      2114580749,
      2130005627,
      2133040789,
      2741594138,
      2782656435,
      2885671219,
      2890735789,
      2902334345,
      3020831056
    ],
    "filter_matches": [
      "tr",
      "tl",
      "trl",
      "rl"
    ],
    "rank": 23475,
    "citation_count": 0,
    "estimated_citation_count": 0,
    "publication_date": "2019-07-01",
    "found_in": 1,
    "transfer_experiment_type": [
      "a",
      "v"
    ],
    "transfer_experiment_subtype": [
      "diff-it"
    ],
    "transfer_data_type": [
      "Q"
    ],
    "transfer_performance_metrics": [
      "tt"
    ],
    "implementation": [
      "Custom",
      "CS",
      "Formulas",
      "Figures",
      "Tables"
    ],
    "policy_type": [
      "SARSA",
      "MASTER"
    ],
    "task_mappings": [
      "sup"
    ],
    "autonomous_transfer": 0,
    "is_deep_rl": 0,
    "tags": [
      "2D",
      "3D",
      "2Dto3D",
      "MountainCar",
      "MountainCar3D",
      "Simulation",
      "ClassicControl"
    ],
    "allowed_learner": [
      "TD"
    ],
    "country": [
      "India"
    ],
    "uni": [
      "REC Ambedkar Nagar"
    ],
    "department": [
      "Information Technology"
    ],
    "source_task_selection": [
      "h"
    ],
    "was_in_survey": [
      "mag"
    ],
    "in_title": [
      "t",
      "r",
      "l",
      "k"
    ],
    "in_abs": [
      "t",
      "r",
      "l",
      "k"
    ],
    "in_contet": [
      "t",
      "r",
      "l",
      "k"
    ],
    "rejected_or_unpublished": null,
    "duplication_status": null,
    "paper_for_thesis": 0,
    "paper_available": 1,
    "behind_paywall": 1,
    "paywall_circumventable": 1,
    "author_names": [
      "Ashish Kumar Mishra",
      "Mahima Chaudhary",
      "Deepika Kumari"
    ]
  },
  {
    "id": 101901138,
    "title": "Improving Batch Reinforcement Learning Performance through Transfer of Samples",
    "abst": "The main objective of transfer in reinforcement learning is to reduce the complexity of learning the solution of a target task by effectively reusing the knowledge retained from solving a set of source tasks. One of the main problems is to avoid negative transfer, that is the transfer of knowledge across tasks that are significantly different that may worsen the learning performance. In this paper, we introduce a novel algorithm that selectively transfers samples (i.e., tuples ) from source to target tasks and that uses them as input for batch reinforcement-learning algorithms. By transferring samples from source tasks that are mostly similar to the target task, we reduce the number of samples actually collected from the target task to learn its solution. We show that the proposed approach is effective in reducing the learning complexity, even when some source tasks are significantly different from the target task.",
    "url": "https://dblp.uni-trier.de/db/conf/stairs/stairs2008.html#LazaricRB08",
    "lang": "en",
    "authors": [
      98988277,
      131065259,
      2103877068
    ],
    "fos": [
      119857082,
      154945302,
      97541855,
      2779178101,
      41008148,
      28006648,
      2983510059,
      47932503,
      206588197,
      118930307,
      150899416
    ],
    "journals": [],
    "conferences": [],
    "conference_series": [
      1187005110
    ],
    "references": [
      131458779,
      1492014007,
      1515851193,
      2004030284,
      2090170171,
      2110292307,
      2120346334,
      2131831090,
      2133040789,
      2568646110
    ],
    "filter_matches": [
      "tr",
      "tl",
      "trl",
      "rl"
    ],
    "rank": 23540,
    "citation_count": 0,
    "estimated_citation_count": 0,
    "publication_date": "2008-06-28",
    "found_in": 1,
    "transfer_experiment_type": [
      "s_i",
      "s_f",
      "s"
    ],
    "transfer_experiment_subtype": [
      "multi"
    ],
    "transfer_data_type": [
      "I"
    ],
    "transfer_performance_metrics": [
      "j",
      "tt"
    ],
    "implementation": [
      "Custom",
      "CS",
      "Pseudo",
      "Formulas",
      "Figures",
      "Tables"
    ],
    "policy_type": [
      "FQI"
    ],
    "task_mappings": [
      "N/A"
    ],
    "autonomous_transfer": 0,
    "is_deep_rl": 0,
    "tags": [
      "2D",
      "Navigation",
      "Obstacle",
      "Samples"
    ],
    "allowed_learner": [
      "TD",
      "batch"
    ],
    "country": [
      "Italy"
    ],
    "uni": [
      "Politecnico di Milano"
    ],
    "department": [
      "IIT-Lab"
    ],
    "source_task_selection": [
      "lib"
    ],
    "was_in_survey": [
      "mag"
    ],
    "in_title": [
      "t",
      "r",
      "l"
    ],
    "in_abs": [
      "t",
      "r",
      "l",
      "k"
    ],
    "in_contet": [
      "t",
      "r",
      "l",
      "k"
    ],
    "rejected_or_unpublished": null,
    "duplication_status": null,
    "paper_for_thesis": 0,
    "paper_available": 1,
    "behind_paywall": 1,
    "paywall_circumventable": 0,
    "author_names": [
      "Andrea Bonarini",
      "Alessandro Lazaric",
      "Marcello Restelli"
    ]
  },
  {
    "id": 2907903452,
    "title": "Simulation and Transfer of Reinforcement Learning Algorithms for Autonomous Obstacle Avoidance",
    "abst": "The explicit programming of obstacle avoidance by an autonomous robot can be a computationally expensive undertaking. The application of reinforcement learning algorithms promises a reduction of programming effort. However, these algorithms build on iterative training processes and therefore are time-consuming. In order to overcome this drawback we propose to displace the training process to abstract simulation scenarios. In this study we trained four different reinforcement algorithms (Q-Learning, Deep-Q-Learning, Deep Deterministic Policy Gradient and Asynchronous Advantage-Actor-Critic) in different abstract simulation scenarios and transferred the learning results to an autonomous robot. Except for the Asynchronous Advantage-Actor-Critic we achieved good obstacle avoidance during the simulation. Without further real-world training the policies learned by Q-Learning and Deep-Q-Learning achieved immediately obstacle avoidance when transferred to an autonomous robot.",
    "url": "https://dblp.uni-trier.de/db/conf/ias/ias2018.html#LenkHMRS18",
    "lang": "en",
    "authors": [
      2688038085,
      2904534311,
      2905349733,
      2907067253,
      2907092495
    ],
    "fos": [
      11413529,
      2781459529,
      151319957,
      6683253,
      41008148,
      67203356,
      2778835581,
      97541855
    ],
    "journals": [],
    "conferences": [],
    "conference_series": [
      2754184140
    ],
    "references": [
      1484387568,
      1557517019,
      1757796397,
      1895872655,
      1972159947,
      1977655452,
      2028684033,
      2042322703,
      2121863487,
      2125612430,
      2135572147,
      2159064451,
      2172968643,
      2907637664,
      2964043796,
      2964295739
    ],
    "filter_matches": [
      "tr",
      "tl",
      "trl",
      "rl"
    ],
    "rank": 23589,
    "citation_count": 0,
    "estimated_citation_count": 0,
    "publication_date": "2018-06-11",
    "found_in": 1,
    "transfer_experiment_type": [
      "t"
    ],
    "transfer_experiment_subtype": [
      "sim2real"
    ],
    "transfer_data_type": [
      "pi"
    ],
    "transfer_performance_metrics": [
      "j"
    ],
    "implementation": [
      "Custom",
      "CS",
      "TensorFlow",
      "Keras",
      "Gym",
      "Figures"
    ],
    "policy_type": [
      "Q",
      "DQN",
      "DDPG",
      "A3C"
    ],
    "task_mappings": [
      "N/A"
    ],
    "autonomous_transfer": 0,
    "is_deep_rl": 1,
    "tags": [
      "Robotics",
      "Mindstorm",
      "Simulation",
      "RealWorld",
      "EV3",
      "Navigation",
      "3D",
      "Driving",
      "Autonomous",
      "Vehicles",
      "Car"
    ],
    "allowed_learner": [
      "TD"
    ],
    "country": [
      "Germany"
    ],
    "uni": [
      "SAP SE",
      "Duale Hochschule Baden-Wrttemberg"
    ],
    "department": [
      "Computer Science"
    ],
    "source_task_selection": [
      "h"
    ],
    "was_in_survey": [
      "mag"
    ],
    "in_title": [
      "t",
      "r",
      "l"
    ],
    "in_abs": [
      "t",
      "r",
      "l"
    ],
    "in_contet": [
      "t",
      "r",
      "l"
    ],
    "rejected_or_unpublished": null,
    "duplication_status": null,
    "paper_for_thesis": 0,
    "paper_available": 1,
    "behind_paywall": 1,
    "paywall_circumventable": 1,
    "author_names": [
      "Marcus Strand",
      "Silvan M&#x00FC;ller",
      "Oliver Rettig",
      "Max Lenk",
      "Paula Hilsendegen"
    ]
  },
  {
    "id": 2889076726,
    "title": "Transfer of reinforcement learning for a robotic skill",
    "abst": "In this work, we develop the transfer learning (TL) of reinforcement learning (RL) for the robotic skill of throwing a ball into a basket, from a computer simulated environment to a real-world impl ...",
    "url": "https://aaltodoc.aalto.fi:443/handle/123456789/33782",
    "lang": "en",
    "authors": [
      2895959021,
      2896393005
    ],
    "fos": [
      97541855,
      150899416,
      154945302,
      207451115,
      34413123,
      107457646,
      41008148
    ],
    "journals": [],
    "conferences": [],
    "conference_series": [],
    "references": [],
    "filter_matches": [
      "tr",
      "tl",
      "trl",
      "rl"
    ],
    "rank": 23614,
    "citation_count": 0,
    "estimated_citation_count": 0,
    "publication_date": "2018-08-20",
    "found_in": 1,
    "transfer_experiment_type": [
      "t"
    ],
    "transfer_experiment_subtype": [
      "sim2real"
    ],
    "transfer_data_type": [
      "pi",
      "curriculum"
    ],
    "transfer_performance_metrics": [
      "j",
      "tt"
    ],
    "implementation": [
      "Custom",
      "CS",
      "LWRSIM"
    ],
    "policy_type": [
      "PILCO"
    ],
    "task_mappings": [],
    "autonomous_transfer": 0,
    "is_deep_rl": 0,
    "tags": [
      "Robotics",
      "Mujoco",
      "RealWorld",
      "Simulation",
      "Kuka",
      "Arm",
      "LWRSim",
      "PILCO"
    ],
    "allowed_learner": [
      "TD",
      "PS",
      "MB"
    ],
    "country": [
      "Finland"
    ],
    "uni": [
      "Aalto University"
    ],
    "department": [
      "School of Electrical Engineering"
    ],
    "source_task_selection": [
      "h"
    ],
    "was_in_survey": [
      "mag"
    ],
    "in_title": [
      "t",
      "r",
      "l",
      "s"
    ],
    "in_abs": [
      "t",
      "r",
      "l",
      "k",
      "s"
    ],
    "in_contet": [
      "t",
      "r",
      "l",
      "k",
      "s"
    ],
    "rejected_or_unpublished": null,
    "duplication_status": null,
    "paper_for_thesis": 2,
    "paper_available": 1,
    "behind_paywall": 0,
    "paywall_circumventable": 0,
    "author_names": [
      "Dulce Adriana",
      "G&#x00F3;mez Rosal"
    ]
  },
  {
    "id": 3016335401,
    "title": "A Confrontation Decision-Making Method with Deep Reinforcement Learning and Knowledge Transfer for Multi-Agent System",
    "abst": "In this paper, deep reinforcement learning (DRL) and knowledge transfer are used to achieve the effective control of the learning agent for the confrontation in the multi-agent systems. Firstly, a multi-agent Deep Deterministic Policy Gradient (DDPG) algorithm with parameter sharing is proposed to achieve confrontation decision-making of multi-agent. In the process of training, the information of other agents is introduced to the critic network to improve the strategy of confrontation. The parameter sharing mechanism can reduce the loss of experience storage. In the DDPG algorithm, we use four neural networks to generate real-time action and Q-value function respectively and use a momentum mechanism to optimize the training process to accelerate the convergence rate for the neural network. Secondly, this paper introduces an auxiliary controller using a policy-based reinforcement learning (RL) method to achieve the assistant decision-making for the game agent. In addition, an effective reward function is used to help agents balance losses of enemies and our side. Furthermore, this paper also uses the knowledge transfer method to extend the learning model to more complex scenes and improve the generalization of the proposed confrontation model. Two confrontation decision-making experiments are designed to verify the effectiveness of the proposed method. In a small-scale task scenario, the trained agent can successfully learn to fight with the competitors and achieve a good winning rate. For large-scale confrontation scenarios, the knowledge transfer method can gradually improve the decision-making level of the learning agent.",
    "url": "https://doi.org/10.3390/sym12040631",
    "lang": null,
    "authors": [
      3016667141
    ],
    "fos": [
      2776960227,
      79699506,
      114614502,
      97541855,
      127576917,
      154945302,
      3019712994,
      41550386,
      57869625,
      50644808,
      33923547
    ],
    "journals": [
      190787756
    ],
    "conferences": [],
    "conference_series": [],
    "references": [
      1983523797,
      2031748952,
      2045386145,
      2047029964,
      2083954950,
      2089904313,
      2164534565,
      2165698076,
      2287454828,
      2326062726,
      2329769476,
      2462102501,
      2518826713,
      2559793483,
      2576551432,
      2605924076,
      2772589676,
      2890218953,
      2910913210,
      2944305248,
      2946362902,
      2963890729,
      2974904967,
      2986543185,
      3006486026
    ],
    "filter_matches": [
      "tr",
      "tl",
      "trl",
      "rl"
    ],
    "rank": 23628,
    "citation_count": 0,
    "estimated_citation_count": 0,
    "publication_date": "2020-04-16",
    "found_in": 1,
    "transfer_experiment_type": [
      "#"
    ],
    "transfer_experiment_subtype": [
      "multi",
      "multiagent"
    ],
    "transfer_data_type": [
      "pi"
    ],
    "transfer_performance_metrics": [
      "tt"
    ],
    "implementation": [
      "Custom",
      "CS",
      "Formulas",
      "Figures",
      "Pseudo",
      "Tables"
    ],
    "policy_type": [
      "A2C",
      "DDPG",
      "MDDPG"
    ],
    "task_mappings": [
      "N/A"
    ],
    "autonomous_transfer": 0,
    "is_deep_rl": 1,
    "tags": [
      "Simulation",
      "InvertedPendulum",
      "RTS",
      "Starcraft",
      "Multiagent",
      "ClassicControl"
    ],
    "allowed_learner": [
      "TD"
    ],
    "country": [
      "China"
    ],
    "uni": [
      "Hubei University of Arts and Science"
    ],
    "department": [
      "Computer Engineering"
    ],
    "source_task_selection": [],
    "was_in_survey": [
      "mag"
    ],
    "in_title": [
      "t",
      "r",
      "l",
      "k"
    ],
    "in_abs": [
      "t",
      "r",
      "l",
      "k"
    ],
    "in_contet": [
      "t",
      "r",
      "l",
      "k"
    ],
    "rejected_or_unpublished": null,
    "duplication_status": null,
    "paper_for_thesis": 0,
    "paper_available": 1,
    "behind_paywall": 0,
    "paywall_circumventable": 0,
    "author_names": [
      "Chunyang Hu"
    ]
  },
  {
    "id": 3023661178,
    "title": "Sim-to-Real Transfer with Incremental Environment Complexity for Reinforcement Learning of Depth-Based Robot Navigation",
    "abst": "Transferring learning-based models to the real world remains one of the hardest problems in model-free control theory. Due to the cost of data collection on a real robot and the limited sample efficiency of Deep Reinforcement Learning algorithms, models are usually trained in a simulator which theoretically provides an infinite amount of data. Despite offering unbounded trial and error runs, the reality gap between simulation and the physical world brings little guarantee about the policy behavior in real operation. Depending on the problem, expensive real fine-tuning and/or a complex domain randomization strategy may be required to produce a relevant policy. In this paper, a Soft-Actor Critic (SAC) training strategy using incremental environment complexity is proposed to drastically reduce the need for additional training in the real world. The application addressed is depth-based mapless navigation, where a mobile robot should reach a given waypoint in a cluttered environment with no prior mapping information. Experimental results in simulated and real environments are presented to assess quantitatively the efficiency of the proposed approach, which demonstrated a success rate twice higher than a naive strategy.",
    "url": "http://arxiv.org/pdf/2004.14684.pdf",
    "lang": null,
    "authors": [
      742794741,
      2143517490,
      2952343075,
      3022982455
    ],
    "fos": [
      90509273,
      44154836,
      127413603,
      32076795,
      2993861310,
      97541855,
      133462117,
      2781271823,
      19966478
    ],
    "journals": [
      2596519289
    ],
    "conferences": [],
    "conference_series": [],
    "references": [
      1130790960,
      1581407678,
      1757796397,
      1996615911,
      2110762409,
      2151834591,
      2155027007,
      2167340365,
      2173248099,
      2218842719,
      2296073425,
      2362143032,
      2461937780,
      2513734981,
      2534269850,
      2535547924,
      2596367596,
      2605102758,
      2618092744,
      2732319713,
      2739161005,
      2746411854,
      2766447205,
      2781726626,
      2912063360,
      2918642789,
      2925306934,
      2942608247,
      2962747693,
      2962957005,
      2963428623,
      2963630234,
      2963871073,
      2964121744,
      2980730703,
      2980825073,
      2981030070,
      2985936292,
      2992693735
    ],
    "filter_matches": [
      "tr",
      "tl",
      "trl",
      "rl"
    ],
    "rank": 23640,
    "citation_count": 0,
    "estimated_citation_count": 0,
    "publication_date": "2020-04-30",
    "found_in": 1,
    "transfer_experiment_type": [
      "s",
      "levels"
    ],
    "transfer_experiment_subtype": [
      "sim2real",
      "multi"
    ],
    "transfer_data_type": [
      "pi"
    ],
    "transfer_performance_metrics": [],
    "implementation": [
      "Custom",
      "CS",
      "Gazebo",
      "PyTorch",
      "Gym",
      "Formulas",
      "Figures",
      "Tables"
    ],
    "policy_type": [
      "SAC"
    ],
    "task_mappings": [
      "N/A"
    ],
    "autonomous_transfer": 0,
    "is_deep_rl": 1,
    "tags": [
      "Robotics",
      "Gazebo",
      "DepthBased",
      "3D",
      "Navigation",
      "Simulation",
      "RealWorld",
      "Vehicle"
    ],
    "allowed_learner": [
      "TD"
    ],
    "country": [
      "France",
      "Australia"
    ],
    "uni": [
      "Universit Paris Saclay",
      "Flinders University"
    ],
    "department": [
      "Aerospace Lab",
      "Computer Science, Engineering and Mathematics"
    ],
    "source_task_selection": [
      "all"
    ],
    "was_in_survey": [
      "mag"
    ],
    "in_title": [
      "t",
      "r",
      "l"
    ],
    "in_abs": [
      "t",
      "r",
      "l"
    ],
    "in_contet": [
      "t",
      "r",
      "l",
      "k"
    ],
    "rejected_or_unpublished": null,
    "duplication_status": null,
    "paper_for_thesis": 0,
    "paper_available": 1,
    "behind_paywall": 0,
    "paywall_circumventable": 0,
    "author_names": [
      "Julien Marzat",
      "Julien Moras",
      "Thomas Chaffre",
      "Adrien Chan-Hon-Tong"
    ]
  },
  {
    "id": 2998972968,
    "title": "Activation and Spreading Sequence for Spreading Activation Policy Selection Method in Transfer Reinforcement Learning",
    "abst": "This paper proposes an automatic policy selection method using spreading activation theory based on psychological theory for transfer learning in reinforcement learning. Intel-ligent robot systems have recently been studied for practical applications such as home robot, communication robot, and warehouse robot. Learning algorithms are key to building useful robot systems important. For example, a robot can explore for optimal policy with trial and error using reinforcement learning. Moreover, transfer learning enables reuse of prior policy and is effective for environment adaptability. However, humans de-termine applicable methods in transfer learning. Policy selection method has been proposed for transfer learning in reinforcement learning using spreading activation model proposed in cognitive psychology. In this paper, novel activation function and spreading sequence is discussed for spreading policy selection method. Fur-ther computer simulations are used to examine the effectiveness of the proposed method for automatic policy selection in simplified shortest-path problem.",
    "url": "https://thesai.org/Downloads/Volume10No12/Paper_2-Activation_and_Spreading_Sequence_for_Spreading_Activation_Policy.pdf",
    "lang": null,
    "authors": [
      2120703638,
      2296688416,
      2306963904,
      2770191215,
      2999127167
    ],
    "fos": [
      38365724,
      65563180,
      154945302,
      150899416,
      32076795,
      206588197,
      97541855,
      124101348,
      90509273,
      177606310,
      41008148
    ],
    "journals": [
      23629721
    ],
    "conferences": [],
    "conference_series": [],
    "references": [],
    "filter_matches": [
      "tr",
      "tl",
      "trl",
      "rl"
    ],
    "rank": 23693,
    "citation_count": 0,
    "estimated_citation_count": 0,
    "publication_date": "2019-01-01",
    "found_in": 1,
    "transfer_experiment_type": [
      "s_i",
      "s_f"
    ],
    "transfer_experiment_subtype": [
      "multi"
    ],
    "transfer_data_type": [
      "pi",
      "rule"
    ],
    "transfer_performance_metrics": [
      "j",
      "ap"
    ],
    "implementation": [
      "Custom",
      "CS",
      "Formulas",
      "Figures",
      "Tables"
    ],
    "policy_type": [
      "PRQ"
    ],
    "task_mappings": [
      "N/A"
    ],
    "autonomous_transfer": 0,
    "is_deep_rl": 0,
    "tags": [
      "2D",
      "Navigation",
      "Grid",
      "Simulation"
    ],
    "allowed_learner": [
      "TD",
      "PS"
    ],
    "country": [
      "Japan"
    ],
    "uni": [
      "Tokyo Polytechnic University",
      "Tokyo Denki University",
      "The University of Tokyo"
    ],
    "department": [
      "Engineering",
      "Electronics and Mechatronics",
      "Information and Communication Engineering",
      "Precision Engineering"
    ],
    "source_task_selection": [
      "h"
    ],
    "was_in_survey": [
      "mag"
    ],
    "in_title": [
      "t",
      "r",
      "l"
    ],
    "in_abs": [
      "t",
      "r",
      "l"
    ],
    "in_contet": [
      "t",
      "r",
      "l",
      "k"
    ],
    "rejected_or_unpublished": null,
    "duplication_status": null,
    "paper_for_thesis": 0,
    "paper_available": 1,
    "behind_paywall": 0,
    "paywall_circumventable": 0,
    "author_names": [
      "Hitoshi Kono",
      "Tsuyoshi Suzuki",
      "Wen Wen",
      "Yusaku Takakuwa",
      "Ren Katayama"
    ]
  },
  {
    "id": 3007745042,
    "title": "Sim2Real Transfer for Reinforcement Learning without Dynamics Randomization.",
    "abst": "In this work we show how to use the Operational Space Control framework (OSC) under joint and cartesian constraints for reinforcement learning in cartesian space. Our method is therefore able to learn fast and with adjustable degrees of freedom, while we are able to transfer policies without additional dynamics randomizations on a KUKA LBR iiwa peg in-hole task. Before learning in simulation starts, we perform a system identification for aligning the simulation environment as far as possible with the dynamics of a real robot. Adding constraints to the OSC controller allows us to learn in a safe way on the real robot or to learn a flexible, goal conditioned policy that can be easily transferred from simulation to the real robot.",
    "url": "https://arxiv.org/abs/2002.11635",
    "lang": "en",
    "authors": [
      2899119525,
      2899286511,
      3008015682
    ],
    "fos": [
      90509273,
      16038011,
      97541855,
      119247159,
      119857082,
      154945302,
      41008148,
      79699506,
      133731056,
      2984638426
    ],
    "journals": [
      2596500785
    ],
    "conferences": [],
    "conference_series": [],
    "references": [
      1569657508,
      2112036188,
      2112474089,
      2121103318,
      2154395924,
      2155007355,
      2426267443,
      2580175322,
      2767050701,
      2771114757,
      2787718229,
      2792514232,
      2885163910,
      2897345632,
      2899059606,
      2904246096,
      2963940579,
      2967466692,
      2968095426,
      2981030070,
      3005305811
    ],
    "filter_matches": [
      "tr",
      "tl",
      "trl",
      "rl"
    ],
    "rank": 23752,
    "citation_count": 0,
    "estimated_citation_count": 0,
    "publication_date": "2020-02-19",
    "found_in": 1,
    "transfer_experiment_type": [
      "t"
    ],
    "transfer_experiment_subtype": [
      "sim2real"
    ],
    "transfer_data_type": [
      "pi"
    ],
    "transfer_performance_metrics": [
      "j"
    ],
    "implementation": [
      "Custom",
      "CS",
      "SurrealAI",
      "Pseudo",
      "Formulas",
      "Figures",
      "Tables",
      "Videos"
    ],
    "policy_type": [
      "SAC",
      "PPO",
      "DDPG"
    ],
    "task_mappings": [],
    "autonomous_transfer": 0,
    "is_deep_rl": 1,
    "tags": [
      "Robotics",
      "Simulation",
      "RealWorld",
      "Industry",
      "OSC"
    ],
    "allowed_learner": [
      "TD",
      "PS"
    ],
    "country": [
      "Germany"
    ],
    "uni": [
      "Corporate Research KUKA Deutschland GmbH"
    ],
    "department": [
      "Industry"
    ],
    "source_task_selection": [
      "h"
    ],
    "was_in_survey": [
      "mag"
    ],
    "in_title": [
      "t",
      "r",
      "l"
    ],
    "in_abs": [
      "t",
      "r",
      "l"
    ],
    "in_contet": [
      "t",
      "r",
      "l",
      "k"
    ],
    "rejected_or_unpublished": 2,
    "duplication_status": null,
    "paper_for_thesis": 0,
    "paper_available": 1,
    "behind_paywall": 0,
    "paywall_circumventable": 0,
    "author_names": [
      "Manuel Kaspar",
      "J&#x00FC;rgen Bock",
      "Juan David Mu&#x00F1;oz Osorio"
    ]
  },
  {
    "id": 2897816339,
    "title": "A Distributed Reinforcement Learning Solution With Knowledge Transfer Capability for A Bike Rebalancing Problem.",
    "abst": "Rebalancing is a critical service bottleneck for many transportation services, such as Citi Bike. Citi Bike relies on manual orchestrations of rebalancing bikes between dispatchers and field agents. Motivated by such problem and the lack of smart autonomous solutions in this area, this project explored a new RL architecture called Distributed RL (DiRL) with Transfer Learning (TL) capability. The DiRL solution is adaptive to changing traffic dynamics when keeping bike stock under control at the minimum cost. DiRL achieved a 350% improvement in bike rebalancing autonomously and TL offered a 62.4% performance boost in managing an entire bike network. Lastly, a field trip to the dispatch office of Chariot, a ride-sharing service, provided insights to overcome challenges of deploying an RL solution in the real world.",
    "url": "https://arxiv.org/pdf/1810.04058",
    "lang": "en",
    "authors": [
      2897937446
    ],
    "fos": [
      123657996,
      2776960227,
      2986151095,
      120314980,
      119857082,
      154945302,
      97541855,
      2780513914,
      150899416,
      2778962692,
      41008148
    ],
    "journals": [
      2596500785
    ],
    "conferences": [],
    "conference_series": [],
    "references": [
      158722652,
      1740140547,
      2145339207,
      2575472443,
      2745868649,
      2766447205,
      2964157221
    ],
    "filter_matches": [
      "tr",
      "tl",
      "trl",
      "rl"
    ],
    "rank": 23849,
    "citation_count": 1,
    "estimated_citation_count": 1,
    "publication_date": "2018-10-09",
    "found_in": 1,
    "transfer_experiment_type": [
      "t",
      "s"
    ],
    "transfer_experiment_subtype": [
      "same_all"
    ],
    "transfer_data_type": [
      "Q",
      "distil"
    ],
    "transfer_performance_metrics": [
      "j",
      "tr",
      "tt",
      "ap"
    ],
    "implementation": [
      "Custom",
      "OSS",
      "pandas",
      "numpy",
      "Formulas",
      "Figures"
    ],
    "policy_type": [
      "Q",
      "DiRL"
    ],
    "task_mappings": [
      "N/A"
    ],
    "autonomous_transfer": 0,
    "is_deep_rl": 0,
    "tags": [
      "Simulation",
      "Bike",
      "Distribution",
      "Distributed",
      "Rebalancing",
      "Mobility"
    ],
    "allowed_learner": [
      "TD"
    ],
    "country": [
      "USA"
    ],
    "uni": [
      "New York University"
    ],
    "department": [],
    "source_task_selection": [
      "h"
    ],
    "was_in_survey": [
      "mag"
    ],
    "in_title": [
      "t",
      "r",
      "l",
      "k"
    ],
    "in_abs": [
      "t",
      "r",
      "l",
      "k"
    ],
    "in_contet": [
      "t",
      "r",
      "l",
      "k",
      "s"
    ],
    "rejected_or_unpublished": 2,
    "duplication_status": null,
    "paper_for_thesis": 0,
    "paper_available": 1,
    "behind_paywall": 0,
    "paywall_circumventable": 0,
    "author_names": [
      "Ian Xiao"
    ]
  },
  {
    "id": 3019373547,
    "title": "TRANSFERABLE TRAINING FOR AUTOMATED REINFORCEMENT-LEARNING-BASED APPLICATION-MANAGERS",
    "abst": "The current document is directed to transfer of training received by a first automated reinforcement-learning-based application manager while controlling a first application is transferred to a second automated reinforcement-learning-based application manager which controls a second application different from the first application. Transferable training provides a basis for automated generation of applications from application components. Transferable training is obtained from composition of applications from application components and composition of reinforcement-learning-based-control-and-learning constructs from reinforcement-learning-based-control-and-learning constructs of application components.",
    "url": "https://lens.org/007-981-527-649-224",
    "lang": null,
    "authors": [
      3019489884,
      3019770304,
      3020113820,
      3020413817,
      3020548623
    ],
    "fos": [
      97541855,
      115903868,
      2777938197,
      41008148
    ],
    "journals": [],
    "conferences": [],
    "conference_series": [],
    "references": [],
    "filter_matches": [
      "tr",
      "tl",
      "trl",
      "rl"
    ],
    "rank": 23854,
    "citation_count": 0,
    "estimated_citation_count": 0,
    "publication_date": "2020-02-27",
    "found_in": 1,
    "transfer_experiment_type": [],
    "transfer_experiment_subtype": [],
    "transfer_data_type": [],
    "transfer_performance_metrics": [],
    "implementation": [
      "Custom",
      "CS"
    ],
    "policy_type": [],
    "task_mappings": [],
    "autonomous_transfer": 2,
    "is_deep_rl": 2,
    "tags": [
      "ApplicationManager"
    ],
    "allowed_learner": [],
    "country": [
      "USA"
    ],
    "uni": [
      "VMWare Inc"
    ],
    "department": [
      "Industry",
      "Patent"
    ],
    "source_task_selection": [],
    "was_in_survey": [
      "mag"
    ],
    "in_title": [
      "t",
      "r",
      "l"
    ],
    "in_abs": [
      "t",
      "r",
      "l"
    ],
    "in_contet": [],
    "rejected_or_unpublished": null,
    "duplication_status": null,
    "paper_for_thesis": 0,
    "paper_available": 1,
    "behind_paywall": 0,
    "paywall_circumventable": 0,
    "author_names": [
      "Nag Dev",
      "Burk Gregory T",
      "Wang Dongni",
      "Stephen Nicholas Mark Grant",
      "Yankov Yanislav"
    ]
  },
  {
    "id": 3018700377,
    "title": "Transfer Learning Applied to Reinforcement Learning-Based HVAC Control",
    "abst": null,
    "url": "http://link.springer.com/content/pdf/10.1007/s42979-020-00146-7.pdf",
    "lang": null,
    "authors": [
      2054530953,
      2196180740,
      3013213340
    ],
    "fos": [
      133731056,
      2987308950,
      150899416,
      97541855,
      41008148
    ],
    "journals": [],
    "conferences": [],
    "conference_series": [],
    "references": [
      37165804,
      125541279,
      605348272,
      1209078686,
      1582436621,
      2049838563,
      2083255397,
      2096943734,
      2097381042,
      2159880874,
      2253429366,
      2281071090,
      2344023930,
      2440926996,
      2625874945,
      2735995851,
      2747014259,
      2757455114,
      2795237036,
      2801379499,
      2891546789,
      2901645090,
      2909510634,
      2980797340,
      3011120880
    ],
    "filter_matches": [
      "tr",
      "tl",
      "trl",
      "rl"
    ],
    "rank": 23897,
    "citation_count": 0,
    "estimated_citation_count": 0,
    "publication_date": "2020-04-20",
    "found_in": 1,
    "transfer_experiment_type": [
      "t"
    ],
    "transfer_experiment_subtype": [
      "same_all"
    ],
    "transfer_data_type": [
      "Q"
    ],
    "transfer_performance_metrics": [
      "j",
      "tt"
    ],
    "implementation": [
      "Custom",
      "CS",
      "Pseudo",
      "Formulas",
      "Tables"
    ],
    "policy_type": [
      "Q"
    ],
    "task_mappings": [
      "N/A"
    ],
    "autonomous_transfer": 0,
    "is_deep_rl": 0,
    "tags": [
      "Simulation",
      "HVAC",
      "Power",
      "SmartEnergy",
      "Heat",
      "Ventilation"
    ],
    "allowed_learner": [
      "TD",
      "B"
    ],
    "country": [
      "Ireland"
    ],
    "uni": [
      "National University of Ireland"
    ],
    "department": [
      "Science and Engineering"
    ],
    "source_task_selection": [
      "h"
    ],
    "was_in_survey": [
      "mag"
    ],
    "in_title": [
      "t",
      "r",
      "l"
    ],
    "in_abs": [
      "t",
      "r",
      "l",
      "k"
    ],
    "in_contet": [
      "t",
      "r",
      "l",
      "k"
    ],
    "rejected_or_unpublished": null,
    "duplication_status": null,
    "paper_for_thesis": 0,
    "paper_available": 1,
    "behind_paywall": 0,
    "paywall_circumventable": 0,
    "author_names": [
      "Michael Schukat",
      "Enda Barrett",
      "Paulo Lissa"
    ]
  },
  {
    "id": 3005639539,
    "title": "Latent Structure Matching for Knowledge Transfer in Reinforcement Learning",
    "abst": "Reinforcement learning algorithms usually require a large number of empirical samples and give rise to a slow convergence in practical applications. One solution is to introduce transfer learning: Knowledge from well-learned source tasks can be reused to reduce sample request and accelerate the learning of target tasks. However, if an unmatched source task is selected, it will slow down or even disrupt the learning procedure. Therefore, it is very important for knowledge transfer to select appropriate source tasks that have a high degree of matching with target tasks. In this paper, a novel task matching algorithm is proposed to derive the latent structures of value functions of tasks, and align the structures for similarity estimation. Through the latent structure matching, the highly-matched source tasks are selected effectively, from which knowledge is then transferred to give action advice, and improve exploration strategies of the target tasks. Experiments are conducted on the simulated navigation environment and the mountain car environment. The results illustrate the significant performance gain of the improved exploration strategy, compared with traditional &#x03F5; -greedy exploration strategy. A theoretical proof is also given to verify the improvement of the exploration strategy based on latent structure matching.",
    "url": "https://doi.org/10.3390/fi12020036",
    "lang": null,
    "authors": [
      3006202462,
      3006354969
    ],
    "fos": [
      150899416,
      3017822451,
      154945302,
      2776960227,
      2993038593,
      97541855,
      119857082,
      31258907,
      61455927,
      41008148
    ],
    "journals": [
      34838331
    ],
    "conferences": [],
    "conference_series": [],
    "references": [
      32403112,
      158722652,
      605348272,
      1992419399,
      2053186076,
      2100811108,
      2114235770,
      2130935956,
      2133040789,
      2134332047,
      2165698076,
      2169911641,
      2326461200,
      2344786740,
      2583813242,
      2799011018,
      2896893468
    ],
    "filter_matches": [
      "tr",
      "tl",
      "trl",
      "rl"
    ],
    "rank": 23908,
    "citation_count": 0,
    "estimated_citation_count": 0,
    "publication_date": "2020-02-13",
    "found_in": 1,
    "transfer_experiment_type": [
      "s",
      "levels"
    ],
    "transfer_experiment_subtype": [
      "multi"
    ],
    "transfer_data_type": [
      "Q",
      "manifold"
    ],
    "transfer_performance_metrics": [
      "j",
      "tt",
      "ap"
    ],
    "implementation": [
      "Custom",
      "CS",
      "Formulas",
      "Pseudo",
      "Gym",
      "Tables",
      "Figures"
    ],
    "policy_type": [
      "Q",
      "LSM"
    ],
    "task_mappings": [
      "N/A"
    ],
    "autonomous_transfer": 0,
    "is_deep_rl": 0,
    "tags": [
      "Simulation",
      "MountainCar",
      "ClassicControl"
    ],
    "allowed_learner": [
      "TD"
    ],
    "country": [
      "China"
    ],
    "uni": [
      "University of Shanghai"
    ],
    "department": [
      "Computer Engineering and Science"
    ],
    "source_task_selection": [
      "h"
    ],
    "was_in_survey": [
      "mag"
    ],
    "in_title": [
      "t",
      "r",
      "l",
      "k"
    ],
    "in_abs": [
      "t",
      "r",
      "l",
      "k"
    ],
    "in_contet": [
      "t",
      "r",
      "l",
      "k"
    ],
    "rejected_or_unpublished": null,
    "duplication_status": null,
    "paper_for_thesis": 0,
    "paper_available": 1,
    "behind_paywall": 0,
    "paywall_circumventable": 0,
    "author_names": [
      "Fenglei Yang",
      "Yi Zhou"
    ]
  },
  {
    "id": 3002235425,
    "title": "Transferring task models in Reinforcement Learning agents",
    "abst": "The main objective of transfer learning is to reuse knowledge acquired in a previous learned task, in order to enhance the learning procedure in a new and more complex task. Transfer learning compr...",
    "url": "https://dl.acm.org/doi/10.1016/j.neucom.2012.08.039",
    "lang": "en",
    "authors": [
      3000782702,
      3000916716,
      3001316097,
      3002525273
    ],
    "fos": [
      154945302,
      97541855,
      107457646,
      206588197,
      119857082,
      33923547,
      150899416
    ],
    "journals": [
      45693802
    ],
    "conferences": [],
    "conference_series": [],
    "references": [],
    "filter_matches": [
      "tr",
      "tl",
      "trl",
      "rl"
    ],
    "rank": 23926,
    "citation_count": 0,
    "estimated_citation_count": 0,
    "publication_date": "2013-05-01",
    "found_in": 1,
    "transfer_experiment_type": [
      "v",
      "t"
    ],
    "transfer_experiment_subtype": [
      "diff-it"
    ],
    "transfer_data_type": [
      "model",
      "r"
    ],
    "transfer_performance_metrics": [
      "tt",
      "tr"
    ],
    "implementation": [
      "Custom",
      "CS",
      "Formulas",
      "Pseudo",
      "Figures",
      "Tables"
    ],
    "policy_type": [
      "TiMRLA",
      "Q",
      "SARSA"
    ],
    "task_mappings": [
      "sup"
    ],
    "autonomous_transfer": 0,
    "is_deep_rl": 0,
    "tags": [
      "2D",
      "3D",
      "2Dto3D",
      "MountainCar",
      "MountainCar3D",
      "Simulation",
      "ServerScheduling",
      "ClassicControl"
    ],
    "allowed_learner": [
      "TD",
      "MB"
    ],
    "country": [
      "Greece",
      "France"
    ],
    "uni": [
      "Aristotle University of Thessaloniki",
      "Universit Joseph Fourier"
    ],
    "department": [
      "Informatics",
      "Laboratoire LIG"
    ],
    "source_task_selection": [
      "h"
    ],
    "was_in_survey": [
      "mag"
    ],
    "in_title": [
      "t",
      "r",
      "l"
    ],
    "in_abs": [
      "t",
      "r",
      "l",
      "k"
    ],
    "in_contet": [
      "t",
      "r",
      "l",
      "k"
    ],
    "rejected_or_unpublished": null,
    "duplication_status": null,
    "paper_for_thesis": 0,
    "paper_available": 1,
    "behind_paywall": 0,
    "paywall_circumventable": 0,
    "author_names": [
      "TsoumakasGrigorios",
      "FachantidisAnestis",
      "VlahavasIoannis",
      "PartalasIoannis"
    ]
  },
  {
    "id": 2997158575,
    "title": "Embodiment Mapping Method between Heterogeneous Robots based on Self-body Representation for Transfer Learning in Reinforcement Learning",
    "abst": null,
    "url": "https://www.jstage.jst.go.jp/article/jsmermd/2019/0/2019_1A1-P01/_pdf",
    "lang": null,
    "authors": [
      2120703638,
      2998333492
    ],
    "fos": [
      90509273,
      154945302,
      2911180341,
      150899416,
      97541855,
      41008148
    ],
    "journals": [],
    "conferences": [],
    "conference_series": [],
    "references": [],
    "filter_matches": [
      "tr",
      "tl",
      "trl",
      "rl"
    ],
    "rank": 23989,
    "citation_count": 0,
    "estimated_citation_count": 0,
    "publication_date": "2019-01-01",
    "found_in": 1,
    "transfer_experiment_type": [],
    "transfer_experiment_subtype": [],
    "transfer_data_type": [],
    "transfer_performance_metrics": [],
    "implementation": [
      "Custom",
      "CS"
    ],
    "policy_type": [],
    "task_mappings": [],
    "autonomous_transfer": 2,
    "is_deep_rl": 2,
    "tags": [
      "Robotics"
    ],
    "allowed_learner": [],
    "country": [
      "Japan"
    ],
    "uni": [],
    "department": [],
    "source_task_selection": [],
    "was_in_survey": [
      "mag"
    ],
    "in_title": [
      "t",
      "r",
      "l"
    ],
    "in_abs": [],
    "in_contet": [],
    "rejected_or_unpublished": null,
    "duplication_status": null,
    "paper_for_thesis": 0,
    "paper_available": 0,
    "behind_paywall": 1,
    "paywall_circumventable": 1,
    "author_names": [
      "Hitoshi Kono",
      "Satoru Ikeda"
    ]
  },
  {
    "id": 2131464614,
    "title": "Learning Transfer Automatic through Data Mining in Reinforcement Learning",
    "abst": "One of the problems in reinforcement learning is that as the environment becomes more complex, the number of parameters used in decision making increase which leads us to a slow decision making process. The main idea here is to come up with a new algorithm which is able to transfer the information, using data mining techniques in extracting the patterns. We introduce a new algorithm for state transitions and actions which happen during the transfer by the agent are saved as a data set for data mining techniques which is presented Learning With Action Transfer (LAT). The main idea is to use the repeated action in each state, as a pattern in similar states as a means to improve learning speed and performance. The results in our algorithm will be compared to the results in Qlearning algorithm.. General Terms Reinforcement learning",
    "url": "https://www.ijcaonline.org/archives/volume88/number13/15411-3885",
    "lang": "en",
    "authors": [
      2317581014,
      2683321685
    ],
    "fos": [
      154945302,
      124101348,
      41008148,
      122308676,
      150899416,
      97541855,
      119857082
    ],
    "journals": [
      150996103
    ],
    "conferences": [],
    "conference_series": [],
    "references": [
      24477102,
      1515851193,
      1534331386,
      1584313244,
      1607318605,
      1822705290,
      2107726111,
      2117629901,
      2121517924,
      2158150115
    ],
    "filter_matches": [
      "tr",
      "tl",
      "trl",
      "rl"
    ],
    "rank": 24002,
    "citation_count": 0,
    "estimated_citation_count": 0,
    "publication_date": "2014-02-14",
    "found_in": 1,
    "transfer_experiment_type": [],
    "transfer_experiment_subtype": [
      "same_all"
    ],
    "transfer_data_type": [
      "exp"
    ],
    "transfer_performance_metrics": [
      "tt"
    ],
    "implementation": [
      "Custom",
      "CS",
      "Pseudo",
      "Figures"
    ],
    "policy_type": [
      "LAT",
      "Q"
    ],
    "task_mappings": [
      "N/A"
    ],
    "autonomous_transfer": 0,
    "is_deep_rl": 0,
    "tags": [
      "2D",
      "Navigation",
      "Simulation",
      "Grid"
    ],
    "allowed_learner": [
      "TD"
    ],
    "country": [
      "Iran"
    ],
    "uni": [
      "University of Bojnord",
      "Amirkabir University of Technology"
    ],
    "department": [
      "Computer Science"
    ],
    "source_task_selection": [],
    "was_in_survey": [
      "mag"
    ],
    "in_title": [
      "t",
      "r",
      "l"
    ],
    "in_abs": [
      "t",
      "r",
      "l"
    ],
    "in_contet": [
      "t",
      "r",
      "l",
      "k"
    ],
    "rejected_or_unpublished": null,
    "duplication_status": null,
    "paper_for_thesis": 0,
    "paper_available": 1,
    "behind_paywall": 0,
    "paywall_circumventable": 0,
    "author_names": [
      "Nafiseh Didkar",
      "Zeinab Arabasadi"
    ]
  },
  {
    "id": 3010746488,
    "title": "Reinforcement learning-based collision avoidance: impact of reward function and knowledge transfer",
    "abst": null,
    "url": "https://www.cambridge.org/core/services/aop-cambridge-core/content/view/S0890060420000141",
    "lang": null,
    "authors": [
      3011355460,
      3011631120
    ],
    "fos": [
      127413603,
      97541855,
      107457646,
      121704057,
      2776960227,
      539667460
    ],
    "journals": [
      97964121
    ],
    "conferences": [],
    "conference_series": [],
    "references": [
      167964105,
      1506146479,
      1527702126,
      1757796397,
      1821462560,
      2017325967,
      2024388749,
      2031727428,
      2047985315,
      2059652044,
      2061562262,
      2076450475,
      2078488070,
      2088173505,
      2089080976,
      2103120971,
      2157762168,
      2162390675,
      2164114810,
      2165698076,
      2168231600,
      2168904841,
      2174786457,
      2201581102,
      2253807446,
      2257979135,
      2277461844,
      2339945684,
      2397746618,
      2404399993,
      2415570156,
      2463627759,
      2467923710,
      2472587927,
      2485267956,
      2528485485,
      2531360032,
      2618530766,
      2766447205,
      2899403804,
      2919115771,
      2952523895,
      2963809389,
      2964121744
    ],
    "filter_matches": [
      "tr",
      "tl",
      "trl",
      "rl",
      "rlreward",
      "trreward",
      "tlrreward"
    ],
    "rank": 24015,
    "citation_count": 0,
    "estimated_citation_count": 0,
    "publication_date": "2020-03-16",
    "found_in": 1,
    "transfer_experiment_type": [
      "s_i",
      "s_f",
      "levels"
    ],
    "transfer_experiment_subtype": [
      "multi"
    ],
    "transfer_data_type": [
      "r",
      "pi"
    ],
    "transfer_performance_metrics": [
      "tt",
      "tr"
    ],
    "implementation": [
      "Custom",
      "CS",
      "Formulas",
      "Figures",
      "Tables"
    ],
    "policy_type": [
      "DQN"
    ],
    "task_mappings": [
      "N/A"
    ],
    "autonomous_transfer": 0,
    "is_deep_rl": 1,
    "tags": [
      "2D",
      "Navigation",
      "Simulation",
      "CollisionAvoidance",
      "Vehicle"
    ],
    "allowed_learner": [
      "TD"
    ],
    "country": [
      "USA"
    ],
    "uni": [
      "University of Southern California"
    ],
    "department": [
      "Aerospace and Mechanical Engineering"
    ],
    "source_task_selection": [
      "h",
      "lib"
    ],
    "was_in_survey": [
      "mag"
    ],
    "in_title": [
      "t",
      "r",
      "l",
      "k"
    ],
    "in_abs": [
      "t",
      "r",
      "l",
      "k"
    ],
    "in_contet": [
      "t",
      "r",
      "l",
      "k",
      "s"
    ],
    "rejected_or_unpublished": null,
    "duplication_status": null,
    "paper_for_thesis": 0,
    "paper_available": 1,
    "behind_paywall": 0,
    "paywall_circumventable": 0,
    "author_names": [
      "Yan Jin",
      "Xiongqing Liu"
    ]
  },
  {
    "id": 2986299952,
    "title": "Improving the learning speed in reinforcement learning issues based on the transfer learning of neuro-fuzzy knowledge",
    "abst": null,
    "url": "https://tjee.tabrizu.ac.ir/article_9441_f0444730a3a1ac375a6c1b54e6330981.pdf",
    "lang": "en",
    "authors": [
      2183941728,
      2670237096
    ],
    "fos": [
      150899416,
      97541855,
      154945302,
      41008148,
      29470771
    ],
    "journals": [],
    "conferences": [],
    "conference_series": [],
    "references": [],
    "filter_matches": [
      "tr",
      "tl",
      "trl",
      "rl"
    ],
    "rank": 24020,
    "citation_count": 0,
    "estimated_citation_count": 0,
    "publication_date": "2019-12-01",
    "found_in": 1,
    "transfer_experiment_type": [],
    "transfer_experiment_subtype": [],
    "transfer_data_type": [
      "Q",
      "fea"
    ],
    "transfer_performance_metrics": [],
    "implementation": [
      "Custom",
      "CS"
    ],
    "policy_type": [
      "Q"
    ],
    "task_mappings": [],
    "autonomous_transfer": 0,
    "is_deep_rl": 0,
    "tags": [
      "Foreign Language"
    ],
    "allowed_learner": [
      "TD"
    ],
    "country": [
      "Iran"
    ],
    "uni": [
      "Science and Arts University"
    ],
    "department": [
      "Computer Engineering"
    ],
    "source_task_selection": [],
    "was_in_survey": [
      "mag"
    ],
    "in_title": [
      "t",
      "r",
      "l"
    ],
    "in_abs": [
      "t",
      "r",
      "l"
    ],
    "in_contet": [
      "t",
      "r",
      "l"
    ],
    "rejected_or_unpublished": null,
    "duplication_status": null,
    "paper_for_thesis": 0,
    "paper_available": 1,
    "behind_paywall": 0,
    "paywall_circumventable": 0,
    "author_names": [
      "E. Ghandehari",
      "F. Saadatjoo"
    ]
  },
  {
    "id": 168011852,
    "title": "From Transfer to Scaling: Lessons Learned in Understanding Novel Reinforcement Learning Algorithms",
    "abst": "A major drawback of reinforcement learning (RL) is the slow learning rate. We are interested in speeding up RL. We first approached this problem with transfer learning where we have two domains. We developed a method to transfer knowledge from a completely trained RL domain to a partially trained related domain (where we want to speed up learning) and this helped increase the learning rate sufficiently. While trying to come up with a theoretical justification we found that our method of transfer of knowledge was actually scaling the Q-values, which was the main reason for the effects seen. We then scaled the Q-values with an appropriate scalar value in the RL domain after partial learning and saw similar results. Empirical results in a variety of grid worlds and a multi-agent block loading domain that is exceptionally difficult to solve using standard reinforcement learning algorithms show significant speedups in learning using scaling.",
    "url": "https://www.aaai.org/Papers/Workshops/2008/WS-08-14/WS08-14-005.pdf",
    "lang": null,
    "authors": [
      2217452330,
      2541907479
    ],
    "fos": [
      28006648,
      11413529,
      77967617,
      77075516,
      50292564,
      97541855,
      41008148,
      119857082,
      112972136,
      154945302,
      24138899,
      199190896
    ],
    "journals": [],
    "conferences": [],
    "conference_series": [],
    "references": [
      108768416,
      1586944634,
      1777239053,
      2095625001,
      2121863487,
      2122451452,
      2126565096,
      2132057084,
      2153353285,
      3011120880
    ],
    "filter_matches": [
      "tr",
      "tl",
      "trl",
      "rl"
    ],
    "rank": 24036,
    "citation_count": 0,
    "estimated_citation_count": 0,
    "publication_date": "2008-01-01",
    "found_in": 1,
    "transfer_experiment_type": [
      "#",
      "v"
    ],
    "transfer_experiment_subtype": [
      "same_all"
    ],
    "transfer_data_type": [
      "Q",
      "scaling"
    ],
    "transfer_performance_metrics": [
      "tt",
      "j"
    ],
    "implementation": [
      "Custom",
      "CS",
      "Formulas",
      "Figures"
    ],
    "policy_type": [
      "Q"
    ],
    "task_mappings": [
      "N/A"
    ],
    "autonomous_transfer": 0,
    "is_deep_rl": 0,
    "tags": [
      "2D",
      "Navigation",
      "Simulation",
      "Grid",
      "MultiAgent"
    ],
    "allowed_learner": [
      "TD"
    ],
    "country": [
      "USA"
    ],
    "uni": [
      "University of Maryland Baltimore County"
    ],
    "department": [
      "Computer Science and Electrical Engineering"
    ],
    "source_task_selection": [],
    "was_in_survey": [
      "mag"
    ],
    "in_title": [
      "t",
      "r",
      "l"
    ],
    "in_abs": [
      "t",
      "r",
      "l",
      "k"
    ],
    "in_contet": [
      "t",
      "r",
      "l",
      "k"
    ],
    "rejected_or_unpublished": null,
    "duplication_status": null,
    "paper_for_thesis": 0,
    "paper_available": 1,
    "behind_paywall": 0,
    "paywall_circumventable": 0,
    "author_names": [
      "Tim Oates",
      "Soumi Ray"
    ]
  },
  {
    "id": 3025362191,
    "title": "Accelerating bio-inspired optimizer with transfer reinforcement learning for reactive power optimization",
    "abst": "This paper proposes a novel accelerating bio-inspired optimizer (ABO) associated with transfer reinforcement learning (TRL) to solve the reactive power optimization (RPO) in large-scale power syste...",
    "url": "https://dl.acm.org/doi/10.1016/j.knosys.2016.10.024",
    "lang": "en",
    "authors": [
      3024089040,
      3024529625,
      3025610209,
      3025770077
    ],
    "fos": [
      119857082,
      97541855,
      154945302,
      108755667,
      41008148
    ],
    "journals": [
      10169007
    ],
    "conferences": [],
    "conference_series": [],
    "references": [],
    "filter_matches": [
      "tr",
      "tl",
      "trl",
      "rl"
    ],
    "rank": 24037,
    "citation_count": 0,
    "estimated_citation_count": 0,
    "publication_date": "2017-01-15",
    "found_in": 1,
    "transfer_experiment_type": [
      "t"
    ],
    "transfer_experiment_subtype": [
      "same_all"
    ],
    "transfer_data_type": [
      "Q"
    ],
    "transfer_performance_metrics": [
      "j",
      "tt",
      "tr"
    ],
    "implementation": [
      "Custom",
      "CS",
      "Pseudo",
      "Formulas",
      "Figures",
      "Tables"
    ],
    "policy_type": [
      "ABO"
    ],
    "task_mappings": [],
    "autonomous_transfer": 0,
    "is_deep_rl": 0,
    "tags": [
      "RPO",
      "Simulation",
      "IEEE118",
      "IEEE300",
      "MemoryMatrix",
      "Decomposition"
    ],
    "allowed_learner": [
      "TD"
    ],
    "country": [
      "China"
    ],
    "uni": [
      "South China University of Technology",
      "Kunming University of Science and Technology"
    ],
    "department": [
      "Electric Power Engineering"
    ],
    "source_task_selection": [
      "h"
    ],
    "was_in_survey": [
      "mag"
    ],
    "in_title": [
      "t",
      "r",
      "l"
    ],
    "in_abs": [
      "t",
      "r",
      "l",
      "k"
    ],
    "in_contet": [
      "t",
      "r",
      "l",
      "k"
    ],
    "rejected_or_unpublished": null,
    "duplication_status": null,
    "paper_for_thesis": 0,
    "paper_available": 1,
    "behind_paywall": 1,
    "paywall_circumventable": 1,
    "author_names": [
      "YuTao",
      "YangBo",
      "ZhangXiaoshun",
      "ChengLefeng"
    ]
  },
  {
    "id": 2604038904,
    "title": "Transfer reinforcement learning framework for energy saving in next generation wireless networks",
    "abst": null,
    "url": "https://repository.iiitd.edu.in/jspui/handle/123456789/417",
    "lang": "en",
    "authors": [
      2292648433,
      2309477498,
      2644663943
    ],
    "fos": [
      150899416,
      108037233,
      41008148,
      97541855,
      154945302,
      158207573
    ],
    "journals": [],
    "conferences": [],
    "conference_series": [],
    "references": [
      568400298,
      1559035773,
      1979367954,
      1998058169,
      2049652116,
      2084032658,
      2087179032,
      2090500089,
      2097381042,
      2121863487,
      2130827801,
      2136519475,
      2151062451,
      2163553782,
      2165698076,
      2788355865
    ],
    "filter_matches": [
      "tr",
      "tl",
      "trl",
      "rl"
    ],
    "rank": 24058,
    "citation_count": 0,
    "estimated_citation_count": 0,
    "publication_date": "2016-09-13",
    "found_in": 1,
    "transfer_experiment_type": [
      "s_i"
    ],
    "transfer_experiment_subtype": [
      "same_all"
    ],
    "transfer_data_type": [
      "pi"
    ],
    "transfer_performance_metrics": [
      "ap",
      "tr"
    ],
    "implementation": [
      "Custom",
      "CS",
      "Formulas",
      "Figures",
      "Tables"
    ],
    "policy_type": [
      "Actor-Critic"
    ],
    "task_mappings": [
      "N/A"
    ],
    "autonomous_transfer": 0,
    "is_deep_rl": 0,
    "tags": [
      "Energy Saving",
      "Network",
      "WiFi",
      "RealWorld",
      "Simulation"
    ],
    "allowed_learner": [
      "TD"
    ],
    "country": [
      "India"
    ],
    "uni": [
      "Indraprastha Institute of Information Technology"
    ],
    "department": [
      "Electronics and Communication Engineering"
    ],
    "source_task_selection": [
      "h"
    ],
    "was_in_survey": [
      "mag"
    ],
    "in_title": [
      "t",
      "r",
      "l"
    ],
    "in_abs": [
      "t",
      "r",
      "l"
    ],
    "in_contet": [
      "t",
      "r",
      "l",
      "k"
    ],
    "rejected_or_unpublished": null,
    "duplication_status": null,
    "paper_for_thesis": 2,
    "paper_available": 1,
    "behind_paywall": 0,
    "paywall_circumventable": 0,
    "author_names": [
      "Shreyata Sharma",
      "Sumit J. Darak",
      "A.K. Srivastava"
    ]
  },
  {
    "id": 2910425974,
    "title": "Knowledge transfer in deep reinforcement learning",
    "abst": "Tesis presentada en cumplimentacion de los requisitos del Master en Ingenieria Computacional y Sistemas Inteligentes en elGrupo de Robotica y Sistemas Autonomos. Departamento de Ciencias de la Computacion e Inteligencia Artificial con la colaboracion de Fundacion Deusto (DeustoTech).",
    "url": "https://addi.ehu.es/handle/10810/30667",
    "lang": "es",
    "authors": [
      2909426460
    ],
    "fos": [
      97541855,
      15708023,
      15744967,
      2776960227
    ],
    "journals": [],
    "conferences": [],
    "conference_series": [],
    "references": [],
    "filter_matches": [
      "tr",
      "tl",
      "trl",
      "rl"
    ],
    "rank": 24084,
    "citation_count": 0,
    "estimated_citation_count": 0,
    "publication_date": "2018-12-20",
    "found_in": 1,
    "transfer_experiment_type": [
      "t",
      "levels"
    ],
    "transfer_experiment_subtype": [
      "multi"
    ],
    "transfer_data_type": [
      "pi"
    ],
    "transfer_performance_metrics": [
      "j",
      "tt"
    ],
    "implementation": [
      "Custom",
      "CS",
      "TensorFlow",
      "Figures",
      "Formulas",
      "Pseudo",
      "Keras",
      "Gym",
      "Tables"
    ],
    "policy_type": [
      "DQN"
    ],
    "task_mappings": [
      "N/A"
    ],
    "autonomous_transfer": 0,
    "is_deep_rl": 1,
    "tags": [
      "Simulation",
      "VideoGames",
      "Games",
      "Atari",
      "SpaceInvaders",
      "DemonAttack",
      "Qbert",
      "CNN"
    ],
    "allowed_learner": [
      "TD"
    ],
    "country": [
      "Spain"
    ],
    "uni": [
      "University of the Basque Country"
    ],
    "department": [
      "Computer Engineering and Intelligent Systems"
    ],
    "source_task_selection": [
      "h"
    ],
    "was_in_survey": [
      "mag"
    ],
    "in_title": [
      "t",
      "r",
      "l",
      "k"
    ],
    "in_abs": [],
    "in_contet": [],
    "rejected_or_unpublished": null,
    "duplication_status": null,
    "paper_for_thesis": 2,
    "paper_available": 1,
    "behind_paywall": 0,
    "paywall_circumventable": 0,
    "author_names": [
      "Rub&#x00E9;n Mulero Mart&#x00ED;nez"
    ]
  },
  {
    "id": 867213644,
    "title": "Task Localization, Similarity, and Transfer; Towards a Reinforcement Learning Task Library System",
    "abst": "TASK LOCALIZATION, SIMILARITY, AND TRANSFER; TOWARDS A REINFORCEMENT LEARNING TASK LIBRARY SYSTEM James Lamond Carroll Department of Computer Science",
    "url": "http://scholarsarchive.byu.edu/cgi/viewcontent.cgi?article=1578&amp;context=etd",
    "lang": null,
    "authors": [
      2169019634
    ],
    "fos": [
      2986563244,
      28006648,
      97541855,
      188888258,
      108771440,
      47932503,
      154945302,
      204126058,
      41008148
    ],
    "journals": [],
    "conferences": [],
    "conference_series": [],
    "references": [
      107715831,
      121337034,
      1255659923,
      1258105458,
      1499408472,
      1546451855,
      1547105496,
      1560550898,
      1566538838,
      1584431645,
      1591803298,
      1598748993,
      1621791442,
      1806053028,
      1853223271,
      1991564165,
      1996847178,
      2006258746,
      2048226872,
      2107726111,
      2114451917,
      2121863487,
      2122451452,
      2153766311,
      2160371091,
      2166996723,
      2172405120,
      2174227032,
      2411466998,
      2795495912,
      2914746235,
      3022194887
    ],
    "filter_matches": [
      "tr",
      "tl",
      "trl",
      "rl"
    ],
    "rank": 24107,
    "citation_count": 0,
    "estimated_citation_count": 0,
    "publication_date": "2005-01-01",
    "found_in": 1,
    "transfer_experiment_type": [
      "s_i",
      "s_f",
      "t",
      "levels"
    ],
    "transfer_experiment_subtype": [
      "multi"
    ],
    "transfer_data_type": [
      "pi",
      "pi_fix",
      "pi_dyn"
    ],
    "transfer_performance_metrics": [
      "tt"
    ],
    "implementation": [
      "Custom",
      "CS",
      "Formulas",
      "Figures",
      "Tables",
      "Theorem"
    ],
    "policy_type": [
      "Q"
    ],
    "task_mappings": [
      "N/A"
    ],
    "autonomous_transfer": 0,
    "is_deep_rl": 0,
    "tags": [
      "2D",
      "Navigation",
      "Grid",
      "Robotics",
      "NomadII",
      "Simulation"
    ],
    "allowed_learner": [
      "TD",
      "H",
      "B"
    ],
    "country": [
      "USA"
    ],
    "uni": [
      "Brigham Young University"
    ],
    "department": [
      "Computer Science"
    ],
    "source_task_selection": [
      "h"
    ],
    "was_in_survey": [
      "mag"
    ],
    "in_title": [
      "t",
      "r",
      "l"
    ],
    "in_abs": [
      "t",
      "r",
      "l"
    ],
    "in_contet": [
      "t",
      "r",
      "l",
      "k",
      "s"
    ],
    "rejected_or_unpublished": null,
    "duplication_status": null,
    "paper_for_thesis": 2,
    "paper_available": 1,
    "behind_paywall": 0,
    "paywall_circumventable": 0,
    "author_names": [
      "James L. Carroll"
    ]
  },
  {
    "id": 2769673101,
    "title": "Policy Forgetting Method with Step Units for Decresing of Negative Transfer in Transfer Learning of Reinforcement learning",
    "abst": null,
    "url": "https://www.jstage.jst.go.jp/article/jsmermd/2017/0/2017_2P1-F06/_article/-char/ja/",
    "lang": null,
    "authors": [
      2120703638,
      2477110478,
      2768884496,
      2770133805,
      2778339008,
      2779821440
    ],
    "fos": [
      97541855,
      2779178101,
      154945302,
      119857082,
      7149132,
      41008148,
      150899416
    ],
    "journals": [],
    "conferences": [],
    "conference_series": [],
    "references": [],
    "filter_matches": [
      "tr",
      "tl",
      "trl",
      "rl"
    ],
    "rank": 24132,
    "citation_count": 0,
    "estimated_citation_count": 0,
    "publication_date": "2017-01-01",
    "found_in": 1,
    "transfer_experiment_type": [],
    "transfer_experiment_subtype": [],
    "transfer_data_type": [],
    "transfer_performance_metrics": [],
    "implementation": [
      "Custom",
      "CS"
    ],
    "policy_type": [],
    "task_mappings": [],
    "autonomous_transfer": 2,
    "is_deep_rl": 2,
    "tags": [
      "Robotics"
    ],
    "allowed_learner": [
      "TD"
    ],
    "country": [
      "Japan"
    ],
    "uni": [],
    "department": [],
    "source_task_selection": [],
    "was_in_survey": [
      "mag"
    ],
    "in_title": [
      "t",
      "r",
      "l"
    ],
    "in_abs": [],
    "in_contet": [],
    "rejected_or_unpublished": null,
    "duplication_status": null,
    "paper_for_thesis": 0,
    "paper_available": 0,
    "behind_paywall": 1,
    "paywall_circumventable": 0,
    "author_names": [
      "Hitoshi Kono",
      "Tsuyoshi Suzuki",
      "Takuro Gunji",
      "Yuki Ito",
      "Akiya Kamimura",
      "Kohji Tomita"
    ]
  },
  {
    "id": 2970910303,
    "title": "Human Sub-goal Transfer in Hierarchical Reinforcement Learning",
    "abst": null,
    "url": "https://confit.atlas.jp/guide/event/jsai2019/subject/1Q2-J-2-02/detail",
    "lang": "en",
    "authors": [
      2970248720
    ],
    "fos": [
      97541855,
      41008148,
      154945302
    ],
    "journals": [],
    "conferences": [],
    "conference_series": [],
    "references": [],
    "filter_matches": [
      "tr",
      "tl",
      "trl",
      "rl"
    ],
    "rank": 24199,
    "citation_count": 0,
    "estimated_citation_count": 0,
    "publication_date": "2019-04-08",
    "found_in": 1,
    "transfer_experiment_type": [],
    "transfer_experiment_subtype": [
      "same_all"
    ],
    "transfer_data_type": [
      "options"
    ],
    "transfer_performance_metrics": [],
    "implementation": [
      "Custom",
      "CS",
      "Pseudo",
      "Tables",
      "Figures",
      "Formulas"
    ],
    "policy_type": [
      "Q"
    ],
    "task_mappings": [],
    "autonomous_transfer": 0,
    "is_deep_rl": 0,
    "tags": [
      "SubGoal",
      "Transfer",
      "Simulation"
    ],
    "allowed_learner": [
      "H",
      "TD"
    ],
    "country": [
      "Japan"
    ],
    "uni": [
      "SOKENDAI",
      "National Institute of Informatics"
    ],
    "department": [],
    "source_task_selection": [],
    "was_in_survey": [
      "mag"
    ],
    "in_title": [
      "t",
      "r",
      "l"
    ],
    "in_abs": [
      "t",
      "r",
      "l",
      "k"
    ],
    "in_contet": [],
    "rejected_or_unpublished": null,
    "duplication_status": null,
    "paper_for_thesis": 0,
    "paper_available": 1,
    "behind_paywall": 0,
    "paywall_circumventable": 0,
    "author_names": [
      "Takato Okudo"
    ]
  },
  {
    "id": 2914188912,
    "title": "Transfer reinforcement learning for task-oriented dialogue systems",
    "abst": null,
    "url": "http://repository.ust.hk/ir/Record/1783.1-92234",
    "lang": "en",
    "authors": [
      2716735303
    ],
    "fos": [
      107457646,
      97541855,
      41008148,
      2986145474
    ],
    "journals": [],
    "conferences": [],
    "conference_series": [],
    "references": [],
    "filter_matches": [
      "tr",
      "tl",
      "trl",
      "rl"
    ],
    "rank": 24226,
    "citation_count": 0,
    "estimated_citation_count": 0,
    "publication_date": "2018-01-01",
    "found_in": 1,
    "transfer_experiment_type": [
      "s",
      "t"
    ],
    "transfer_experiment_subtype": [
      "multi"
    ],
    "transfer_data_type": [
      "WT",
      "special_weights"
    ],
    "transfer_performance_metrics": [
      "ap"
    ],
    "implementation": [
      "Custom",
      "CS",
      "Formulas",
      "Figures",
      "Tables",
      "Pseudo"
    ],
    "policy_type": [
      "Q",
      "PETAL",
      "PROMISE"
    ],
    "task_mappings": [
      "exp"
    ],
    "autonomous_transfer": 0,
    "is_deep_rl": 0,
    "tags": [
      "Task",
      "Oriented",
      "Dialogue",
      "System",
      "CNN",
      "NLP",
      "Simulation"
    ],
    "allowed_learner": [
      "TD",
      "H",
      "B"
    ],
    "country": [
      "China"
    ],
    "uni": [
      "Hong Kong University of Science and Technology"
    ],
    "department": [
      "Computer Science and Engineering"
    ],
    "source_task_selection": [
      "h"
    ],
    "was_in_survey": [
      "mag"
    ],
    "in_title": [
      "t",
      "r",
      "l"
    ],
    "in_abs": [
      "t",
      "r",
      "l",
      "k"
    ],
    "in_contet": [
      "t",
      "r",
      "l",
      "k",
      "s"
    ],
    "rejected_or_unpublished": null,
    "duplication_status": null,
    "paper_for_thesis": 1,
    "paper_available": 1,
    "behind_paywall": 0,
    "paywall_circumventable": 0,
    "author_names": [
      "Kaixiang Mo"
    ]
  },
  {
    "id": 2945850821,
    "title": "Improving Deep Reinforcement Learning Using Graph Convolution and Visual Domain Transfer",
    "abst": null,
    "url": "https://tigerprints.clemson.edu/all_dissertations/2268/",
    "lang": "en",
    "authors": [
      2232493843
    ],
    "fos": [
      152003226,
      80444323,
      41008148,
      97541855,
      132525143
    ],
    "journals": [],
    "conferences": [],
    "conference_series": [],
    "references": [],
    "filter_matches": [
      "tr",
      "tl",
      "trl",
      "rl"
    ],
    "rank": 24226,
    "citation_count": 0,
    "estimated_citation_count": 0,
    "publication_date": "2018-01-01",
    "found_in": 1,
    "transfer_experiment_type": [
      "s"
    ],
    "transfer_experiment_subtype": [
      "same_all"
    ],
    "transfer_data_type": [
      "Q"
    ],
    "transfer_performance_metrics": [
      "tr",
      "j"
    ],
    "implementation": [
      "Custom",
      "CS",
      "Formulas",
      "Pseudo",
      "Theorem",
      "TensorFlow",
      "Tables",
      "Figures"
    ],
    "policy_type": [
      "GVIN",
      "VIN",
      "DQN"
    ],
    "task_mappings": [],
    "autonomous_transfer": 0,
    "is_deep_rl": 1,
    "tags": [
      "2D",
      "Simulation",
      "Graph",
      "IrregularGraph",
      "RoadNetwork",
      "Navigation",
      "CARLA",
      "CNN",
      "MonteCarlo"
    ],
    "allowed_learner": [
      "TD",
      "MB"
    ],
    "country": [
      "USA"
    ],
    "uni": [
      "Clemson University"
    ],
    "department": [
      "Computer Engineering"
    ],
    "source_task_selection": [],
    "was_in_survey": [
      "mag"
    ],
    "in_title": [
      "t",
      "r",
      "l"
    ],
    "in_abs": [
      "t",
      "r",
      "l"
    ],
    "in_contet": [
      "t",
      "r",
      "l",
      "k"
    ],
    "rejected_or_unpublished": null,
    "duplication_status": null,
    "paper_for_thesis": 1,
    "paper_available": 1,
    "behind_paywall": 0,
    "paywall_circumventable": 0,
    "author_names": [
      "Sufeng Niu"
    ]
  },
  {
    "id": 1483433356,
    "title": "Skill Transfer of a Mobile Robot Obtained by Reinforcement Learning to a Different Mobile Robot",
    "abst": "Reinforcement learning (RL) is suitable for navigation of a mobile robot. We overcame some difficulties of RL which are large computational cost and determination of parameter values for RL with the help of a genetic algorithm (GA) and method of parameter prediction based on results of GA and complexity measure. As a result of these proposals, we succeeded in navigating the real robot practically. In our previous studies, we just one kind of mobile robot, which has three wheels. Our RL method can decrease the computational cost for learning of navigation and development of mobile robots, provided the skill obtained by RL for one mobile robot can be transferred to other mobile robots.",
    "url": "https://dblp.uni-trier.de/db/series/sci/sci266.html#KameiI10",
    "lang": "en",
    "authors": [
      2168347238,
      2430752679
    ],
    "fos": [
      41008148,
      19966478,
      8880873,
      97541855,
      65401140,
      90509273,
      2986635905,
      188888258,
      26990112,
      154945302
    ],
    "journals": [],
    "conferences": [],
    "conference_series": [],
    "references": [
      375927847,
      1981684023,
      2089100788,
      2124595713,
      2911283634,
      2914656440
    ],
    "filter_matches": [
      "tr",
      "tl",
      "trl",
      "rl"
    ],
    "rank": 24349,
    "citation_count": 0,
    "estimated_citation_count": 0,
    "publication_date": "2010-01-01",
    "found_in": 1,
    "transfer_experiment_type": [
      "t"
    ],
    "transfer_experiment_subtype": [
      "same_all"
    ],
    "transfer_data_type": [
      "Q"
    ],
    "transfer_performance_metrics": [
      "j"
    ],
    "implementation": [
      "Custom",
      "CS",
      "Formulas",
      "Figures"
    ],
    "policy_type": [
      "Q"
    ],
    "task_mappings": [
      "N/A"
    ],
    "autonomous_transfer": 0,
    "is_deep_rl": 0,
    "tags": [
      "Simulation",
      "RealWorld",
      "Robot",
      "Navigation",
      "2D",
      "GeneticAlgorithm",
      "Vehicle",
      "MultiAgent"
    ],
    "allowed_learner": [
      "TD",
      "H",
      "B"
    ],
    "country": [
      "Japan"
    ],
    "uni": [
      "Nishinippon Institute of Technology",
      "Kyushu Institute of Technology"
    ],
    "department": [
      "Electrical, Electronics and Information Technology"
    ],
    "source_task_selection": [],
    "was_in_survey": [
      "mag"
    ],
    "in_title": [
      "t",
      "r",
      "l",
      "s"
    ],
    "in_abs": [
      "t",
      "r",
      "l",
      "s"
    ],
    "in_contet": [
      "t",
      "r",
      "l",
      "s"
    ],
    "rejected_or_unpublished": null,
    "duplication_status": null,
    "paper_for_thesis": 0,
    "paper_available": 1,
    "behind_paywall": 1,
    "paywall_circumventable": 1,
    "author_names": [
      "Masumi Ishikawa",
      "Keiji Kamei"
    ]
  },
  {
    "id": 2770456017,
    "title": "Multi-Agent Reinforcement Learning: - Learning Effects by Transfer Learning, Eligibility Trace and Sensor Range -@@@&#x2015;&#x8EE2;&#x79FB;&#x5B66;&#x7FD2;&#xFF0C;&#x9069;&#x683C;&#x5EA6;&#xFF0C;&#x30BB;&#x30F3;&#x30B5;&#x7BC4;&#x56F2;&#x306E;&#x5B66;&#x7FD2;&#x52B9;&#x679C;&#x2015;",
    "abst": null,
    "url": "https://www.jstage.jst.go.jp/article/jsmermd/2017/0/2017_2A1-G09/_article/-char/ja/",
    "lang": null,
    "authors": [
      2769242408,
      2769679125
    ],
    "fos": [
      97541855,
      10431821,
      154945302,
      119857082,
      41008148,
      150899416
    ],
    "journals": [],
    "conferences": [],
    "conference_series": [],
    "references": [],
    "filter_matches": [
      "tr",
      "tl",
      "trl",
      "rl"
    ],
    "rank": 24399,
    "citation_count": 0,
    "estimated_citation_count": 0,
    "publication_date": "2017-01-01",
    "found_in": 1,
    "transfer_experiment_type": [],
    "transfer_experiment_subtype": [
      "multi",
      "multiagent"
    ],
    "transfer_data_type": [
      "rule"
    ],
    "transfer_performance_metrics": [],
    "implementation": [
      "Custom",
      "CS"
    ],
    "policy_type": [
      "Q"
    ],
    "task_mappings": [],
    "autonomous_transfer": 2,
    "is_deep_rl": 0,
    "tags": [
      "Simulation",
      "NarrowPath",
      "Rules"
    ],
    "allowed_learner": [
      "TD"
    ],
    "country": [
      "Japan"
    ],
    "uni": [],
    "department": [],
    "source_task_selection": [],
    "was_in_survey": [
      "mag"
    ],
    "in_title": [],
    "in_abs": [],
    "in_contet": [],
    "rejected_or_unpublished": null,
    "duplication_status": null,
    "paper_for_thesis": 0,
    "paper_available": 0,
    "behind_paywall": 1,
    "paywall_circumventable": 0,
    "author_names": [
      "Yuta Kajii",
      "Kazuaki Yamada"
    ]
  },
  {
    "id": 2778259398,
    "title": "Evolutionary transfer learning for complex multi-agent reinforcement learning systems",
    "abst": null,
    "url": "https://dr.ntu.edu.sg/handle/10356/72997",
    "lang": "en",
    "authors": [
      2776786694
    ],
    "fos": [
      154945302,
      150899416,
      97541855,
      41008148
    ],
    "journals": [],
    "conferences": [],
    "conference_series": [],
    "references": [],
    "filter_matches": [
      "tr",
      "tl",
      "trl",
      "rl"
    ],
    "rank": 24399,
    "citation_count": 0,
    "estimated_citation_count": 0,
    "publication_date": "2017-01-01",
    "found_in": 1,
    "transfer_experiment_type": [
      "s_i",
      "s_f",
      "levels"
    ],
    "transfer_experiment_subtype": [
      "multi"
    ],
    "transfer_data_type": [
      "Q",
      "advisor"
    ],
    "transfer_performance_metrics": [
      "ap"
    ],
    "implementation": [
      "Custom",
      "CS",
      "Pseudo",
      "Formulas",
      "Figures",
      "Tables"
    ],
    "policy_type": [
      "Q",
      "FALCON",
      "BP",
      "eTL",
      "PTL",
      "AE-AVG",
      "MAS"
    ],
    "task_mappings": [
      "N/A"
    ],
    "autonomous_transfer": 0,
    "is_deep_rl": 1,
    "tags": [
      "2D",
      "3D",
      "Simulation",
      "Games",
      "Navigation",
      "Minefield",
      "MinefieldNavigation",
      "UnrealTournament",
      "MultiAgent"
    ],
    "allowed_learner": [
      "TD",
      "MB"
    ],
    "country": [
      "Singapore"
    ],
    "uni": [
      "Nanyang Technological University"
    ],
    "department": [
      "Interdisciplinary Graduate School"
    ],
    "source_task_selection": [
      "all"
    ],
    "was_in_survey": [
      "mag"
    ],
    "in_title": [
      "t",
      "r",
      "l"
    ],
    "in_abs": [
      "t",
      "r",
      "l",
      "k"
    ],
    "in_contet": [
      "t",
      "r",
      "l",
      "k",
      "s"
    ],
    "rejected_or_unpublished": null,
    "duplication_status": null,
    "paper_for_thesis": 1,
    "paper_available": 1,
    "behind_paywall": 0,
    "paywall_circumventable": 0,
    "author_names": [
      "Yaqing Hou"
    ]
  },
  {
    "id": 2292364897,
    "title": "Transfer in reinforcement learning",
    "abst": null,
    "url": "https://ethos.bl.uk/OrderDetails.do?uin=uk.bl.ethos.675561",
    "lang": "en",
    "authors": [
      2639457211
    ],
    "fos": [
      154945302,
      47932503,
      97541855,
      41008148
    ],
    "journals": [],
    "conferences": [],
    "conference_series": [],
    "references": [],
    "filter_matches": [
      "tr",
      "tl",
      "trl",
      "rl"
    ],
    "rank": 24450,
    "citation_count": 0,
    "estimated_citation_count": 0,
    "publication_date": "2015-01-01",
    "found_in": 1,
    "transfer_experiment_type": [
      "t"
    ],
    "transfer_experiment_subtype": [
      "same_all"
    ],
    "transfer_data_type": [
      "Q",
      "pri"
    ],
    "transfer_performance_metrics": [
      "j",
      "tr",
      "tt"
    ],
    "implementation": [
      "Custom",
      "CS",
      "Pseudo",
      "Formulas",
      "Theorem",
      "Figures",
      "Tables",
      "Lemma"
    ],
    "policy_type": [
      "Q",
      "SARSA"
    ],
    "task_mappings": [],
    "autonomous_transfer": 0,
    "is_deep_rl": 0,
    "tags": [
      "Simulation",
      "CartPole",
      "MountainCar",
      "Acrobot",
      "Pinball",
      "ClassicControl"
    ],
    "allowed_learner": [
      "TD",
      "MB"
    ],
    "country": [
      "UK"
    ],
    "uni": [
      "University of Aberdeen"
    ],
    "department": [
      "Computer Science"
    ],
    "source_task_selection": [
      "h"
    ],
    "was_in_survey": [
      "mag"
    ],
    "in_title": [
      "t",
      "r",
      "l"
    ],
    "in_abs": [
      "t",
      "r",
      "l",
      "s",
      "k"
    ],
    "in_contet": [
      "t",
      "r",
      "l",
      "s",
      "k"
    ],
    "rejected_or_unpublished": null,
    "duplication_status": null,
    "paper_for_thesis": 1,
    "paper_available": 1,
    "behind_paywall": 0,
    "paywall_circumventable": 0,
    "author_names": [
      "John W. Alexander"
    ]
  },
  {
    "id": 1511270451,
    "title": "Graph Laplacian Based Transfer Learning Methods in Reinforcement Learning",
    "abst": "In the real world, people often reuse their knowledge in dealing with daily life problems. They can observe facts in an environment and recall similar experience in the past to deal with new situations. This phenomenon implies that there must be some features for people to compare the similarity between two environments. For example, toilet papers are usually placed nearby cashiers in different marts in Taiwan as shown in Fig. 1. In these two photos, orange ovals represent features for cashiers and red ovals represent features for toilet papers. The features which allow people to recognize the fact &#x201C;Toilet papers are usually placed nearby cashiers.&#x201D; are the kinds of experience which could be reused.",
    "url": "http://cdn.intechweb.org/pdfs/10989.pdf",
    "lang": "en",
    "authors": [
      1974178992,
      2141359790,
      2155552642,
      2329082613
    ],
    "fos": [
      8038995,
      2778357586,
      154945302,
      206588197,
      150899416,
      115178988,
      119857082,
      100660578,
      199190896,
      41008148,
      97541855
    ],
    "journals": [],
    "conferences": [],
    "conference_series": [],
    "references": [
      90468634,
      1578099820,
      1607318605,
      2098723043,
      2119567691,
      2121863487,
      2122410182,
      2128905965,
      2143958939,
      2154328025,
      2161795906,
      2164114810
    ],
    "filter_matches": [
      "tr",
      "tl",
      "trl",
      "rl"
    ],
    "rank": 24495,
    "citation_count": 0,
    "estimated_citation_count": 0,
    "publication_date": "2010-06-01",
    "found_in": 1,
    "transfer_experiment_type": [
      "v",
      "s",
      "levels",
      "s_i",
      "s_f"
    ],
    "transfer_experiment_subtype": [
      "multi"
    ],
    "transfer_data_type": [
      "Q",
      "pvf"
    ],
    "transfer_performance_metrics": [
      "j"
    ],
    "implementation": [
      "Custom",
      "CS",
      "Pseudo",
      "Formulas",
      "Figures"
    ],
    "policy_type": [
      "Q",
      "LaplacianGraph"
    ],
    "task_mappings": [
      "exp"
    ],
    "autonomous_transfer": 0,
    "is_deep_rl": 0,
    "tags": [
      "2D",
      "Navigation",
      "Grid",
      "Maze",
      "Simulation"
    ],
    "allowed_learner": [
      "TD"
    ],
    "country": [
      "Taiwan"
    ],
    "uni": [
      "National Tsing-Hua University"
    ],
    "department": [
      "Computer Science"
    ],
    "source_task_selection": [
      "h"
    ],
    "was_in_survey": [
      "mag"
    ],
    "in_title": [
      "t",
      "r",
      "l"
    ],
    "in_abs": [
      "t",
      "r",
      "l"
    ],
    "in_contet": [
      "t",
      "r",
      "l",
      "k"
    ],
    "rejected_or_unpublished": null,
    "duplication_status": 8,
    "paper_for_thesis": 1604516390,
    "paper_available": 1,
    "behind_paywall": 0,
    "paywall_circumventable": 0,
    "author_names": [
      "Ke-Ting Xiao",
      "Von-Wun Soo",
      "Yi-Ting Tsao",
      "Chung-Cheng Chiu"
    ]
  },
  {
    "id": 2559014014,
    "title": "Parallel Transfer Learning: Accelerating Reinforcement Learning in Multi-Agent Systems",
    "abst": null,
    "url": "http://www.tara.tcd.ie/handle/2262/77886",
    "lang": "en",
    "authors": [
      2444392358
    ],
    "fos": [
      47932503,
      41008148,
      41550386,
      150899416,
      154945302,
      97541855
    ],
    "journals": [],
    "conferences": [],
    "conference_series": [],
    "references": [],
    "filter_matches": [
      "tr",
      "tl",
      "trl",
      "rl"
    ],
    "rank": 24531,
    "citation_count": 0,
    "estimated_citation_count": 0,
    "publication_date": "2015-01-01",
    "found_in": 1,
    "transfer_experiment_type": [],
    "transfer_experiment_subtype": [
      "same_all"
    ],
    "transfer_data_type": [
      "Q",
      "r",
      "imitation"
    ],
    "transfer_performance_metrics": [
      "j",
      "tt",
      "ap"
    ],
    "implementation": [
      "Custom",
      "CS",
      "Pseudo",
      "Figures",
      "Tables",
      "Formulas",
      "GridLab"
    ],
    "policy_type": [
      "Q",
      "DistributedW"
    ],
    "task_mappings": [
      "exp"
    ],
    "autonomous_transfer": 0,
    "is_deep_rl": 0,
    "tags": [
      "MultiAgent",
      "Simulation",
      "MountainCar",
      "CartPole",
      "GridLab",
      "SmartGrid",
      "Parallel",
      "ClassicControl"
    ],
    "allowed_learner": [
      "TD"
    ],
    "country": [
      "Ireland"
    ],
    "uni": [
      "University of Dublin"
    ],
    "department": [
      "Computer Science"
    ],
    "source_task_selection": [
      "h"
    ],
    "was_in_survey": [
      "mag"
    ],
    "in_title": [
      "t",
      "r",
      "l"
    ],
    "in_abs": [
      "t",
      "r",
      "l",
      "k"
    ],
    "in_contet": [
      "t",
      "r",
      "l",
      "k"
    ],
    "rejected_or_unpublished": null,
    "duplication_status": null,
    "paper_for_thesis": 1,
    "paper_available": 1,
    "behind_paywall": 0,
    "paywall_circumventable": 0,
    "author_names": [
      "Adam Taylor"
    ]
  },
  {
    "id": 2473744971,
    "title": "Policy abstraction for transfer learning using learning vector quantization in reinforcement learning framework",
    "abst": "People grow up every day exposed to the infinite state space environment interacting with active biological subjects and machines. There are routines that are always expected and unpredicted events that are not completely known beforehand as well. When people interact with the future routines, they do not require the same effort as they do during the first time. Based on experience, irrelevant information that does not affect the achievement is ignored. For example, a new worker in his/her first day will carefully recognize the road to his/her office, including the road&#039;s name, signboards, and buildings as well as focusing on the traffic. After several months he/she, possibly, will focus only on buildings and traffic. Furthermore, when people interact with an unpredicted event, they will usually try to cope with the situation using their knowledge that is acquiredfrom their past experience. For example, an accident happened and the worker&#039;s daily route was jammed, here, he/she will try to find the alternate route based on the distance and the location of his/her office. This shows that people have an ability to benefit from their previous experience and knowledge for the future. Furthermore, the knowledge is not stored in a concrete or very detailed form, but in an abstract form that is ready to be used for routine events and also to be used for assisting in unknown events. Such abilities are obviously acquired through the most significant ability of a human being, which is learning ability from its successes and failures.",
    "url": "http://umpir.ump.edu.my/id/eprint/13521/",
    "lang": "en",
    "authors": [
      2127435356,
      2158735953
    ],
    "fos": [
      40567965,
      97541855,
      72434380,
      150899416,
      124304363,
      3020439559,
      154945302,
      127413603
    ],
    "journals": [],
    "conferences": [],
    "conference_series": [],
    "references": [],
    "filter_matches": [
      "tr",
      "tl",
      "trl",
      "rl"
    ],
    "rank": 24597,
    "citation_count": 0,
    "estimated_citation_count": 0,
    "publication_date": "2015-08-01",
    "found_in": 1,
    "transfer_experiment_type": [
      "s_i",
      "s_f",
      "levels",
      "t"
    ],
    "transfer_experiment_subtype": [
      "multi"
    ],
    "transfer_data_type": [
      "pri"
    ],
    "transfer_performance_metrics": [
      "tt"
    ],
    "implementation": [
      "Custom",
      "CS",
      "Formulas",
      "Figures",
      "Tables",
      "Pseudo"
    ],
    "policy_type": [
      "Q",
      "LVQ"
    ],
    "task_mappings": [
      "N/A"
    ],
    "autonomous_transfer": 0,
    "is_deep_rl": 0,
    "tags": [
      "2D",
      "Navigation",
      "Grid",
      "Maze",
      "Simulation"
    ],
    "allowed_learner": [
      "TD"
    ],
    "country": [
      "Japan"
    ],
    "uni": [
      "Kyushu University"
    ],
    "department": [
      "Electrical and Electronic Engineering"
    ],
    "source_task_selection": [
      "h"
    ],
    "was_in_survey": [
      "mag"
    ],
    "in_title": [
      "t",
      "r",
      "l"
    ],
    "in_abs": [
      "t",
      "r",
      "l"
    ],
    "in_contet": [
      "t",
      "r",
      "l"
    ],
    "rejected_or_unpublished": null,
    "duplication_status": null,
    "paper_for_thesis": 1,
    "paper_available": 1,
    "behind_paywall": 0,
    "paywall_circumventable": 0,
    "author_names": [
      "Ahmad Afif",
      "Mohd Faudzi"
    ]
  },
  {
    "id": 2613227537,
    "title": "Sample Complexity Reduction in Reinforcement Learning by Transferred Transition and Reward Probability",
    "abst": null,
    "url": "https://www.ieice.org/ken/paper/20131113eB7B/eng/",
    "lang": "en",
    "authors": [
      2225053515,
      2561557930,
      2613239990
    ],
    "fos": [
      154945302,
      119857082,
      41008148,
      150899416,
      97541855,
      2778445095
    ],
    "journals": [],
    "conferences": [],
    "conference_series": [],
    "references": [],
    "filter_matches": [
      "tr",
      "tl",
      "trl",
      "rl",
      "rlreward",
      "trreward",
      "tlrreward"
    ],
    "rank": 24612,
    "citation_count": 0,
    "estimated_citation_count": 0,
    "publication_date": "2013-11-05",
    "found_in": 1,
    "transfer_experiment_type": [
      "t"
    ],
    "transfer_experiment_subtype": [
      "multi"
    ],
    "transfer_data_type": [
      "fea",
      "r"
    ],
    "transfer_performance_metrics": [],
    "implementation": [
      "Custom",
      "CS"
    ],
    "policy_type": [
      "TR-MAX"
    ],
    "task_mappings": [],
    "autonomous_transfer": 0,
    "is_deep_rl": 0,
    "tags": [
      "MDP",
      "PAC-MDP"
    ],
    "allowed_learner": [
      "TD",
      "MB"
    ],
    "country": [
      "Japan"
    ],
    "uni": [
      "Tohoku University"
    ],
    "department": [
      "Graduate School of Information Sciences"
    ],
    "source_task_selection": [],
    "was_in_survey": [
      "mag"
    ],
    "in_title": [
      "t",
      "r",
      "l"
    ],
    "in_abs": [
      "t",
      "r",
      "l"
    ],
    "in_contet": [],
    "rejected_or_unpublished": null,
    "duplication_status": null,
    "paper_for_thesis": 0,
    "paper_available": 0,
    "behind_paywall": 1,
    "paywall_circumventable": 0,
    "author_names": [
      "Shinohara Ayumi",
      "Narisawa Kazuyuki",
      "Oguni Kouta"
    ]
  },
  {
    "id": 1470793189,
    "title": "Transferring Human Navigation Behaviors Into Robot Planning using Inverse Reinforcement Learning Techniques",
    "abst": null,
    "url": "https://idus.us.es/xmlui/handle/11441/26823",
    "lang": "es",
    "authors": [
      2152068532
    ],
    "fos": [
      2985814175,
      26990112,
      188888258,
      2986734826,
      47932503,
      127413603,
      154945302
    ],
    "journals": [],
    "conferences": [],
    "conference_series": [],
    "references": [],
    "filter_matches": [
      "tr",
      "tl",
      "trl",
      "rl"
    ],
    "rank": 24697,
    "citation_count": 0,
    "estimated_citation_count": 0,
    "publication_date": "2014-12-01",
    "found_in": 1,
    "transfer_experiment_type": [
      "s"
    ],
    "transfer_experiment_subtype": [
      "same_all"
    ],
    "transfer_data_type": [
      "I"
    ],
    "transfer_performance_metrics": [
      "j",
      "tt"
    ],
    "implementation": [
      "Custom",
      "CS",
      "Formulas",
      "Figures",
      "Tables"
    ],
    "policy_type": [
      "GPIRL",
      "PRX"
    ],
    "task_mappings": [
      "N/A"
    ],
    "autonomous_transfer": 0,
    "is_deep_rl": 0,
    "tags": [
      "Inverse",
      "IRL",
      "RealWorld",
      "Simulation",
      "Navigation",
      "Human",
      "Demonstration"
    ],
    "allowed_learner": [
      "TD",
      "B"
    ],
    "country": [
      "Spain"
    ],
    "uni": [
      "University of Seville"
    ],
    "department": [
      "Systems Engineering"
    ],
    "source_task_selection": [
      "all"
    ],
    "was_in_survey": [
      "mag"
    ],
    "in_title": [
      "t",
      "r",
      "l"
    ],
    "in_abs": [],
    "in_contet": [
      "t",
      "r",
      "l",
      "k",
      "s"
    ],
    "rejected_or_unpublished": null,
    "duplication_status": null,
    "paper_for_thesis": 1,
    "paper_available": 1,
    "behind_paywall": 0,
    "paywall_circumventable": 0,
    "author_names": [
      "Rafael Ram&#x00F3;n Vigo"
    ]
  },
  {
    "id": 2523226036,
    "title": "Integrating policy transfer, policy reuse and experience replay in speeding up reinforcement learning of the obstacle avoidance task",
    "abst": null,
    "url": "http://erepository.uonbi.ac.ke/handle/11295/90177",
    "lang": "en",
    "authors": [
      2523424088
    ],
    "fos": [
      154945302,
      97541855,
      2776731479,
      6683253,
      41008148,
      206588197
    ],
    "journals": [],
    "conferences": [],
    "conference_series": [],
    "references": [],
    "filter_matches": [
      "tr",
      "tl",
      "trl",
      "rl"
    ],
    "rank": 24697,
    "citation_count": 0,
    "estimated_citation_count": 0,
    "publication_date": "2014-01-01",
    "found_in": 1,
    "transfer_experiment_type": [
      "a"
    ],
    "transfer_experiment_subtype": [
      "multi"
    ],
    "transfer_data_type": [
      "pi",
      "experiencereplay"
    ],
    "transfer_performance_metrics": [
      "j",
      "tt",
      "ap"
    ],
    "implementation": [
      "Custom",
      "CS",
      "Figures",
      "Formulas",
      "Tables",
      "Pseudo"
    ],
    "policy_type": [
      "SARSA"
    ],
    "task_mappings": [
      "sup"
    ],
    "autonomous_transfer": 0,
    "is_deep_rl": 0,
    "tags": [
      "3D",
      "Navigation",
      "Robotics",
      "Obstacle",
      "Avoidance",
      "ExperienceReplay",
      "Vehicle",
      "Simulation"
    ],
    "allowed_learner": [
      "TD",
      "H"
    ],
    "country": [
      "Kenia"
    ],
    "uni": [
      "University of Nairobi"
    ],
    "department": [
      "Computing and Informatics"
    ],
    "source_task_selection": [],
    "was_in_survey": [
      "mag"
    ],
    "in_title": [
      "t",
      "r",
      "l"
    ],
    "in_abs": [
      "t",
      "r",
      "l",
      "k"
    ],
    "in_contet": [
      "t",
      "r",
      "l",
      "k",
      "s"
    ],
    "rejected_or_unpublished": null,
    "duplication_status": null,
    "paper_for_thesis": 1,
    "paper_available": 1,
    "behind_paywall": 0,
    "paywall_circumventable": 0,
    "author_names": [
      "K.A. Evans Miriti"
    ]
  },
  {
    "id": 2327840474,
    "title": "The Knowledge Transfer Methods and Its Application in Robot Based on Subtask Hierarchical Reinforcement Learning",
    "abst": null,
    "url": "http://www.aicit.org/aiss/global/paper_detail.html?jname=AISS&amp;q=1770",
    "lang": null,
    "authors": [
      2397209824,
      2694310899
    ],
    "fos": [
      56739046,
      2776960227,
      97541855,
      154945302,
      188888258,
      41008148,
      90509273
    ],
    "journals": [
      114506903
    ],
    "conferences": [],
    "conference_series": [],
    "references": [],
    "filter_matches": [
      "tr",
      "tl",
      "trl",
      "rl"
    ],
    "rank": 24794,
    "citation_count": 0,
    "estimated_citation_count": 0,
    "publication_date": "2012-11-30",
    "found_in": 1,
    "transfer_experiment_type": [],
    "transfer_experiment_subtype": [],
    "transfer_data_type": [],
    "transfer_performance_metrics": [],
    "implementation": [],
    "policy_type": [],
    "task_mappings": [],
    "autonomous_transfer": 2,
    "is_deep_rl": 2,
    "tags": [
      "Robotics",
      "Cleaning",
      "RealWorld"
    ],
    "allowed_learner": [],
    "country": [],
    "uni": [],
    "department": [],
    "source_task_selection": [],
    "was_in_survey": [
      "mag"
    ],
    "in_title": [],
    "in_abs": [],
    "in_contet": [],
    "rejected_or_unpublished": null,
    "duplication_status": null,
    "paper_for_thesis": 0,
    "paper_available": 0,
    "behind_paywall": 1,
    "paywall_circumventable": 0,
    "author_names": [
      "Li Ming",
      "Zhang Qian"
    ]
  }
]